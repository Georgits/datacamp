{
    "collab_server" : "",
    "contents" : "library(dplyr)\nlibrary(tidytext)\nlibrary(tidyr)\nlibrary(ggplot2)\nlibrary(lubridate)\n\nload(\"geocoded_tweets.rda\")\nload(\"shakespeare.rda\")\nload(\"climate_text.rda\")\nload(\"song_lyrics.rda\")\n\n# Chapter 1:  Tweets across the United States ----\n# Sentiment lexicons ----\n\n# Choose the bing lexicon\nget_sentiments(\"bing\")\n\n# Choose the nrc lexicon\nget_sentiments(\"nrc\") %>%\n  count(sentiment) # Count words by sentiment\n\n\n\n#  Implement an inner join ----\n# geocoded_tweets has been pre-defined\ngeocoded_tweets\n\n# Access bing lexicon: bing\nbing <- get_sentiments(\"bing\")\n\n# Use data frame with text data\ngeocoded_tweets %>%\n  # With inner join, implement sentiment analysis using `bing`\n  inner_join(bing)\n\n\n\n# What are the most common sadness words? ----\n# tweets_nrc has been pre-defined\nnrc <- get_sentiments(\"nrc\")\ntweets_nrc <- geocoded_tweets %>%\n  inner_join(nrc)\n\n\ntweets_nrc %>%\n  # Filter to only choose the words associated with sadness\n  filter(sentiment == \"sadness\") %>%\n  # Group by word\n  group_by(word) %>%\n  # Use the summarize verb to find the mean frequency\n  summarize(freq = mean(freq)) %>%\n  # Arrange to sort in order of descending frequency\n  arrange(desc(freq))\n\n\n\n\n# What are the most common joy words? ----\njoy_words <- tweets_nrc %>%\n  # Filter to choose only words associated with joy\n  filter(sentiment == \"joy\") %>%\n  # Group by each word\n  group_by(word) %>%\n  # Use the summarize verb to find the mean frequency\n  summarize(freq = mean(freq)) %>%\n  # Arrange to sort in order of descending frequency\n  arrange(desc(freq))    \n\n\njoy_words %>%\n  top_n(20) %>%\n  mutate(word = reorder(word, freq)) %>%\n  # Use aes() to put words on the x-axis and frequency on the y-axis\n  ggplot(aes(x = word, y = freq)) +\n  # Make a bar chart with geom_col()\n  geom_col() +\n  coord_flip()\n# If you are familiar with geom_bar(stat = \"identity\"), geom_col() does the same thing.\n\n\n\n\n# Do people in different states use different words? ----\ntweets_nrc %>%\n  # Find only the words for the state of Utah and associated with joy\n  filter(state == \"utah\",\n         sentiment == \"joy\") %>%\n  # Arrange to sort in order of descending frequency\n  arrange(desc(freq))\n\ntweets_nrc %>%\n  # Find only the words for the state of Louisiana and associated with joy\n  filter(state == \"louisiana\",\n         sentiment == \"joy\") %>%\n  # Arrange to sort in order of descending frequency\n  arrange(desc(freq))\n\n\n\n\n# Which states have the most positive Twitter users? ----\ntweets_bing <- geocoded_tweets %>%\n  # With inner join, implement sentiment analysis using `bing`\n  inner_join(bing)\n\ntweets_bing %>% \n  # Group by two columns: state and sentiment\n  group_by(state, sentiment) %>%\n  # Use summarize to calculate the mean frequency for these groups\n  summarize(freq = mean(freq)) %>%\n  spread(sentiment, freq) %>%\n  ungroup() %>%\n  # Calculate the ratio of positive to negative words\n  mutate(ratio = positive / negative,\n         state = reorder(state, ratio)) %>%\n  # Use aes() to put state on the x-axis and ratio on the y-axis\n  ggplot(aes(x = state, y = ratio)) +\n  # Make a plot with points using geom_point()\n  geom_point() +\n  coord_flip()\n\n\n\n# Chapter 2: Shakespeare gets Sentimental ----\n# To be, or not to be ----\n# The data set shakespeare in available in the workspace\nshakespeare\n\n# Pipe the shakespeare data frame to the next line\nshakespeare %>% \n  # Use count to find out how many titles/types there are\n  count(title, type)\n\n\n\n# Unnesting from text to word ----\ntidy_shakespeare <- shakespeare %>%\n  # Group by the titles of the plays\n  group_by(title) %>%\n  # Define a new column linenumber\n  mutate(linenumber = row_number()) %>%\n  # Transform the non-tidy text data to tidy text data\n  unnest_tokens(word, text) %>%\n  ungroup()\n\n# Pipe the tidy Shakespeare data frame to the next line\ntidy_shakespeare %>% \n  # Use count to find out how many times each word is used\n  count(word, sort = TRUE)\n\n\n\n\n# Sentiment analysis of Shakespeare ----\nshakespeare_sentiment <- tidy_shakespeare %>%\n  # Implement sentiment analysis with the \"bing\" lexicon\n  inner_join(get_sentiments(\"bing\"))\n\nshakespeare_sentiment %>%\n  # Find how many positive/negative words each play has\n  count(title, sentiment)\n\n\n\n\n# Tragedy or comedy? ----\nsentiment_counts <- tidy_shakespeare %>%\n  # Implement sentiment analysis using the \"bing\" lexicon\n  inner_join(get_sentiments(\"bing\")) %>%\n  # Count the number of words by title, type, and sentiment\n  count(title, type, sentiment)\n\nsentiment_counts %>%\n  # Group by the titles of the plays\n  group_by(title) %>%\n  # Find the total number of words in each play\n  mutate(total = sum(n),\n         # Calculate the number of words divided by the total\n         percent = n / total) %>%\n  # Filter the results for only negative sentiment\n  filter(sentiment == \"negative\") %>%\n  arrange(percent)\n\n\n\n# Most common positive and negative words ----\nword_counts <- tidy_shakespeare %>%\n  # Implement sentiment analysis using the \"bing\" lexicon\n  inner_join(get_sentiments(\"bing\")) %>%\n  # Count by word and sentiment\n  count(word, sentiment)\n\ntop_words <- word_counts %>%\n  # Group by sentiment\n  group_by(sentiment) %>%\n  # Take the top 10 for each sentiment\n  top_n(10) %>%\n  ungroup() %>%\n  # Make word a factor in order of n\n  mutate(word = reorder(word, n))\n\n# Use aes() to put words on the x-axis and n on the y-axis\nggplot(top_words, aes(x = word, y = n, fill = sentiment)) +\n  # Make a bar chart with geom_col()\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~sentiment, scales = \"free\") +  \n  coord_flip()\n\n\n\n\n# Word contributions by play ----\ntidy_shakespeare %>%\n  # Count by title and word\n  count(title, word, sort = TRUE) %>%\n  # Implement sentiment analysis using the \"afinn\" lexicon\n  inner_join(get_sentiments(\"afinn\")) %>%\n  # Filter to only examine the scores for Macbeth that are negative\n  filter(title == \"The Tragedy of Macbeth\", score < 0)\n\n\n\n\n# Calculating a contribution score ----\nsentiment_contributions <- tidy_shakespeare %>%\n  # Count by title and word\n  count(title, word, sort = TRUE) %>%\n  # Implement sentiment analysis using the \"afinn\" lexicon\n  inner_join(get_sentiments(\"afinn\")) %>%\n  # Group by title\n  group_by(title) %>%\n  # Calculate a contribution for each word in each title\n  mutate(contribution = (score * n) / sum (n)) %>%\n  ungroup()\n\nsentiment_contributions\n\n\n\n# Alas, poor Yorick! ----\nsentiment_contributions %>%\n  # Filter for Hamlet\n  filter(title == \"Hamlet, Prince of Denmark\") %>%\n  # Arrange to see the most negative words\n  arrange(contribution)\n\nsentiment_contributions %>%\n  # Filter for The Merchant of Venice\n  filter(title == \"The Merchant of Venice\") %>%\n  # Arrange to see the most positive words\n  arrange(desc(contribution))\n\n\n\n\n# Sentiment changes through a play ----\ntidy_shakespeare %>%\n  # Implement sentiment analysis using \"bing\" lexicon\n  inner_join(get_sentiments(\"bing\")) %>%\n  # Count using four arguments\n  count(title, type, index = linenumber %/% 70, sentiment)\n\n\n\n# Calculating net sentiment ----\ntidy_shakespeare %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(title, type, index = linenumber %/% 70, sentiment) %>%\n  # Spread sentiment and n across multiple columns\n  spread(sentiment, n, fill = 0) %>%\n  # Use mutate to find net sentiment\n  mutate(sentiment = positive - negative)\n\n\n\n\n# Visualizing narrative arcs ----\ntidy_shakespeare %>%\n  inner_join(get_sentiments(\"bing\")) %>%\n  count(title, type, index = linenumber %/% 70, sentiment) %>%\n  spread(sentiment, n, fill = 0) %>%\n  mutate(sentiment = positive - negative) %>%\n  # Put index on x-axis, sentiment on y-axis, and map comedy/tragedy to fill\n  ggplot(aes(x = index, y = sentiment, fill = type)) +\n  # Make a bar chart with geom_col()\n  geom_col() +\n  # Separate panels for each title with facet_wrap()\n  facet_wrap(~ title, scales = \"free_x\")\n# be sure to add scales = \"free_x\" so the x-axes behave nicely.\n\n\n\n\n\n# Chapter 3: Analyzing TV News ----\n# Tidying TV news ----\n# Pipe the climate_text dataset to the next line\ntidy_tv <- climate_text %>%\n  # Transform the non-tidy text data to tidy text data\n  unnest_tokens(word, text)\n\n\n\n\n# Counting totals ----\ntidy_tv %>% \n  anti_join(stop_words) %>%\n  # Count by word with sort = TRUE\n  count(word, sort = TRUE)\n\ntidy_tv %>%\n  # Count by station\n  count(station) %>%\n  # Rename the new column station_total\n  rename(station_total = n)\n\n\n\n\n# Sentiment analysis of TV news ----\ntv_sentiment <- tidy_tv %>% \n  # Group by station\n  group_by(station) %>% \n  # Define a new column station_total\n  mutate(station_total = n()) %>%\n  ungroup() %>%\n  # Implement sentiment analysis with the NRC lexicon\n  inner_join(get_sentiments(\"nrc\"))\n\n\n\n\n# Which station uses the most positive or negative words? ----\n# Which stations use the most negative words?\ntv_sentiment %>% \n  count(station, sentiment, station_total) %>%\n  # Define a new column percent\n  mutate(percent = n / station_total) %>%\n  # Filter only for negative words\n  filter(sentiment == \"negative\") %>%\n  # Arrange by percent\n  arrange(percent)\n\n# Now do the same but for positive words\ntv_sentiment %>% \n  count(station, sentiment, station_total) %>%\n  # Define a new column percent\n  mutate(percent = n / station_total) %>%\n  # Filter only for negative words\n  filter(sentiment == \"positive\") %>%\n  # Arrange by percent\n  arrange(percent)\n\n\n\n# Which words contribute to the sentiment scores? ----\ntv_sentiment %>%\n  # Count by word and sentiment\n  count(word, sentiment) %>%\n  # Group by sentiment\n  group_by(sentiment) %>%\n  # Take the top 10 words for each sentiment\n  top_n(10,n) %>%\n  ungroup() %>%\n  mutate(word = reorder(word, n)) %>%\n  # Set up the plot with aes()\n  ggplot(aes(x = word, y = n, fill = sentiment)) +\n  geom_col(show.legend = FALSE) +\n  facet_wrap(~ sentiment, scales = \"free\") +\n  coord_flip()\n\n\n\n# Word choice and TV station ----\ntv_sentiment %>%\n  # Filter for only negative words\n  filter(sentiment == \"negative\") %>%\n  # Count by word and station\n  count(word, station) %>%\n  # Group by station\n  group_by(station) %>%\n  # Take the top 10 words for each station\n  top_n(10, n) %>%\n  ungroup() %>%\n  mutate(word = reorder(paste(word, station, sep = \"__\"), n)) %>%\n  # Set up the plot with aes()\n  ggplot(aes(word, n, fill = station)) +\n  geom_col(show.legend = FALSE) +\n  scale_x_discrete(labels = function(x) gsub(\"__.+$\", \"\", x)) +\n  facet_wrap(~ station, nrow = 2, scales = \"free\") +\n  coord_flip()\n\n\n\n# Visualizing sentiment over time ----\nsentiment_by_time <- tidy_tv %>%\n  # Define a new column using floor_date()\n  mutate(date = floor_date(show_date, unit = \"6 months\")) %>%\n  # Group by date\n  group_by(date) %>%\n  mutate(total_words = n()) %>%\n  ungroup() %>%\n  # Implement sentiment analysis using the NRC lexicon\n  inner_join(get_sentiments(\"nrc\"))\n\nsentiment_by_time %>%\n  # Filter for positive and negative words\n  filter(sentiment %in% c(\"positive\", \"negative\")) %>%\n  # Count by date, sentiment, and total_words\n  count(date, sentiment, total_words) %>%\n  ungroup() %>%\n  mutate(percent = n / total_words) %>%\n  # Set up the plot with aes()\n  ggplot(aes(x = date, y = percent, color = sentiment)) +\n  geom_line(size = 1.5) +\n  geom_smooth(method = \"lm\", se = FALSE, lty = 2) +\n  expand_limits(y = 0)\n\n\n\n# Word changes over time -----\ntidy_tv %>%\n  # Define a new column that rounds each date to the nearest 1 month\n  mutate(date = floor_date(show_date, unit = \"1 months\")) %>%\n  filter(word %in% c(\"threat\", \"hoax\", \"denier\",\n                     \"real\", \"warming\", \"hurricane\")) %>%\n  # Count by date and word\n  count(date, word) %>%\n  ungroup() %>%\n  # Set up your plot with aes()\n  ggplot(aes(x = date, y = n, color = word)) +\n  # Make facets by word\n  facet_wrap(~ word) +\n  geom_line(size = 1.5, show.legend = FALSE) +\n  expand_limits(y = 0)\n\n\n\n\n# Chapter 4: Singing a Happy Song (or Sad?!)  ----\n# Tidying song lyrics ----\n# Pipe song_lyrics to the next line\ntidy_lyrics <- song_lyrics %>% \n  # Transform the lyrics column to a word column\n  unnest_tokens(word, lyrics)\n\n# Print tidy_lyrics\ntidy_lyrics\n\n\n# Calculating total words per song ----\ntotals <- tidy_lyrics %>%\n  # Count by song to find the word totals for each song\n  count(song) %>%\n  # Rename the new column\n  rename(total_words = n)\n\n# Print totals    \ntotals\n\nlyric_counts <- tidy_lyrics %>%\n  # Combine totals with tidy_lyrics using the \"song\" column\n  left_join(totals, by = \"song\")\n\n\n\n# Sentiment analysis on song lyrics ----\nlyric_sentiment <- lyric_counts %>%\n  # Implement sentiment analysis with the \"nrc\" lexicon\n  inner_join(get_sentiments(\"nrc\"))\n\nlyric_sentiment %>%\n  # Find how many sentiment words each song has\n  count(song, sentiment, sort = TRUE)\n\n\n\n\n# The most positive and negative songs ----\n# What songs have the highest proportion of negative words?\nlyric_sentiment %>%\n  # Count using three arguments\n  count(song, sentiment, total_words) %>%\n  ungroup() %>%\n  # Make a new percent column with mutate \n  mutate(percent = n / total_words) %>%\n  # Filter for only negative words\n  filter(sentiment == \"negative\") %>%\n  # Arrange by descending percent\n  arrange(desc(percent))\n\n# What songs have the highest proportion of positive words?\nlyric_sentiment %>%\n  count(song, sentiment, total_words) %>%\n  ungroup() %>%\n  mutate(percent = n / total_words) %>%\n  filter(sentiment == \"positive\") %>%\n  arrange(desc(percent))\n\n\n\n# Sentiment and Billboard rank ----\nlyric_sentiment %>%\n  filter(sentiment == \"positive\") %>%\n  # Count by song, Billboard rank, and the total number of words\n  count(song, rank, total_words) %>%\n  ungroup() %>%\n  # Use the correct dplyr verb to make two new columns\n  mutate(percent = n / total_words,\n         rank = 10 * floor(rank / 10)) %>%\n  ggplot(aes(as.factor(rank), percent)) +\n  # Make a boxplot\n  geom_boxplot()\n\n\n\n# More on Billboard rank and sentiment scores ----\nlyric_sentiment %>%\n  # Filter for only negative words\n  filter(sentiment == \"negative\") %>%\n  # Count by song, Billboard rank, and the total number of words\n  count(song, rank, total_words) %>%\n  ungroup() %>%\n  # Mutate to make a percent column\n  mutate(percent = n / total_words,\n         rank = 10 * floor(rank / 10)) %>%\n  # Use ggplot to set up a plot with rank and percent\n  ggplot(aes(x = as.factor(rank), y = percent)) +\n  # Make a boxplot\n  geom_boxplot()\n\n\n\n# Sentiment scores by year ----\n# How is negative sentiment changing over time?\nlyric_sentiment %>%\n  # Filter for only negative words\n  filter(sentiment == \"negative\") %>%\n  # Count by song, year, and the total number of words\n  count(song, year, total_words) %>%\n  ungroup() %>%\n  mutate(percent = n / total_words,\n         year = 10 * floor(year / 10)) %>%\n  # Use ggplot to set up a plot with year and percent\n  ggplot(aes(x = as.factor(year), y =  percent))+\n  geom_boxplot()\n\n# How is positive sentiment changing over time?\nlyric_sentiment %>%\n  filter(sentiment == \"positive\") %>%\n  count(song, year, total_words) %>%\n  ungroup() %>%\n  mutate(percent = n / total_words,\n         year = 10 * floor(year / 10)) %>%\n  ggplot(aes(x = as.factor(year), y =  percent))+\n  geom_boxplot()\n\n\n\n\n# Modeling negative sentiment ----\nnegative_by_year <- lyric_sentiment %>%\n  # Filter for negative words\n  filter(sentiment == \"negative\") %>%\n  count(song, year, total_words) %>%\n  ungroup() %>%\n  # Define a new column: percent\n  mutate(percent = n / total_words)\n\n# Specify the model with percent as the response and year as the predictor\nmodel_negative <- lm(percent ~ year, data = negative_by_year)\n\n# Use summary to see the results of the model fitting\nsummary(model_negative)\n\n\n\n# Modeling positive sentiment ----\npositive_by_year <- lyric_sentiment %>%\n  filter(sentiment == \"positive\") %>%\n  # Count by song, year, and total number of words\n  count(song, year, total_words) %>%\n  ungroup() %>%\n  # Define a new column: percent\n  mutate(percent = n / total_words)\n\n# Fit a linear model with percent as the response and year as the predictor\nmodel_positive <- lm(percent ~ year, data = positive_by_year)\n\n# Use summary to see the results of the model fitting\nsummary(model_positive)",
    "created" : 1501879853457.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3526066183",
    "id" : "75B82FAA",
    "lastKnownWriteTime" : 1502017949,
    "last_content_update" : 1502017949933,
    "path" : "C:/Users/d91067/Desktop/datacamp/Sentiment_Analysis_in_R_The_Tidy_Way/Sentiment_Analysis_in_R_The_Tidy_Way.R",
    "project_path" : "Sentiment_Analysis_in_R_The_Tidy_Way.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}