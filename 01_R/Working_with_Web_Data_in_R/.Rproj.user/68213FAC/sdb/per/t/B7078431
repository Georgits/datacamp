{
    "collab_server" : "",
    "contents" : "library(birdnik)\nlibrary(httr)\nlibrary(jsonlite)\nlibrary(rlist)\nlibrary(dplyr)\nlibrary(xml2)\nlibrary(rvest)\n\n\n# Chapter 1:  Downloading Files and Using API Clients -----\n\n# Downloading files and reading them into R\n  # Here are the URLs! As you can see they're just normal strings\n  csv_url <- \"http://s3.amazonaws.com/assets.datacamp.com/production/course_1561/datasets/chickwts.csv\"\n  tsv_url <- \"http://s3.amazonaws.com/assets.datacamp.com/production/course_3026/datasets/tsv_data.tsv\"\n  \n  # Read a file in from the CSV URL and assign it to csv_data\n  csv_data <- read.csv(csv_url)\n  \n  # Read a file in from the TSV URL and assign it to tsv_data\n  tsv_data <- read.delim(tsv_url)\n  \n  # Examine the objects with head()\n  head(csv_data)\n  head(tsv_data)\n\n  \n  \n  \n# Saving raw files to disk\n  # Download the file with download.file()\n  download.file(url = csv_url, destfile = \"feed_data.csv\")\n  \n  # Read it in with read.csv()\n  csv_data <- read.csv(\"feed_data.csv\")\n\n\n\n# Saving formatted files to disk  \n  # Add a new column: square_weight\n  csv_data$square_weight <- csv_data$weight^2\n  \n  # Save it to disk with saveRDS()\n  saveRDS(csv_data, \"modified_feed_data.RDS\")\n  \n  # Read it back in with readRDS()\n  modified_feed_data <- readRDS(\"modified_feed_data.RDS\")\n  \n  # Examine modified_feed_data\n  str(modified_feed_data)\n  \n  \n  \n# Using API clients\n  # Load pageviews\n  library(pageviews)\n  \n  # Get the pageviews for \"Hadley Wickham\"\n  hadley_pageviews <- article_pageviews(project = \"en.wikipedia\", \"Hadley Wickham\")\n  \n  # Examine the resulting object\n  str(hadley_pageviews)\n  \n  \n# Using access tokens\n  # Get the word frequency for \"vector\", using api_key to access it\n  api_key = \"d8ed66f01da01b0c6a0070d7c1503801993a39c126fbc3382\"\n  vector_frequency <- word_frequency(api_key, \"vector\")\n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n  \n\n# Chapter 2: Using httr to interact with APIs directly -----\n\n# GET requests in practice\n  # Make a GET request to http://httpbin.org/get\n  get_result <- GET('http://httpbin.org/get')\n  \n  # Print it to inspect it\n  get_result\n  \n  \n# POST requests in practice  \n  # Load the httr package\n  library(httr)\n  \n  # Make a POST request to http://httpbin.org/post with the body \"this is a test\"\n  post_result <- POST(url = 'http://httpbin.org/post', body = \"this is a test\")\n  \n  # Print it to inspect it\n  post_result\n  \n  \n# Extracting the response\n  url = \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia.org/all-access/all-agents/Hadley_Wickham/daily/20170101/20170102\"\n  # Make a GET request to url and save the results\n  pageview_response <- GET(url = url)\n  \n  # Call content() to retrieve the data the server sent back\n  pageview_data <- content(pageview_response)\n  \n  # Examine the results with str()\n  str(pageview_data)\n  \n  \n  \n  \n  \n# Handling http failures  \n  fake_url <- \"http://google.com/fakepagethatdoesnotexist\"\n  \n  # Make the GET request\n  request_result <- GET(url = fake_url)\n  \n  # Check request_result\n  if (http_error(request_result)) {\n    warning(\"The request failed\")\n  } else {\n    content(request_result)\n  } \n  \n  \n  \n# Constructing queries (Part I)\n  # Construct a directory-based API URL to `http://swapi.co/api`,\n  # looking for person `1` in `people`\n  directory_url <- paste(\"http://swapi.co/api\", \"people\", \"1\", sep = \"/\")\n  \n  # Make a GET call with it\n  result <- GET(directory_url)\n  \n  \n  \n# Constructing queries (Part II)\n  # Create list with nationality and country elements\n  query_params <- list(nationality = \"americans\", \n                       country = \"antigua\")\n  \n  # Make parameter-based call to httpbin, with query_params\n  parameter_response <- GET(\"https://httpbin.org/get\", query = query_params)\n  \n  # Print parameter_response\n  print(parameter_response)\n  \n  \n  \n#  Using user agents\n  # Do not change the url\n  url <- \"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents/Aaron_Halfaker/daily/2015100100/2015103100\"\n  \n  # Add the email address and the test sentence inside user_agent()\n  server_response <- GET(url, user_agent(\"my@email.addres this is a test\"))\n  \n  \n  \n# Rate-limiting\n  # Construct a vector of 2 URLs\n  urls <- c(\"http://fakeurl.com/api/1.0/\", \"http://fakeurl.com/api/2.0/\")\n  \n  for (url in urls) {\n    # Send a GET request to url\n    result <- GET(url)\n    # Delay for 5 seconds between requests\n    Sys.sleep(5)\n  }\n  \n  \n#  Tying it all together\n  \n  get_pageviews <- function(article_title){\n    \n    url <- paste0(\"https://wikimedia.org/api/rest_v1/metrics/pageviews/per-article/en.wikipedia/all-access/all-agents\", \n                  article_title, \n                  \"daily/2015100100/2015103100\", sep = \"/\") \n    \n    response <- GET(url, user_agent(\"my@email.com this is a test\")) \n    \n    if (http_error(response)){ \n      \n      stop(\"the request failed\") \n      \n    } else { \n      \n      result <- content(response) \n      \n      return(result) \n      \n    }\n  }\n  \n  \n  \n  \n# Chapter 3: Handling JSON and XML ----\n  # helper-function\n  rev_history <- function(title, format = \"json\"){\n    if (title != \"Hadley Wickham\") {\n      stop('rev_history() only works for `title = \"Hadley Wickham\"`')\n    }\n    if (format == \"json\") {\n        resp <- readRDS(\"had_rev_json.rds\")\n    } else if (format == \"xml\") {\n      resp <- readRDS(\"had_rev_xml.rds\")\n    } else {\n      stop('Invalid format supplied, try \"json\" or \"xml\"')\n    }\n    resp  \n  }  \n  \n  # Parsing JSON\n  # Get revision history for \"Hadley Wickham\"\n  resp_json <- rev_history(\"Hadley Wickham\")\n  \n  # Check http_type() of resp_json\n  http_type(resp_json)\n  \n  # Examine returned text with content()\n  content(resp_json, as = \"text\")\n  \n  # Parse response with content()\n  content(resp_json, as = \"parsed\")\n  \n  # Parse returned text with fromJSON()\n  fromJSON(content(resp_json, as = \"text\"))\n  \n  \n  \n  \n  # Manipulating parsed JSON\n\n  resp_json <- GET(\"https://en.wikipedia.org/w/api.php?action=query&titles=Hadley%20Wickham&prop=revisions&rvprop=timestamp%7Cuser%7Ccomment%7Ccontent&rvlimit=5&format=json&rvdir=newer&rvstart=2015-01-14T17%3A12%3A45Z&rvsection=0\")\n  # Examine output of this code\n  str(content(resp_json), max.level = 4)\n  \n  # Store revision list\n  revs <- content(resp_json)$query$pages$`41916270`$revisions\n  \n  # Extract the user element\n  user_time <- list.select(revs, user, timestamp)\n  \n  # Print user_time\n  print(user_time)\n  \n  # Stack to turn into a data frame\n  list.stack(user_time)\n  \n  \n  \n  # Reformatting JSON\n\n  # Pull out revision list\n  revs <- content(resp_json)$query$pages$`41916270`$revisions\n  \n  # Extract user and timestamp\n  revs %>%\n    bind_rows() %>%           \n    select(user, timestamp)\n  \n  \n  # Examining XML documents\n  # helper-function\n  rev_history <- function(title, format = \"json\"){\n    if (title != \"Hadley Wickham\") {\n      stop('rev_history() only works for `title = \"Hadley Wickham\"`')\n    }\n    \n    if (format == \"json\") {\n      resp <- readRDS(\"had_rev_json.rds\")\n    } else if (format == \"xml\") {\n      resp <- readRDS(\"had_rev_xml.rds\")\n    } else {\n      stop('Invalid format supplied, try \"json\" or \"xml\"')\n    }\n    resp  \n  }\n  # Get XML revision history\n  resp_xml <- rev_history(\"Hadley Wickham\", format = \"xml\")\n\n  # Check response is XML \n  http_type(resp_xml)\n  \n  # Examine returned text with content()\n  rev_text <- content(resp_xml, as = \"text\")\n  rev_text\n  \n  # Turn rev_text into an XML document\n  rev_xml <- read_xml(rev_text)\n  \n  # Examine the structure of rev_xml\n  xml_structure(rev_xml)\n  \n  \n  \n  # Extracting XML data\n  # Find all nodes using XPATH \"/api/query/pages/page/revisions/rev\"\n  xml_find_all(rev_xml, \"/api/query/pages/page/revisions/rev\")\n  \n  # Find all rev nodes anywhere in document\n  rev_nodes <- xml_find_all(rev_xml, \"//rev\")\n  \n  # Use xml_text() to get text from rev_nodes\n  xml_text(rev_nodes)\n  \n  \n  \n  # Extracting XML attributes\n  \n  # Be careful with the difference between the singular (i.e. without the s, xml_attr()) and plural (xml_attrs()) functions. \n  # If you are extracting a specific named attribute you'll always use the singular xml_attr() and \n  # need to supply two arguments: the XML nodeset and the name of the attribute.\n  \n  # All rev nodes\n  rev_nodes <- xml_find_all(rev_xml, \"//rev\")\n  \n  # The first rev node\n  first_rev_node <- xml_find_first(rev_xml, \"//rev\")\n  \n  # Find all attributes with xml_attrs()\n  xml_attrs(first_rev_node)\n  \n  # Find user attribute with xml_attr()\n  xml_attr(first_rev_node, \"user\")\n  \n  # Find user attribute for all rev nodes\n  xml_attr(rev_nodes, \"user\")\n  \n  # Find anon attribute for all rev nodes\n  xml_attr(rev_nodes, \"anon\")\n  \n  \n  \n  # Wrapup: returning nice API output\n  get_revision_history <- function(article_title){\n    # Get raw revision response\n    rev_resp <- rev_history(article_title, format = \"xml\")\n    \n    # Turn the content() of rev_resp into XML\n    rev_xml <- read_xml(content(rev_resp, \"text\"))\n    \n    # Find revision nodes\n    rev_nodes <- xml_find_all(rev_xml, \"//rev\")\n    \n    # Parse out usernames\n    user <- xml_attr(rev_nodes, \"user\")\n    \n    # Parse out timestamps\n    timestamp <- readr::parse_datetime(xml_attr(rev_nodes, \"timestamp\"))\n    \n    # Parse out content\n    content <- xml_text(rev_nodes)\n    \n    # Return data frame \n    data.frame(user = user,\n               timestamp = timestamp,\n               content = substr(content, 1, 40))\n  }\n  \n  # Call function for \"Hadley Wickham\"\n  get_revision_history(\"Hadley Wickham\")\n  \n  \n  \n  \n# Chapter 4: Web scraping with XPATHs ----\n  # Reading HTML\n  # Hadley Wickham's Wikipedia page\n  test_url <- \"https://en.wikipedia.org/wiki/Hadley_Wickham\"\n  \n  # Read the URL stored as \"test_url\" with read_html()\n  test_xml <- read_html(test_url)\n  \n  # Print test_xml\n  test_xml\n  \n  \n  \n  # Extracting nodes by XPATH\n  test_node_xpath <- \"//*[contains(concat( \\\" \\\", @class, \\\" \\\" ), concat( \\\" \\\", \\\"vcard\\\", \\\" \\\" ))]\"\n  \n  # Use html_node() to grab the node with the XPATH stored as `test_node_xpath`\n  node <- html_node(x = test_xml, xpath = test_node_xpath)\n  \n  # Print the first element of the result\n  node[1]\n  \n  \n  \n  # Extracting names\n  # Extract the name of table_element\n  element_name <- html_name(node)\n  \n  # Print the name\n  element_name\n  \n  \n  \n  # Extracting values\n  second_xpath_val <- \"//*[contains(concat( \\\" \\\", @class, \\\" \\\" ), concat( \\\" \\\", \\\"fn\\\", \\\" \\\" ))]\"\n  \n  # Extract the element of table_element referred to by second_xpath_val and store it as page_name\n  page_name <- html_node(x = node, xpath = second_xpath_val)\n  \n  # Extract the text from page_name\n  page_title <- html_text(page_name)\n  \n  # Print page_title\n  page_title\n  \n  \n  \n  \n  # Extracting tables\n  # Turn table_element into a data frame and assign it to wiki_table\n  wiki_table <- html_table(node)\n  \n  # Print wiki_table\n  wiki_table\n  \n  \n  \n  # Cleaning a data frame\n  # Rename the columns of wiki_table\n  colnames(wiki_table) <- c(\"key\", \"value\")\n  \n  # Remove the empty row from wiki_table\n  cleaned_table <- subset(wiki_table, (!key == \"\" | !value == \"\"))\n  \n  # Print cleaned_table\n  cleaned_table\n  \n  \n  \n  \n# Chapter 5: CSS web scraping in theory ----\n  # Using CSS to scrape nodes\n  test_url <- \"https://en.wikipedia.org/wiki/Hadley_Wickham\"\n  test_xml <- read_html(test_url)\n  \n  # Select the table elements\n  html_nodes(test_xml, css = \"table\")\n  \n  # Select elements with class = \"infobox\"\n  html_nodes(test_xml, css = \".infobox\")\n  \n  # Select elements with id = \"firstHeading\"\n  html_nodes(test_xml, css = \"#firstHeading\")\n  \n  \n  \n  # Scraping names\n  # Extract element with class infobox\n  infobox_element <- html_nodes(test_xml, css = \".infobox\")\n  \n  # Get tag name of infobox_element\n  element_name <- html_name(infobox_element)\n  \n  # Print element_name\n  element_name\n  \n  \n  \n  # Scraping text\n  # Extract element with class fn\n  page_name <- html_node(x = infobox_element, css = \".fn\")\n  \n  # Get contents of page_name\n  page_title <- html_text(page_name)\n  \n  # Print page_title\n  page_title\n  \n  \n  \n  \n  # API calls\n  # Load httr\n  library(httr)\n  \n  # The API url\n  base_url <- \"https://en.wikipedia.org/w/api.php\"\n  \n  # Set query parameters\n  query_params <- list(action = \"parse\", \n                       page = \"Hadley Wickham\", \n                       format = \"xml\")\n  \n  # Get data from API\n  resp <- GET(url = base_url, query = query_params)\n  \n  # Parse response\n  resp_xml <- content(resp)\n  \n  \n  \n  # Extracting information\n  # Load rvest\n  library(rvest)\n  \n  # Read page contents as HTML\n  page_html <- read_html(xml_text(resp_xml))\n  \n  # Extract infobox element\n  infobox_element <- html_node(page_html, css = \".infobox\")\n  \n  # Extract page name element from infobox\n  page_name <- html_node(infobox_element, css = \".fn\")\n  \n  # Extract page name as text\n  page_title <- html_text(page_name)\n  \n  \n  \n  # Normalising information\n  # Your code from earlier exercises\n  wiki_table <- html_table(infobox_element)\n  colnames(wiki_table) <- c(\"key\", \"value\")\n  cleaned_table <- subset(wiki_table, !key == \"\")\n  \n  # Create a dataframe for full name\n  name_df <- data.frame(key = \"Full name\", value = page_title)\n  \n  # Combine name_df with cleaned_table\n  wiki_table2 <- rbind(name_df, cleaned_table)\n  \n  # Print wiki_table\n  wiki_table2\n  \n  \n  \n  # Reproducibility\n  library(httr)\n  library(rvest)\n  library(xml2)\n  \n  get_infobox <- function(title){\n    base_url <- \"https://en.wikipedia.org/w/api.php\"\n    \n    # Change \"Hadley Wickham\" to title\n    query_params <- list(action = \"parse\", \n                         page = title, \n                         format = \"xml\")\n    \n    resp <- GET(url = base_url, query = query_params)\n    resp_xml <- content(resp)\n    \n    page_html <- read_html(xml_text(resp_xml))\n    infobox_element <- html_node(x = page_html, css =\".infobox\")\n    page_name <- html_node(x = infobox_element, css = \".fn\")\n    page_title <- html_text(page_name)\n    \n    wiki_table <- html_table(infobox_element)\n    colnames(wiki_table) <- c(\"key\", \"value\")\n    cleaned_table <- subset(wiki_table, !wiki_table$key == \"\")\n    name_df <- data.frame(key = \"Full name\", value = page_title)\n    wiki_table <- rbind(name_df, cleaned_table)\n    \n    wiki_table\n  }\n  \n  # Test get_infobox with \"Hadley Wickham\"\n  get_infobox(title = \"Hadley Wickham\")\n  \n  # Try get_infobox with \"Ross Ihaka\"\n  get_infobox(title = \"Ross Ihaka\")\n  \n  # Try get_infobox with \"Grace Hopper\"\n    get_infobox(title = \"Grace Hopper\")",
    "created" : 1508617393147.000,
    "dirty" : true,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "27504486",
    "id" : "B7078431",
    "lastKnownWriteTime" : 1508676384,
    "last_content_update" : 1510516813438,
    "path" : "C:/Users/d91067/Desktop/datacamp/01_R/Working_with_Web_Data_in_R/Working_with_Web_Data_in_R.R",
    "project_path" : "Working_with_Web_Data_in_R.R",
    "properties" : {
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}