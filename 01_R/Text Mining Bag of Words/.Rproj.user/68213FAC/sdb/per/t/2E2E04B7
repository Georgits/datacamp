{
    "collab_server" : "",
    "contents" : "# Chapter 1.  Jumping into text mining with bag of words ----\n# Quick taste of text mining ----\n\nnew_text <- \"DataCamp is the first online learning platform that focuses on building the best learning experience specifically for Data Science. We have offices in Boston and Belgium and to date, we trained over 250,000 (aspiring) data scientists in over 150 countries. These data science enthusiasts completed more than 9 million exercises. You can take free beginner courses, or subscribe for $25/month to get access to all premium courses.\"\n\n# Load qdap\nlibrary(qdap)\n\n# Print new_text to the console\nprint(new_text)\n\n# Find the 10 most frequent terms: term_count\nterm_count <- freq_terms(new_text, 10)\n\n# Plot term_count\nplot(term_count)\n\n\n\n\n# Load some text -----\n# Import text data\ntweets <- read.csv(\"coffee.csv\", stringsAsFactors = FALSE)\n\n# View the structure of tweets\nstr(tweets)\n\n# Print out the number of rows in tweets\nnrow(tweets)\n\n# Isolate text from tweets: coffee_tweets\ncoffee_tweets <- tweets$text\n\n\n\n\n# Make the vector a VCorpus object (1) ----\n# Load tm\nlibrary(tm)\n\n# Make a vector source: coffee_source\ncoffee_source <- VectorSource(coffee_tweets)\n\n\n\n\n# Make the vector a VCorpus object (2) ----\n## coffee_source is already in your workspace\n\n# Make a volatile corpus: coffee_corpus\ncoffee_corpus <- VCorpus(coffee_source)\n\n# Print out coffee_corpus\ncoffee_corpus\n\n# Print data on the 15th tweet in coffee_corpus\ncoffee_corpus[[15]]\n\n# Print the content of the 15th tweet in coffee_corpus\ncoffee_corpus[[15]][1]\n\n\n\n# Make a VCorpus from a data frame ----\n# Print example_text to the console\nexample_text\n\n# Create a DataframeSource on columns 2 and 3: df_source\ndf_source <- DataframeSource(example_text[, 2:3])\n\n# Convert df_source to a corpus: df_corpus\ndf_corpus <- VCorpus(df_source)\n\n# Examine df_corpus\ndf_corpus\n\n# Create a VectorSource on column 3: vec_source\nvec_source <- VectorSource(example_text[,3])\n\n# Convert vec_source to a corpus: vec_corpus\nvec_corpus <-  VCorpus(vec_source)\n\n# Examine vec_corpus\nvec_corpus\n\n\n\n# Common cleaning functions from tm ----\n# Create the object: text\ntext <- \"<b>She</b> woke up at       6 A.M. It\\'s so early!  She was only 10% awake and began drinking coffee in front of her computer.\"\n\n# All lowercase\ntolower(text)\n\n# Remove punctuation\nremovePunctuation(text)\n\n# Remove numbers\nremoveNumbers(text)\n\n# Remove whitespace\nstripWhitespace(text)\n\n\n\n\n# Cleaning with qdap ----\n## text is still loaded in your workspace\n\n# Remove text within brackets\nbracketX(text)\n\n# Replace numbers with words\nreplace_number(text)\n\n# Replace abbreviations\nreplace_abbreviation(text)\n\n# Replace contractions\nreplace_contraction(text)\n\n# Replace symbols with words\nreplace_symbol(text)\n\n\n\n# All about stop words ----\n## text is preloaded into your workspace\n# List standard English stop words\nstopwords(\"en\")\n\n# Print text without standard stop words\nremoveWords(text, stopwords(\"en\"))\n\n# Add \"coffee\" and \"bean\" to the list: new_stops\nnew_stops <- c(\"coffee\", \"bean\", stopwords(\"en\"))\n\n# Remove stop words from text\nremoveWords(text, new_stops)\n\n\n\n# Intro to word stemming and stem completion ----\n# Create complicate\ncomplicate <- c(\"complicated\", \"complication\", \"complicatedly\")\n\n# Perform word stemming: stem_doc\nstem_doc <- stemDocument(complicate)\n\n# Create the completion dictionary: comp_dict\ncomp_dict <- c(\"complicate\")\n\n# Perform stem completion: complete_text \ncomplete_text <- stemCompletion(stem_doc, comp_dict)\n\n# Print complete_text\ncomplete_text\n\n\n\n# Word stemming and stem completion on a sentence ----\ntext_data <- \"In a complicated haste, Tom rushed to fix a new complication, too complicatedly.\"\ncomp_dict <- c(\"In\", \"a\", \"complicate\", \"haste\", \"Tom\", \"rush\", \"to\", \"fix\", \"new\", \"too\")\n\n# Remove punctuation: rm_punc\nrm_punc <- removePunctuation(text_data)\n\n# Create character vector: n_char_vec\nn_char_vec <- unlist(strsplit(rm_punc, split = ' '))\n\n# Perform word stemming: stem_doc\nstem_doc <- stemDocument(n_char_vec)\n\n# Print stem_doc\nstem_doc\n\n# Re-complete stemmed document: complete_doc\ncomplete_doc <- stemCompletion(stem_doc, comp_dict)\n\n# Print complete_doc\ncomplete_doc\n\n\n\n\n# Apply preprocessing steps to a corpus ----\n# Notice how the tm package functions do not need content_transformer(), but base R and qdap functions do.\n# Alter the function code to match the instructions\nclean_corpus <- function(corpus){\n  corpus <- tm_map(corpus, stripWhitespace)\n  corpus <- tm_map(corpus, removePunctuation)\n  corpus <- tm_map(corpus, content_transformer(tolower))\n  corpus <- tm_map(corpus, removeWords, c(stopwords(\"en\"), \"coffee\"))\n  corpus <- tm_map(corpus, content_transformer(replace_abbreviation))\n  corpus <- tm_map(corpus, removeNumbers)\n  return(corpus)\n}\n\n# Apply your customized function to the tweet_corp: clean_corp\nclean_corp <- clean_corpus(tweet_corp)\n\n# Print out a cleaned up tweet\nclean_corp[[227]][1]\n\n# Print out the same tweet in original form\ntweet_corp[[227]][1]\n\n\n\n\n# Make a document-term matrix ----\n# Create the dtm from the corpus: coffee_dtm\ncoffee_dtm <- DocumentTermMatrix(clean_corp)\n\n# Print out coffee_dtm data\ncoffee_dtm\n\n# Convert coffee_dtm to a matrix: coffee_m\ncoffee_m <- as.matrix(coffee_dtm)\n\n# Print the dimensions of coffee_m\ndim(coffee_m)\n\n# Review a portion of the matrix\ncoffee_m[148:150,2587:2590]\n\n\n\n# Make a term-document matrix ----\n# Create a TDM from clean_corp: coffee_tdm\ncoffee_tdm <- TermDocumentMatrix(clean_corp)\n\n# Print coffee_tdm data\ncoffee_tdm\n\n# Convert coffee_tdm to a matrix: coffee_m\ncoffee_m <- as.matrix(coffee_tdm)\n\n# Print the dimensions of the matrix\ndim(coffee_m)\n\n# Review a portion of the matrix\ncoffee_m[2587:2590, 148:150]\n\n\n\n# Chapter 2. Word clouds and more interesting visuals ----\n# Frequent terms with tm ----\n## coffee_tdm is still loaded in your workspace\n\n# Create a matrix: coffee_m\ncoffee_m <- as.matrix(coffee_tdm)\n\n# Calculate the rowSums: term_frequency\nterm_frequency <- rowSums(coffee_m)\n\n# Sort term_frequency in descending order\nterm_frequency <- sort(term_frequency, decreasing = TRUE)\n\n# View the top 10 most common words\nterm_frequency[1:10]\n\n# Plot a barchart of the 10 most common words\nbarplot(term_frequency[1:10], col = \"tan\", las = 2)\n\n\n\n# Frequent terms with qdap ---\n# Create frequency\nfrequency <- freq_terms(tweets$text, top = 10, at.least = 3, stopwords = \"Top200Words\")\n\n# Make a frequency barchart\nplot(frequency)\n\n# Create frequency2\nfrequency2 <- freq_terms(tweets$text, top = 10, at.least = 3, tm::stopwords(\"english\"))\n\n# Make a frequency2 barchart\nplot(frequency2)\n\n\n\n\n# A simple word cloud ----\n## term_frequency is loaded into your workspace\n\n# Load wordcloud package\nlibrary(wordcloud)\n\n# Print the first 10 entries in term_frequency[1:10]\nterm_frequency[1:10]\n\n# Create word_freqs\nword_freqs <- data.frame(term = names(term_frequency), num = term_frequency)\n\n# Create a wordcloud for the values in word_freqs\nwordcloud(word_freqs$term, word_freqs$num, max.words = 100, colors = \"red\")\n\n\n\n# Stop words and word clouds ----\n# Add new stop words to clean_corpus()\nclean_corpus <- function(corpus){\n  corpus <- tm_map(corpus, removePunctuation)\n  corpus <- tm_map(corpus, stripWhitespace)\n  corpus <- tm_map(corpus, removeNumbers)\n  corpus <- tm_map(corpus, content_transformer(tolower))\n  corpus <- tm_map(corpus, removeWords, \n                   c(stopwords(\"en\"), \"amp\", \"chardonnay\", \"wine\", \"glass\"))\n  return(corpus)\n}\n\n# Create clean_chardonnay\nclean_chardonnay <- clean_corpus(chardonnay_corp)\n\n# Create chardonnay_tdm\nchardonnay_tdm <- TermDocumentMatrix(clean_chardonnay)\n\n# Create chardonnay_m\nchardonnay_m <- as.matrix(chardonnay_tdm)\n\n# Create chardonnay_words\nchardonnay_words <- rowSums(chardonnay_m)\n\n\n\n# Plot the better word cloud ----\n# Sort the chardonnay_words in descending order\nchardonnay_words <- sort(chardonnay_words, decreasing = TRUE)\n\n# Print the 6 most frequent chardonnay terms\nchardonnay_words[1:6]\n\n# Create chardonnay_freqs\nchardonnay_freqs <- data.frame(term = names(chardonnay_words), num = chardonnay_words)\n\n# Create a wordcloud for the values in word_freqs\nwordcloud(chardonnay_freqs$term, chardonnay_freqs$num, max.words = 50, color = \"red\")\n\n\n\n\n# Improve word cloud colors ----\n# Print the list of colors\ncolors()\n\n# Print the wordcloud with the specified colors\nwordcloud(chardonnay_freqs$term, chardonnay_freqs$num, max.words = 100, color = c(\"grey80\", \"darkgoldenrod1\", \"tomato\"))\n\n\n\n# Use prebuilt color palettes ----\n# List the available colors\ndisplay.brewer.all()\n\n# Create purple_orange\npurple_orange <- brewer.pal(10, \"PuOr\")\n\n# Drop 2 faintest colors\npurple_orange <- purple_orange[-(1:2)]\n\n# Create a wordcloud with purple_orange palette\nwordcloud(chardonnay_freqs$term, chardonnay_freqs$num, max.words = 100, color = purple_orange)\n\n\n\n\n# Find common words ----\n# Create all_coffee\nall_coffee <- paste(coffee_tweets$text, collapse = \" \")\n\n# Create all_chardonnay\nall_chardonnay <- paste(chardonnay_tweets$text, collapse = \" \")\n\n# Create all_tweets\nall_tweets <- c(all_coffee, all_chardonnay)\n\n# Convert to a vector source\nall_tweets <- VectorSource(all_tweets)\n\n# Create all_corpus\nall_corpus <- VCorpus(all_tweets)\n\n\n\n\n\n# Visualize common words ----\n# Clean the corpus\nall_clean <- clean_corpus(all_corpus)\n\n# Create all_tdm\nall_tdm <- TermDocumentMatrix(all_clean)\n\n# Create all_m\nall_m <- as.matrix(all_tdm)\n\n# Print a commonality cloud\ncommonality.cloud(all_m, max.words = 100, colors = \"steelblue1\")\n\n\n\n# Visualize dissimilar words ----\n# Clean the corpus\nall_clean <- clean_corpus(all_corpus)\n\n# Create all_tdm\nall_tdm <- TermDocumentMatrix(all_clean)\n\n# Give the columns distinct names\ncolnames(all_tdm) <- c(\"coffee\", \"chardonnay\")\n\n# Create all_m\nall_m <- as.matrix(all_tdm)\n\n# Create comparison cloud\ncomparison.cloud(all_m, colors = c(\"orange\", \"blue\"), max.words = 50)\n\n\n\n\n# Polarized tag cloud ----\nall_tdm_m <- as.matrix(all_tdm)\n\n# Create common_words\ncommon_words <- subset(all_tdm_m, all_tdm_m[, 1] > 0 & all_tdm_m[, 2] > 0)\n\n# Create difference\ndifference <- abs(common_words[, 1] - common_words[, 2])\n\n# Combine common_words and difference\ncommon_words <- cbind(common_words, difference)\n\n# Order the data frame from most differences to least\ncommon_words <- common_words[order(common_words[, 3], decreasing = TRUE), ]\n\n# Create top25_df\ntop25_df <- data.frame(x = common_words[1:25, 1], \n                       y = common_words[1:25, 2], \n                       labels = rownames(common_words[1:25, ]))\n\n# Create the pyramid plot\npyramid.plot(top25_df$x, top25_df$y,\n             labels = top25_df$labels, gap = 8,\n             top.labels = c(\"Chardonnay\", \"Words\", \"Coffee\"),\n             main = \"Words in Common\", laxlab = NULL, \n             raxlab = NULL, unit = NULL)\n\n\n\n\n\n# Visualize word networks ----\n# Word association\nword_associate(coffee_tweets$text, match.string = c(\"barista\"), \n               stopwords = c(Top200Words, \"coffee\", \"amp\"), \n               network.plot = TRUE, cloud.colors = c(\"gray85\", \"darkred\"))\n\n# Add title\ntitle(main = \"Barista Coffee Tweet Associations\")\n\n\n\n\n# Chapter 3. Adding to your tm skills ----\n# Distance matrix and dendrogram ----\n# Create dist_rain\ndist_rain <- dist(rain[,2])\n\n# View the distance matrix\ndist_rain\n\n# Create hc\nhc <- hclust(dist_rain)\n\n# Plot hc\nplot(hc, labels = rain$city)\n\n\n\n\n# Make a distance matrix and dendrogram from a TDM ----\n# Print the dimensions of tweets_tdm\ndim(tweets_tdm)\n\n# Create tdm1\ntdm1 <- removeSparseTerms(tweets_tdm, sparse = 0.95)\n\n# Create tdm2\ntdm2 <- removeSparseTerms(tweets_tdm, sparse = 0.975)\n\n# Print tdm1\ntdm1\n\n# Print tdm2\ntdm2\n\n\n\n# Put it all together: a text based dendrogram ----\n# Create tweets_tdm2\ntweets_tdm2 <- removeSparseTerms(tweets_tdm, sparse = 0.975)\n\n# Create tdm_m\ntdm_m <- as.matrix(tweets_tdm2)\n\n# Create tdm_df\ntdm_df <- as.data.frame(tdm_m)\n\n# Create tweets_dist\ntweets_dist <- dist(tdm_df)\n\n# Create hc\nhc <- hclust(tweets_dist)\n\n# Plot the dendrogram\nplot(hc)\n\n\n\n\n# Dendrogram aesthetics ----\n# Load dendextend\nlibrary(dendextend)\n\n# Create hc\nhc <- hclust(tweets_dist)\n\n# Create hcd\nhcd <- as.dendrogram(hc)\n\n# Print the labels in hcd\nlabels(hcd)\n\n# Change the branch color to red for \"marvin\" and \"gaye\"\nhcd <- branches_attr_by_labels(hcd, c(\"marvin\", \"gaye\"), color = \"red\")\n\n# Plot hcd\nplot(hcd, main = \"Better Dendrogram\")\n\n# Add cluster rectangles \nrect.dendrogram(hcd, k = 2, border = \"grey50\")\n\n\n\n\n\n# Using word association ----\n# Create associations\nassociations <- findAssocs(tweets_tdm, \"venti\", 0.2)\n\n# View the venti associations\nassociations\n\n# Create associations_df\nassociations_df <- list_vect2df(associations)[, 2:3]\n\n# Plot the associations_df values (don't change this)\nggplot(associations_df, aes(y = associations_df[, 1])) + \n  geom_point(aes(x = associations_df[, 2]), \n             data = associations_df, size = 3) + \n  theme_gdocs()\n\n\n\n# Changing n-grams ----\n# Make tokenizer function \ntokenizer <- function(x) \n  NGramTokenizer(x, Weka_control(min = 2, max = 2))\n\n# Create unigram_dtm\nunigram_dtm <- DocumentTermMatrix(text_corp)\n\n# Create bigram_dtm\nbigram_dtm <- DocumentTermMatrix(text_corp, control = list(tokenize = tokenizer))\n\n# Examine unigram_dtm\nunigram_dtm\n\n# Examine bigram_dtm\nbigram_dtm\n\n\n\n# How do bigrams affect word clouds? ----\n# Create bigram_dtm_m\nbigram_dtm_m <- as.matrix(bigram_dtm)\n\n# Create freq\nfreq <- colSums(bigram_dtm_m)\n\n# Create bi_words\nbi_words <- names(freq)\n\n# Examine part of bi_words\nbi_words[2577:2587]\n\n# Plot a wordcloud\nwordcloud(bi_words, freq = freq, max.words = 15)\n\n\n\n# Changing frequency weights ----\n# Create tf_tdm\ntf_tdm <- TermDocumentMatrix(text_corp)\n\n# Create tfidf_tdm\ntfidf_tdm <- TermDocumentMatrix(text_corp, control = list(weighting = weightTfIdf))\n\n# Create tf_tdm_m\ntf_tdm_m <- as.matrix(tf_tdm)\n\n# Create tfidf_tdm_m \ntfidf_tdm_m <- as.matrix(tfidf_tdm)\n\n# Examine part of tf_tdm_m\ntf_tdm_m[508:509,5:10]\n\n# Examine part of tfidf_tdm_m\ntfidf_tdm_m[508:509,5:10]\n\n\n\n\n\n# Capturing metadata in tm ----\n# Add author to custom reading list\ncustom_reader <- readTabular(mapping = list(content = \"text\", \n                                            id = \"num\",\n                                            author = \"screenName\",\n                                            date = \"created\"\n))\n\n# Make corpus with custom reading\ntext_corpus <- VCorpus(\n  DataframeSource(tweets), \n  readerControl = list(reader = custom_reader)\n)\n\n# Clean corpus\ntext_corpus <- clean_corpus(text_corpus)\n\n# Print data\ntext_corpus[[1]][1]\n\n# Print metadata\ntext_corpus[[1]][2]\n\n\n\n\n# Chapter 4. Battle of the tech giants for talent ----\n# Step 1: Problem definition----\n# Step 2: Identifying the text sources ----\n# Print the structure of amzn\nstr(amzn)\n\n# Create amzn_pros\namzn_pros <- amzn$pros\n\n# Create amzn_cons\namzn_cons <- amzn$cons\n\n# Print the structure of goog\nstr(goog)\n\n# Create goog_pros\ngoog_pros <- goog$pros\n\n# Create goog_cons\ngoog_cons <- goog$cons\n\n\n\n# Text organization ----\n\n# applies a series of qdap functions to a text vector\nqdap_clean <- function(x){\n  x <- replace_abbreviation(x)\n  x <- replace_contraction(x)\n  x <- replace_number(x)\n  x <- replace_ordinal(x)\n  x <- replace_ordinal(x)\n  x <- replace_symbol(x)\n  x <- tolower(x)\n  return(x)\n}\n\n\n# applies a series of tm functions to a corpus object\ntm_clean <- function(corpus){\n  corpus <- tm_map(corpus, removePunctuation)\n  corpus <- tm_map(corpus, stripWhitespace)\n  corpus <- tm_map(corpus, removeWords, \n                   c(stopwords(\"en\"), \"Google\", \"Amazon\", \"company\"))\n  return(corpus)\n}\n\n# Alter amzn_pros\namzn_pros <- qdap_clean(amzn_pros)\n\n# Alter amzn_cons\namzn_cons <- qdap_clean(amzn_cons)\n\n# Create az_p_corp \naz_p_corp <- VCorpus(VectorSource(amzn_pros))\n\n# Create az_c_corp\naz_c_corp <- VCorpus(VectorSource(amzn_cons))\n\n# Create amzn_pros_corp\namzn_pros_corp <- tm_clean(az_p_corp)\n\n# Create amzn_cons_corp\namzn_cons_corp <- tm_clean(az_c_corp)\n\n\n\n\n\n# Working with Google reviews ----\n# Apply qdap_clean to goog_pros\ngoog_pros <- qdap_clean(goog_pros)\n\n# Apply qdap_clean to goog_cons\ngoog_cons <- qdap_clean(goog_cons)\n\n# Create goog_p_corp\ngoog_p_corp <- VCorpus(VectorSource(goog_pros))\n\n# Create goog_c_corp\ngoog_c_corp <- VCorpus(VectorSource(goog_cons))\n\n# Create goog_pros_corp\ngoog_pros_corp <- tm_clean(goog_p_corp)\n\n# Create goog_cons_corp\ngoog_cons_corp <- tm_clean(goog_c_corp)\n\n\n\n\n\n# Steps 4 & 5: Feature extraction & analysis ----\n# Feature extraction & analysis: amzn_pros ----\nlibrary(Rweka)\ntokenizer <- function(x) \n  NGramTokenizer(x, Weka_control(min = 2, max = 2))\n\n# Create amzn_p_tdm\namzn_p_tdm <- TermDocumentMatrix(amzn_pros_corp, list(tokenize = tokenizer))\n\n# Create amzn_p_tdm_m\namzn_p_tdm_m <- as.matrix(amzn_p_tdm)\n\n# Create amzn_p_freq\namzn_p_freq <- rowSums(amzn_p_tdm_m)\n\n# Plot a wordcloud using amzn_p_freq values\nwordcloud(names(amzn_p_freq), max.words = 25, color = \"blue\")\n\n\n\n\n# Feature extraction & analysis: amzn_cons ----\n# Create amzn_c_tdm\namzn_c_tdm <- TermDocumentMatrix(amzn_cons_corp, list(tokenize = tokenizer))\n\n# Create amzn_c_tdm_m\namzn_c_tdm_m <- as.matrix(amzn_c_tdm)\n\n# Create amzn_c_freq\namzn_c_freq <- rowSums(amzn_c_tdm_m)\n\n# Plot a wordcloud of negative Amazon bigrams\nwordcloud(names(amzn_c_freq), max.words = 25, color = \"blue\")\n\n\n\n\n# amzn_cons dendrogram ----\n# Create amzn_c_tdm\namzn_c_tdm <- TermDocumentMatrix(amzn_cons_corp, control = list(tokenize = tokenizer))\n\n# Print amzn_c_tdm to the console\namzn_c_tdm\n\n# Create amzn_c_tdm2 by removing sparse terms \namzn_c_tdm2 <- removeSparseTerms(amzn_c_tdm, sparse = 0.993) \n\n# Create hc as a cluster of distance values\nhc <- hclust(dist(amzn_c_tdm2, method = \"euclidean\"), method = \"complete\")\n\n# Produce a plot of hc\nplot(hc)\n\n\n\n# Word association ----\n# Create amzn_p_tdm\namzn_p_tdm <- TermDocumentMatrix(amzn_pros_corp, control = list(tokenize = tokenizer))\n\n# Create amzn_p_m\namzn_p_m <- as.matrix(amzn_p_tdm)\n\n# Create amzn_p_freq\namzn_p_freq <- rowSums(amzn_p_m)\n\n# Create term_frequency\nterm_frequency <- sort(amzn_p_freq, decreasing = TRUE)\n\n# Print the 5 most common terms\nterm_frequency[1:5]\n\n# Find associations with fast paced\nfindAssocs(amzn_p_tdm, \"fast paced\", 0.2)\n\n\n\n\n# Quick review of Google reviews ----\n# We've provided you with a corpus all_goog_corpus, which has the 500 positive and 500 negative reviews for Google. \n# Wie dies geschieht s. oben, bei all_tweets\n\n# Create all_goog_corp\nall_goog_corp <- tm_clean(all_goog_corpus)\n\n# Create all_tdm\nall_tdm <- TermDocumentMatrix(all_goog_corp)\n\n# Name the columns of all_tdm\ncolnames(all_tdm) <- c(\"Goog_Pros\", \"Goog_Cons\")\n\n# Create all_m\nall_m <- as.matrix(all_tdm)\n\n# Build a comparison cloud\ncomparison.cloud(all_m, max.word = 100, colors = c(\"#F44336\", \"#2196f3\"))\n\n\n\n\n\n# Cage match! Amazon vs. Google pro reviews ----\n# Create common_words\ncommon_words <- subset(all_tdm_m, all_tdm_m[, 1] > 0 & all_tdm_m[, 2] > 0)\n\n# Create difference\ndifference <- abs(common_words[, 1] - common_words[, 2])\n\n# Add difference to common_words\n  common_words <- cbind(common_words, difference)\n\n# Order the data frame from most differences to least\ncommon_words <- common_words[order(common_words[, 3], decreasing = TRUE), ]\n\n# Create top15_df\ntop15_df <- data.frame(\n  x = common_words[1:15, 1],\n  y = common_words[1:15, 2],\n  labels = rownames(common_words[1:15, ]))\n\n# Create the pyramid plot\npyramid.plot(top15_df$x, top15_df$y, \n             labels = top15_df$labels, gap = 12, \n             top.labels = c(\"Amzn\", \"Pro Words\", \"Google\"), \n             main = \"Words in Common\", unit = NULL)\n\n\n\n\n\n\n# Cage match, part 2! Negative reviews ----\n# all_goog_corpus muss nun mit cons (negativen)-begriffsüpaaren gebildet werden.\n# Create common_words\ncommon_words <- subset(all_tdm_m, all_tdm_m[, 1] > 0 & all_tdm_m[, 2] > 0)\n\n# Create difference\ndifference <- abs(common_words[, 1] - common_words[, 2])\n\n# Bind difference to common_words\ncommon_words <- cbind(common_words, difference)\n\n# Order the data frame from most differences to least\ncommon_words <- common_words[order(common_words[, 3], decreasing = TRUE), ]\n\n# Create top15_df\ntop15_df <- data.frame(\n  x = common_words[1:15, 1],\n  y = common_words[1:15, 2],\n  labels = rownames(common_words[1:15, ]))\n\n# Create the pyramid plot\npyramid.plot(top15_df$x, top15_df$y, \n             labels = top15_df$labels, gap = 12, \n             top.labels = c(\"Amzn\", \"Cons Words\", \"Google\"), \n             main = \"Words in Common\", unit = NULL)\n",
    "created" : 1498381988766.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "4071428896",
    "id" : "2E2E04B7",
    "lastKnownWriteTime" : 1499009998,
    "last_content_update" : 1502481722782,
    "path" : "C:/Users/d91067/Desktop/datacamp/01_R/Text Mining Bag of Words/Text Mining Bag of Words.R",
    "project_path" : "Text Mining Bag of Words.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}