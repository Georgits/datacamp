{
    "collab_server" : "",
    "contents" : "library(gstat)\nlibrary(automap)\nca_geo <- readRDS(\"ca_geo.rds\")\ngeo_bounds <- readRDS(\"ca_geo_bounds.rds\")\npreston_crime <- readRDS(\"pcrime-spatstat.rds\")\npreston_osm <- readRDS(\"osm_preston_gray.rds\")\n\n\n\n# Chapter 1. Introduction\n# Simple spatial principles ----\n# The number of points to create\nn <- 200\n\n# Set the range\nxmin <- 0\nxmax <- 1\nymin <- 0\nymax <- 2\n\n# Sample from a Uniform distribution\nx <- runif(n, xmin, xmax)\ny <- runif(n, ymin, ymax)\n\n\n\n# Plotting areas ----\n# See pre-defined variables\nls.str()\n\n# Plot points and a rectangle\n\nmapxy <- function(a = NA){\n  plot(x, y, asp = a)\n  rect(xmin, ymin, xmax, ymax)\n}\n\nmapxy(a = 1)\n\n\n\n\n# Uniform in a circle ----\n# Load the spatstat package\nlibrary(spatstat)\n\n# Create this many points, in a circle of this radius\nn_points <- 300\nradius <- 10\n\n# Generate uniform random numbers up to radius-squared\nr_squared <- runif(n_points, 0, radius^2)\nangle <- runif(n_points, 0, 2*pi)\n\n# Take the square root of the values to get a uniform spatial distribution\nx <- sqrt(r_squared) * cos(angle)\ny <- sqrt(r_squared) * sin(angle)\n\nplot(disc(radius)); points(x, y)\n\n\n\n\n\n\n# Quadrat count test for uniformity ----\n# Some variables have been pre-defined\nls.str()\n\n# Set coordinates and window\nppxy <- ppp(x = x, y = y, window = disc(radius))\n\n# Test the point pattern\nqt <- quadrat.test(ppxy)\n\n# Inspect the results\nplot(qt)\nprint(qt)\n\n\n\n\n# Creating a uniform point pattern with spatstat ----\n# Create a disc of radius 10\ndisc10 <- disc(10)\n\n# Compute the rate as count divided by area\nlambda <- 500 / area(disc10)\n\n# Create a point pattern object\nppois <- rpoispp(lambda = lambda, win = disc10)\n\n# Plot the Poisson point pattern\nplot(ppois)\n\n\n\n\n# Simulating clustered and inhibitory patterns ----\n# Create a disc of radius 10\ndisc10 <- disc(10)\n\n# Generate clustered points from a Thomas process\nset.seed(123)\np_cluster <- rThomas(kappa = 0.35, scale = 1, mu = 3, win = disc10)\nplot(p_cluster)\n\n# Run a quadrat test\n  quadrat.test(p_cluster, alternative = \"clustered\")\n\n# Regular points from a Strauss process\nset.seed(123)\np_regular <- rStrauss(beta = 2.9, gamma = 0.025, R = .5, W = disc10)\nplot(p_regular)\n\n# Run a quadrat test\nquadrat.test(p_regular, alternative = \"regular\")\n\n\n\n# Nearest-neighbor distributions ----\n# Point patterns are pre-defined\np_poisson; p_regular\n\n# Calc nearest-neighbor distances for Poisson point data\nnnd_poisson <- nndist(p_poisson)\n\n# Draw a histogram of nnds\nhist(nnd_poisson)\n\n# Estimate G(r)\nG_poisson <- Gest(p_poisson)\n\n# Plot G(r) vs. r\nplot(G_poisson)\n\n# Repeat for regular point data\nnnd_regular <- nndist(p_regular)\nhist(nnd_regular)\nG_regular <- Gest(p_regular)\nplot(G_regular)\n\n\n\n\n\n# Other point pattern distribution functions ----\n# Point patterns are pre-defined\np_poisson; p_cluster; p_regular\n\n# Estimate the K-function for the Poisson points\nK_poisson <- Kest(p_poisson, correction = \"border\")\n\n# The default plot shows quadratic growth\nplot(K_poisson, . ~ r)\n\n# Subtract pi * r ^ 2 from the Y-axis and plot\nplot(K_poisson, . - pi * r ^ 2 ~ r)\n\n# Compute envelopes of K under random locations\nK_cluster_env <- envelope(p_cluster, Kest, correction = \"border\")\n\n# Insert the full formula to plot K minus pi * r^2\nplot(K_cluster_env, . - pi * r ^ 2 ~ r)\n\n# Repeat for regular data\nK_regular_env <- envelope(p_regular, Kest, correction = \"border\")\nplot(K_regular_env, . - pi * r ^ 2 ~ r)\n\n\n\n\n# Tree location pattern ----\nquadrat.test(redoak)\n\n\n\n\n\n\n# Chapter 2. Point Pattern Analysis ----\n# Crime in Preston ----\n# Load the spatstat package\nlibrary(spatstat)\nlibrary(raster)\n\n# Get some summary information on the dataset\nsummary(preston_crime)\n\n# Get a table of marks\ntable(marks(preston_crime))\n\n# Define a function to create a map\npreston_map <- function(cols = c(\"green\",\"red\"), cex = c(1, 1), chars = c(1, 1)) {\n  plotRGB(preston_osm) # from the raster package\n  plot(preston_crime, cols = cols, chars = chars, cex = cex, add = TRUE, show.window = TRUE)\n}\n\n# Draw the map with colors, sizes and plot character\npreston_map(\n  cols = c(\"black\", \"red\"), \n  cex = c(0.5, 1), \n  chars = c(19,19)\n)\n\n\n\n\n# Violent crime proportion estimation ----\n# preston_crime has been pre-defined\npreston_crime\n\n# Use the split function to show the two point patterns\ncrime_splits <- split(preston_crime)\n\n# Plot the split crime\nplot(crime_splits)\n\n# Compute the densities of both sets of points\ncrime_densities <- density(crime_splits)\n\n# Calc the violent density divided by the sum of both\nfrac_violent_crime_density <- crime_densities[[2]] / \n  (crime_densities[[1]] + crime_densities[[2]])\n\n# Plot the density of the fraction of violent crime\nplot(frac_violent_crime_density)\n\n\n\n\n# Bandwidth selection ----\n# The first step is to compute the optimal bandwidth for kernel smoothing under the segregation model. \n# spseg() will scan over a range of bandwidths and compute a test statistic using a cross-validation method. \n# The bandwidth that maximizes this test statistic is the one to use. The returned value from spseg() in this case is a list, \n# and its hcv element is the one the contains this best bandwidth.\n\n\nlibrary(spatialkernel)\n\n# Scan from 500m to 1000m in steps of 50m\nbw_choice <- spseg(\n  preston_crime, \n  h = seq(500, 1000, by = 50),\n  opt = 1)\n\n# Plot the results and highlight the best bandwidth\nplotcv(bw_choice); abline(v = bw_choice$hcv, lty = 2, col = \"red\")\n\n# Print the best bandwidth\nprint(bw_choice$hcv)\n\n# Now you know the optimal smoothing parameter, you can do some kernel smoothing simulations. \n\n\n\n# Segregation probabilities ----\n# The second step is to compute the probabilities for violent and non-violent crimes as a smooth surface, \n# as well as the p-values for a point-wise test of segregation. This is done by calling spseg() with opt = 3 and \n# a fixed bandwidth parameter h.\n# Set the correct bandwidth and run for 10 simulations only\nseg10 <- spseg(\n  pts = preston_crime, \n  h = 800,\n  opt = 3,\n  ntest = 10, \n  proc =FALSE)\n# Plot the segregation map for violent crime\nplotmc(seg10, \"Violent crime\")\n\n# Plot seg, the result of running 1000 simulations\nplotmc(seg, \"Violent crime\")\n\n\n\n# Mapping segregation ----\n# The seg object is a list with several components. The X and Y coordinates of the grid are stored in the $gridx and $gridy \n# elements. The probabilities of each class of data (violent or non-violent crime) are in a matrix element $p with \n# a column for each class. The p-value of the significance test is in a similar matrix element called $stpvalue. \n# Rearranging columns of these matrices into a grid of values can be done with R's matrix() function. From there you can \n# construct list objects with a vector $x of X-coordinates, $y of Y-coordinates, and $z as the matrix. You can then feed \n# this to image() or contour() for visualization.\n\n# Inspect the structure of the spatial segregation object\nstr(seg)\n\n# Get the number of columns in the data so we can rearrange to a grid\nncol <- length(seg$gridx)\n\n# Rearrange the probability column into a grid\nprob_violent <- list(x = seg$gridx,\n                     y = seg$gridy,\n                     z = matrix(seg$p[, \"Violent crime\"],\n                                ncol = ncol))\nimage(prob_violent)\n\n# Rearrange the p-values, but choose a p-value threshold\np_value <- list(x = seg$gridx,\n                y = seg$gridy,\n                z = matrix(seg$stpvalue[, \"Violent crime\"] < 0.05,\n                           ncol = ncol))\nimage(p_value)\n\n# Create a mapping function\nsegmap <- function(prob_list, pv_list, low, high){\n  \n  # background map\n  plotRGB(preston_osm)\n  \n  # p-value areas\n  image(pv_list, \n        col = c(\"#00000000\", \"#FF808080\"), add = TRUE) \n  \n  # probability contours\n  contour(prob_list,\n          levels = c(low, high),\n          col = c(\"#206020\", \"red\"),\n          labels = c(\"Low\", \"High\"),\n          add = TRUE)\n  \n  # boundary window\n  plot(Window(preston_crime), add = TRUE)\n}\n\n# Map the probability and p-value\nsegmap(prob_violent, p_value, 0.05, 0.15)\n\n\n\n\n# Sasquatch data ----\n# Get a quick summary of the dataset\nsummary(sasq)\n\n# Plot unmarked points\nplot(unmark(sasq))\n\n# Plot the points using a circle sized by date\nplot(sasq, which.marks = \"date\")\n\n\n# Temporal pattern of bigfoot sightings ----\n# Show the available marks\nnames(marks(sasq))\n\n# Histogram the dates of the sightings, grouped by year\nhist(marks(sasq)$date, \"year\", freq = TRUE)\n\n# Plot and tabulate the calendar month of all the sightings\nplot(table(marks(sasq)$month))\n\n# Split on the month mark\nsaqs_by_month <- split(sasq, \"month\", un = TRUE)\n\n# Plot monthly maps\nplot(saqs_by_month)\n\n# Plot smoothed versions of the above split maps\nplot(density(saqs_by_month))\n\n\n# Preparing data for space-time clustering ----\nlibrary(splancs)\nlibrary(spatstat)\n# To do a space-time clustering test with stmctest() from the splancs package, you first need to convert parts \n# of your ppp object. Functions in splancs tend to use matrix data instead of data frames.\n\n# Get a matrix of event coordinates\nsasq_xy <- as.matrix(coords(sasq))\n\n# Check the matrix has two columns\ndim(sasq_xy)\n\n# Get a vector of event times\nsasq_t <- marks(sasq)$date\n\n# Extract a two-column matrix from the ppp object\nsasq_poly <- as.matrix(as.data.frame(Window(sasq)))\ndim(sasq_poly)\n\n# Set the time limit to 1 day before and 1 day after the range of times\ntlimits <- range(sasq_t) + c(-1, 1)\n\n# Scan over 400m intervals from 100m to 20km\ns <- seq(100, 20000, by = 400)\n\n# Scan over 14 day intervals from one week to 31 weeks\ntm <- seq(7, 7 * 31, by = 14)\n\n\n\n\n# Monte-carlo test of space-time clustering -----\n# Any space-time clustering in a data set will be removed if you randomly rearrange the dates of the data points. \n# The stmctest() function computes a clustering test statistic for your data based on the space-time K-function - how many \n# points are within a spatial and temporal window of a point of the data.\n\n# The output from stmctest() is a list with a single t0 which is the test statistic for your data, and a vector of t from \n# the simulations.\n# Run 999 simulations \nsasq_mc <- stmctest(sasq_xy, sasq_t, sasq_poly, tlimits, s, tm, nsim = 999, quiet = TRUE)\nnames(sasq_mc)\n\n# Histogram the simulated statistics and add a line at the data value\nggplot(data.frame(sasq_mc), aes(x = t)) +\n  geom_histogram(binwidth = 1e13) +\n  geom_vline(aes(xintercept = t0))\n\n# Compute the p-value as the proportion of tests greater than the data\nsum(sasq_mc$t > sasq_mc$t0) / 1000\n\n\n\n\n\n# Chapter 3. Areal Statistics ----\n# London EU referendum data ----\nlibrary(raster)\n# See what information we have for each borough\nsummary(london_ref)\n\n# Which boroughs voted to \"Leave\"?\nlondon_ref$NAME[london_ref$Leave > london_ref$Remain]\n\n# Plot a map of the percentage that voted \"Remain\"\nspplot(london_ref, zcol = \"Pct_Remain\")\n\n\n\n# Cartogram ----\n# Use the cartogram and rgeos packages\nlibrary(cartogram)\nlibrary(rgeos)\n\n# Make a scatterplot of electorate vs borough area\nnames(london_ref)\nplot(london_ref$Electorate, gArea(london_ref, byid = TRUE))\n\n# Make a cartogram, scaling the area to the electorate\ncarto_ref <- cartogram(london_ref, \"Electorate\")\nplot(carto_ref)\n\n# Check the linearity of the electorate-area plot\nplot(carto_ref$Electorate, gArea(carto_ref, byid = TRUE))\n\n# Make a fairer map of the Remain percentage\nspplot(carto_ref, \"Pct_Remain\")\n\n\n\n# Spatial autocorrelation test ----\n# Use the spdep package\nlibrary(spdep)\n\n# Make neighbor list\nborough_nb <- poly2nb(london_ref)\n\n# Get center points of each borough\nborough_centers <- coordinates(london_ref)\n\n# Show the connections\nplot(london_ref); plot(borough_nb, borough_centers, add = TRUE)\n\n# Map the total pop'n\nspplot(london_ref, zcol = \"TOTAL_POP\")\n\n# Run a Moran I test on total pop'n\nmoran.test(\n  london_ref$TOTAL_POP, \n  nb2listw(borough_nb)\n)\n\n# Map % Remain\nspplot(london_ref, zcol = \"Pct_Remain\")\n\n# Run a Moran I MC test on % Remain\nmoran.mc(\n  london_ref$Pct_Remain, \n  nb2listw(borough_nb), \n  nsim = 999\n)\n\n\n\n\n\n# London health data ----\nlibrary(sp)\n# Get a summary of the data set\nsummary(london)\n\n# Map the OBServed number of flu reports\nspplot(london, \"Flu_OBS\")\n\n# Compute the overall incidence of flu\nr <- sum(london$Flu_OBS) / sum(london$TOTAL_POP)\nr\n\n# Calculate the expected number for each borough\nlondon$Flu_EXP <- london$TOTAL_POP * r\n\n# Calculate the ratio of OBServed to EXPected\nlondon$Flu_SMR <- london$Flu_OBS / london$Flu_EXP\n\n# Map the SMR\nspplot(london, \"Flu_SMR\")\n\n\n\n\n# Binomial confidence intervals ----\nlibrary(sp)\nlibrary(ggplot)\n\n# For the binomial statistics function\nlibrary(epitools)\n\n# Get CI from binomial distribution\nflu_ci <- binom.exact(london$Flu_OBS, london$TOTAL_POP)\n\n# Add borough names\nflu_ci$NAME <- london$NAME\n\n# Calculate London rate, then compute SMR\nr <- sum(london$Flu_OBS) / sum(london$TOTAL_POP)\nflu_ci$SMR <- flu_ci$proportion / r\n\n# Subset the high SMR data\nflu_high <- flu_ci[flu_ci$SMR > 1, ]\n\n# Plot estimates with CIs\nlibrary(ggplot2)\nggplot(flu_high, aes(x = NAME, y = proportion / r,\n                     ymin = lower / r, ymax = upper / r)) +\n  geom_pointrange() +\n  theme(axis.text.x = element_text(angle = 45, hjust = 1))\n\n\n\n\n\n# Exceedence probabilities ----\n# Probability of a binomial exceeding a multiple\nbinom.exceed <- function(observed, population, expected, e){\n  1 - pbinom(e * expected, population, prob = observed / population)\n}\n\n# Compute P(rate > 2)\nlondon$Flu_gt_2 <- binom.exceed(\n  observed = london$Flu_OBS,\n  population = london$TOTAL_POP,\n  expected = london$Flu_EXP,\n  e = 2)\n\n# Use a 50-color palette that only starts changing at around 0.9\npal <- c(\n  rep(\"#B0D0B0\", 40),\n  colorRampPalette(c(\"#B0D0B0\", \"orange\"))(5), \n  colorRampPalette(c(\"orange\", \"red\"))(5)\n)\n\n# Plot the P(rate > 2) map\nspplot(london, \"Flu_gt_2\", col.regions = pal, at = seq(0, 1, len = 50))\n\n\n\n\n\n# A Poisson GLM ----\n# Fit a poisson GLM.\n# To cope with count data coming from populations of different sizes, you specify an offset argument. \n# This adds a constant term for each row of the data in the model. The log of the population is used in the offset term.\nmodel_flu <- glm(\n  Flu_OBS ~ HealthDeprivation, \n  offset = log(TOTAL_POP), \n  data = london, \n  family = poisson)\n\n# Is HealthDeprivation significant?\nsummary(model_flu)\n\n# Put residuals into the spatial data.\nlondon$Flu_Resid <- residuals(model_flu)\n\n# Map the residuals using spplot\nspplot(london, \"Flu_Resid\")\n\n\n\n# Residuals ----\n# Compute the neighborhood structure.\nlibrary(spdep)\nborough_nb <- poly2nb(london)\n\n# Test spatial correlation of the residuals.\nmoran.mc(london$Flu_Resid, listw = nb2listw(borough_nb), nsim = 999)\n\n\n\n\n# Fit a Bayesian GLM ----\n# Use R2BayesX\nlibrary(R2BayesX)\n\n# Fit a GLM\nmodel_flu <- glm(Flu_OBS ~ HealthDeprivation, offset = log(TOTAL_POP),\n                 data = london, family = poisson)\n\n# Summarize it                    \nsummary(model_flu)\n\n# Calculate coeff confidence intervals\nconfint(model_flu)\n\n# Fit a Bayesian GLM\n# he syntax for bayesx() is similar, but the offset has to be specified explicitly from the data frame, \n# the family name is in quotes, and the spatial data frame needs to be turned into a plain data frame.\nbayes_flu <- bayesx(Flu_OBS ~ HealthDeprivation, offset = log(london$TOTAL_POP), \n                    family = \"poisson\", data = data.frame(london), \n                    control = bayesx.control(seed = 17610407))\n\n# Summarize it                    \n# Plot the samples from the Bayesian model. On the left is the \"trace\" of samples in sequential order, \n# and on the right is the parameter density. For this model there is an intercept and a slope for the \n# Health Deprivation score. The parameter density should correspond with the parameter summary.\nsummary(bayes_flu)\n\n# Look at the samples from the Bayesian model\nplot(samples(bayes_flu))\n\n\n\n\n\n# Adding a spatially autocorrelated effect ----\n# Compute adjacency objects\nborough_nb <- poly2nb(london)\nborough_gra <- nb2gra(borough_nb)\n\n# Fit spatial model\nflu_spatial <- bayesx(\n  Flu_OBS ~ HealthDeprivation + sx(i, bs = \"spatial\", map = borough_gra),\n  offset = log(london$TOTAL_POP),\n  family = \"poisson\", data = data.frame(london), \n  control = bayesx.control(seed = 17610407)\n)\n\n# Summarize the model\nsummary(flu_spatial)\n\n\n\n\n# Mapping the spatial effects ----\n# Summarise the model\nsummary(flu_spatial)\n\n# Map the fitted spatial term only\nlondon$spatial <- fitted(flu_spatial, term = \"sx(i):mrf\")[, \"Mean\"]\nspplot(london, zcol = \"spatial\")\n\n# Map the residuals\nlondon$spatial_resid <- residuals(flu_spatial)[, \"mu\"]\nspplot(london, zcol = \"spatial_resid\")\n\n# Test residuals for spatial correlation\nmoran.mc(london$spatial_resid, nb2listw(borough_nb), 999)\n\n\n\n# Chapter 4.  Geostatistics ----\n# Canadian geochemical survey data ----\n# ca_geo has been pre-defined\nstr(ca_geo, 1)\n\n# See what measurements are at each location\nnames(ca_geo)\n\n# Get a summary of the acidity (pH) values\nsummary(ca_geo$pH)\n\n# Look at the distribution\nhist(ca_geo$pH)\n\n# Make a vector that is TRUE for the missing data\nmiss <- is.na(ca_geo$pH)\ntable(miss)\n\n# Plot a map of acidity\nspplot(ca_geo[!is.na(ca_geo$pH), ], \"pH\")\n\n\n\n\n# Fitting a trend surface ----\n# ca_geo has been pre-defined\nstr(ca_geo, 1)\n\n# Are they called lat-long, up-down, or what?\ncoordnames(ca_geo)\n\n# Complete the formula\nm_trend <- lm(pH ~ x + y, as.data.frame(ca_geo))\n\n# Check the coefficients\nsummary(m_trend)\n\n\n\n# Predicting from a trend surface ----\n# The acidity survey data, ca_geo, and the linear model, m_trend have been pre-defined.\n# ca_geo, miss, m_trend have been pre-defined\nls.str()\n\n# Make a vector that is TRUE for the missing data\nmiss <- is.na(ca_geo$pH)\n\n# Create a data frame of missing data\nca_geo_miss <- as.data.frame(ca_geo)[miss, ]\n\n# Predict pH for the missing data\npredictions <- predict(m_trend, newdata = ca_geo_miss, se.fit = TRUE)\n\n# Compute the exceedence probability\npAlkaline <- 1 - pnorm(7, mean = predictions$fit, sd = predictions$se.fit)\nhist(pAlkaline)\n\n\n\n# Variogram estimation ----\n# ca_geo, miss have been pre-defined\nls.str()\n\n# Make a cloud from the non-missing data up to 10km\nplot(variogram(pH ~ 1, ca_geo[!miss, ], cloud = TRUE, cutoff = 10000))\n\n# Make a variogram of the non-missing data\nplot(variogram(pH ~ 1, ca_geo[!miss, ]))\n\n\n\n# Variogram with spatial trend ----\n# ca_geo, miss have been pre-defined\nls.str()\n\n# See what coordinates are called\ncoordnames(ca_geo)\n\n# The pH depends on the coordinates\nph_vgm <- variogram(pH ~ x + y, ca_geo[!miss, ])\nplot(ph_vgm)\n\n\n\n\n# Variogram model fitting ----\n\n# The nugget is the value of the semivariance at zero distance.\n# The partial sill, psill is the difference between the sill and the nugget.\n# Set the range to the distance at which the variogram has got about half way between the nugget and the sill.\n# ca_geo, miss, ph_vgm have been pre-defined\nls.str()\n\n# Eyeball the variogram and estimate the initial parameters\nnugget <- 0.16\npsill <- 0.13\nrange <- 10000\n\n# Fit the variogram\nv_model <- fit.variogram(\n  ph_vgm, \n  model = vgm(\n    model = \"Ste\",\n    nugget = nugget,\n    psill = psill,\n    range = range,\n    kappa = 0.5\n  )\n)\n\n# Show the fitted variogram on top of the binned variogram\nplot(ph_vgm, model = v_model)\nprint(v_model)\n\n\n\n\n\n# Filling in the gaps ----\n# ca_geo, miss, v_model have been pre-defined\nls.str()\n\n# Set the trend formula and the new data\nkm <- krige(pH ~ x + y, ca_geo[!miss, ], newdata = ca_geo[miss, ], model = v_model)\nnames(km)\n\n# Plot the predicted values\nspplot(km, \"var1.pred\")\n\n# Compute the probability of alkaline samples, and map\nkm$pAlkaline <- 1 - pnorm(7, mean = km$var1.pred, sd = sqrt(km$var1.var))\nspplot(km, \"pAlkaline\")\n\n\n\n\n\n# Making a prediction grid ----\n# ca_geo, geo_bounds have been pre-defined\nls.str()\n\n# Plot the polygon and points\nplot(geo_bounds); points(ca_geo)\n\n# Find the corners of the boundary\nbbox(geo_bounds)\n\n# Define a 2.5km square grid over the polygon extent. The first parameter is\n# the bottom left corner.\ngrid <- GridTopology(c(537853, 5536290), c(2500, 2500), c(72, 48))\n\n# Create points with the same coordinate system as the boundary\ngridpoints <- SpatialPoints(grid, proj4string = CRS(projection(geo_bounds)))\nplot(gridpoints)\n\n# Crop out the points outside the boundary\ncropped_gridpoints <- crop(gridpoints, geo_bounds)\nplot(cropped_gridpoints)\n\n# Convert to SpatialPixels\nspgrid <- SpatialPixels(cropped_gridpoints)\ncoordnames(spgrid) <- c(\"x\", \"y\")\nplot(spgrid)\n\n\n\n\n# Gridded predictions ----\n# spgrid, v_model have been pre-defined\nls.str()\n\n# Do kriging predictions over the grid\nph_grid <- krige(pH ~ x + y, ca_geo[!miss, ], newdata = spgrid, model = v_model)\n\n# Calc the probability of pH exceeding 7\nph_grid$pAlkaline <- 1 - pnorm(7, mean = ph_grid$var1.pred, sd = sqrt(ph_grid$var1.pred))\n\n# Map the probability of alkaline samples\nspplot(ph_grid, zcol = \"pAlkaline\")\n\n\n\n# Auto-kriging at point locations ----\n# The autoKrige() function in the automap package computes binned variograms, fits models, does model selection, and performs kriging \n# by making multiple calls to the gstat functions you used previously. It can be a great time-saver but you should always check the results carefully.\n# ca_geo, miss are pre-defined\nmiss <- is.na(ca_geo$pH)\n\nls.str()\n\n# Kriging with linear trend, predicting over the missing points\nph_auto <- autoKrige(\n  pH ~ x + y, \n  input_data = ca_geo[!miss, ], \n  new_data = ca_geo[miss, ], \n  model = \"Mat\"\n)\n\n# Plot the variogram, predictions, and standard error\nplot(ph_auto)\n\n\n\n\n\n# Auto-kriging over a grid ----\n# ca_geo, miss, spgrid, ph_grid, v_model are pre-defined\nls.str()\n\n# Auto-run the kriging\nph_auto_grid <- autoKrige(pH ~ x + y, input_data = ca_geo[!miss, ], new_data = spgrid)\n\n# Remember predictions from manual kriging\nplot(ph_grid)\n\n# Plot predictions and variogram fit\nplot(ph_auto_grid)\n\n# Compare the variogram model to the earlier one\nv_model\nph_auto_grid$var_model",
    "created" : 1499234928015.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1584974979",
    "id" : "A4D5AE4F",
    "lastKnownWriteTime" : 1499413335,
    "last_content_update" : 1499413335160,
    "path" : "C:/Users/d91067/Desktop/datacamp/Spatial Statistics in R/Spatial_Statistics_in_R.R",
    "project_path" : "Spatial_Statistics_in_R.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}