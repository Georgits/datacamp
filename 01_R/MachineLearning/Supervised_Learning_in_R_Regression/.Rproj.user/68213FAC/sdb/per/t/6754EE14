{
    "collab_server" : "",
    "contents" : "load(\"Income.RData\")\nincome_train <- incometrain\nincome_test <- incometest\n\nunemployment <- readRDS(\"unemployment.rds\")\nbloodpressure <- readRDS(\"bloodpressure.rds\")\nhouseprice <- readRDS(\"houseprice.rds\")\nbikes <- load(\"bikes.RData\")\nSoybean <- load(\"Soybean.RData\")\ndframe <- read.csv(\"dframe.csv\", sep = \";\",  row.names = 1)\ntestframe <- read.csv(\"testframe.csv\", sep = \";\",  row.names = 1)\nsparrow <- readRDS(\"sparrow.rds\")\n\nlibrary(broom)\nlibrary(sigr)\nlibrary(ggplot2)\nlibrary(WVPlots)\nlibrary(vtreat)\nlibrary(Sleuth3)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(broom)\nlibrary(mgcv)\nlibrary(ranger)\nlibrary(vtreat)\nlibrary(magrittr)\nlibrary(xgboost)\n\n\n# 1. Chapter : Whats is a regression? ----\n# Code a simple one-variable regression ----\n# unemployment is loaded in the workspace\nsummary(unemployment)\n\n# Define a formula to express female_unemployment as a function of male_unemployment\nfmla <- female_unemployment ~ male_unemployment\n\n# Print it\nfmla\n\n# Use the formula to fit a model: unemployment_model\nunemployment_model <- lm(fmla, data = unemployment)\n\n# Print it\nunemployment_model\n\n\n\n\n\n\n# Examining a model ----\n# broom and sigr are already loaded in your workspace\n# Print unemployment_model\nunemployment_model\n\n# Call summary() on unemployment_model to get more details\nsummary(unemployment_model)\n\n# Call glance() on unemployment_model to see the details in a tidier form\nglance(unemployment_model)\n\n# Call wrapFTest() on unemployment_model to see the most relevant details\nwrapFTest(unemployment_model)\n\n\n\n\n# Predicting from the unemployment model ----\n# unemployment is in your workspace\nsummary(unemployment)\n\n# newrates is in your workspace\nnewrates <- data.frame(male_unemployment  = c(5))\nnewrates\n\n# Predict female unemployment in the unemployment data set\nunemployment$prediction <-  predict(unemployment_model, newdata = unemployment)\n\n# Make a plot to compare predictions to actual (prediction on x axis). \nggplot(unemployment, aes(x = prediction, y = female_unemployment)) + \n  geom_point() +\n  geom_abline(color = \"blue\")\n\n# Predict female unemployment rate when male unemployment is 5%\npred <- predict(unemployment_model, newdata = newrates)\n# Print it\npred\n\n\n\n\n# Multivariate linear regression (Part 1) ----\n# bloodpressure is in the workspace\nsummary(bloodpressure)\n\n# Create the formula and print it\nfmla <- blood_pressure ~ age + weight\nfmla\n\n# Fit the model: bloodpressure_model\nbloodpressure_model <- lm(fmla, data = bloodpressure)\n\n# Print bloodpressure_model and call summary() \nbloodpressure_model\nsummary(bloodpressure_model)\n\n\n\n\n# Multivariate linear regression (Part 2) ----\n# predict blood pressure using bloodpressure_model :prediction\nbloodpressure$prediction <- predict(bloodpressure_model, newdata = bloodpressure)\n\n# plot the results\nggplot(bloodpressure, aes(x = prediction, y = blood_pressure)) + \n  geom_point() +\n  geom_abline(color = \"blue\")\n  \n\n\n\n\n\n\n# Chapter 2: Training and Evaluating Regression ----\n# Graphically evaluate the unemployment model ----\n# unemployment is in the workspace\nsummary(unemployment)\n\n# unemployment_model is in the workspace\nsummary(unemployment_model)\n\n# Make predictions from the model\nunemployment$predictions <- predict(unemployment_model, newdata = unemployment)\n\n# Fill in the blanks to plot predictions (on x-axis) versus the female_unemployment rates\nggplot(unemployment, aes(x = predictions, y = female_unemployment)) + \n  geom_point() + \n  geom_abline()\n\n# Calculate residuals\nunemployment$residuals <- unemployment$female_unemployment - unemployment$predictions\n\n# Fill in the blanks to plot predictions (on x-axis) versus the residuals\nggplot(unemployment, aes(x = predictions, y = residuals)) + \n  geom_pointrange(aes(ymin = 0, ymax = residuals)) + \n  geom_hline(yintercept = 0, linetype = 3) + \n  ggtitle(\"residuals vs. linear model prediction\")\n\n\n\n\n# The gain curve to evaluate the unemployment model ----\n# Plot the Gain Curve\nGainCurvePlot(unemployment, \"prediction\", \"female_unemployment\", \"Unemployment model\")\n\n\n# Calculate RMSE ----\n# For convenience put the residuals in the variable res\nres <- unemployment$residuals\n\n# Calculate RMSE, assign it to the variable rmse and print it\n(rmse <- sqrt(mean(res^2)))\n\n# Calculate the standard deviation of female_unemployment and print it\n(sd_unemployment <- sd(unemployment$female_unemployment))\n\n\n\n# Calculate R-Squared ----\n# Calculate mean female_unemployment: fe_mean. Print it\n(fe_mean <- mean(unemployment$female_unemployment))\n\n# Calculate total sum of squares: tss. Print it\n(tss <- sum((unemployment$female_unemployment - fe_mean)^2))\n\n# Calculate residual sum of squares: rss. Print it\n(rss <- sum(unemployment$residuals^2))\n\n# Calculate R-squared: rsq. Print it. Is it a good fit?\n(rsq <- 1 - rss / tss)\n\n# Get R-squared from glance. Print it\n(rsq_glance <- glance(unemployment_model)$r.squared)\n\n\n\n# Correlation and R-squared ----\n# Get the correlation between the prediction and true outcome: rho and print it\n(rho <- cor(unemployment$female_unemployment, unemployment$predictions))\n\n# Square rho: rho2 and print it\n(rho2 <- rho^2)\n\n# Get R-squared from glance and print it\n(rsq_glance <- glance(unemployment_model)$r.squared)\n\n\n\n\n\n# Generating a random test/train split ----\n# mpg is in the workspace\nsummary(mpg)\ndim(mpg)\n\n# Use nrow to get the number of rows in mpg (N) and print it\n(N <- nrow(mpg))\n\n# Calculate how many rows 75% of N should be and print it\n# Hint: use round() to get an integer\n(target <- round(0.75 * N, 0))\n\n# Create the vector of N uniform random variables: gp\ngp <- runif(N)\n\n# Use gp to create the training set: mpg_train (75% of data) and mpg_test (25% of data)\nmpg_train <- mpg[gp < 0.75,]\nmpg_test <- mpg[gp >= 0.75,]\n\n# Use nrow() to examine mpg_train and mpg_test\nnrow(mpg_train)\nnrow(mpg_test)\n\n\n\n\n\n# Train a model using test/train split ----\n# mpg_train is in the workspace\nsummary(mpg_train)\n\n# Create a formula to express cty as a function of hwy: fmla and print it.\n(fmla <- cty ~ hwy)\n\n# Now use lm() to build a model mpg_model from mpg_train that predicts cty from hwy \nmpg_model <- lm(fmla, data = mpg_train)\n\n# Use summary() to examine the model\nsummary(mpg_model)\n\n\n\n# Evaluate a model using test/train split ----\n# Functions rmse() and r_squared() to calculate RMSE and R-squared have been provided for convenience:\nrmse <- function(predcol, ycol) {\n  res = predcol-ycol\n  sqrt(mean(res^2))\n}\n\n\nr_squared <- function(predcol, ycol) {\n  tss = sum( (ycol - mean(ycol))^2 )\n  rss = sum( (predcol - ycol)^2 )\n  1 - rss/tss\n}\n\n# predict cty from hwy for the training set\nmpg_train$pred <- predict(mpg_model, newdata = mpg_train)\n\n# predict cty from hwy for the test set\nmpg_test$pred <- predict(mpg_model, newdata = mpg_test)\n\n# Evaluate the rmse on both training and test data and print them\n(rmse_train <- rmse(mpg_train$pred, mpg_train$cty))\n(rmse_test <- rmse(mpg_test$pred, mpg_test$cty))\n\n\n# Evaluate the r-squared on both training and test data.and print them\n(rsq_train <- r_squared(mpg_train$pred, mpg_train$cty))\n(rsq_test <- r_squared(mpg_test$pred, mpg_test$cty))\n\n# Plot the predictions (on the x-axis) against the outcome (cty) on the test data\nggplot(mpg_test, aes(x = pred, y = cty)) + \n  geom_point() + \n  geom_abline()\n\n\n\n\n\n# Create a cross validation plan ----\n# kWayCrossValidation() creates a cross validation plan with the following call:\n  \n  # splitPlan <- kWayCrossValidation(nRows, nSplits, dframe, y)\n\n# where nRows is the number of rows of data to be split, and nSplits is the desired number of cross-validation folds.\n# Strictly speaking, dframe and y aren't used by kWayCrossValidation; they are there for compatibility \n# with other vtreat data partitioning functions. You can set them both to NULL.\n\n# The resulting splitPlan is a list of nSplits elements; each element contains two vectors:\n\n# train: the indices of dframe that will form the training set\n# app: the indices of dframe that will form the test (or application) set\n\n# mpg is in the workspace\nsummary(mpg)\n\n# Get the number of rows in mpg\nnRows <- nrow(mpg)\n\n# Implement the 3-fold cross-fold plan with vtreat\nk <- 3\nsplitPlan <- kWayCrossValidation(nRows, k, NULL, NULL)\n\n# Examine the split plan\nstr(splitPlan)\n\n# !!! die app-Vektoren haben keine Überschneidungen, deswegen funktioniert die Schleife unten.\n# intersect(splitPlan[[1]][[\"app\"]],splitPlan[[2]][[\"app\"]])\n\n\n# Evaluate a modeling procedure using n-fold cross-validation\n# mpg is in the workspace\nsummary(mpg)\n\n# splitPlan is in the workspace\nstr(splitPlan)\n\n# Run the 3-fold cross validation plan from splitPlan\nk <- 3 # Number of folds\nmpg$pred.cv <- 0 \nfor(i in 1:k) {\n  split <- splitPlan[[i]]\n  model <- lm(cty ~ hwy, data = mpg[split$train, ])\n  mpg$pred.cv[split$app] <- predict(model, newdata = mpg[split$app,])\n}\n\n# Predict from a full model\nmpg$pred <- predict(lm(cty ~ hwy, data = mpg))\n\n# Get the rmse of the full model's predictions\nrmse(mpg$pred, mpg$cty)\n\n# Get the rmse of the cross-validation predictions\nrmse(mpg$pred.cv, mpg$cty)\n\n\n\n\n\n\n# Chapter 3: Issues to Consider ----\n# Examining the structure of categorical inputs ----\n# Call str on flowers to see the types of each column\nstr(flowers)\n\n# Use unique() to see how many possible values Time takes\nunique(flowers$Time)\n\n# Build a formula to express Flowers as a function of Intensity and Time: fmla. Print it\n(fmla <- as.formula(\"Flowers ~ Intensity + Time\"))\n\n# Use fmla and model.matrix to see how the data is represented for modeling\nmmat <- model.matrix(fmla, data = flowers)\n\n# Examine the first 20 lines of flowers\nhead(flowers, n = 20)\n\n# Examine the first 20 lines of mmat\nhead(mmat, n = 20)\n\n\n\n# Modeling with categorical inputs ----\n# flowers in is the workspace\nstr(flowers)\n\n# fmla is in the workspace\nfmla\n\n# Fit a model to predict Flowers from Intensity and Time : flower_model\nflower_model <- lm(fmla, data = flowers)\n\n# Use summary on mmat to remind yourself of its structure\nsummary(mmat)\n\n# Use summary to examine flower_model \nsummary(flower_model)\n\n# Predict the number of flowers on each plant\nflowers$predictions <- predict(flower_model, data = flowers)\n\n# Plot predictions vs actual flowers (predictions on x-axis)\nggplot(flowers, aes(x = predictions, y = Flowers)) + \n  geom_point() +\n  geom_abline(color = \"blue\") \n\n\n\n\n\n\n# Modeling an interaction ----\n# alcohol is in the workspace\nsummary(alcohol)\n\n# Create the formula with main effects only\n(fmla_add <- Metabol ~ Gastric + Sex )\n\n# Create the formula with interactions\n(fmla_interaction <- Metabol ~ Gastric + Gastric : Sex )\n\n# Fit the main effects only model\nmodel_add <- lm(fmla_add, data = alcohol)\n\n# Fit the interaction model\nmodel_interaction <- lm(fmla_interaction, data = alcohol)\n\n# Call summary on both models and compare\nsummary(model_add)\nsummary(model_interaction)\n\n\n\n\n\n\n\n\n# Modeling an interaction (2) ----\n# alcohol is in the workspace\nsummary(alcohol)\n\n# Both the formulae are in the workspace\nfmla_add\nfmla_interaction\n\n# Create the splitting plan for 3-fold cross validation\nset.seed(34245)  # set the seed for reproducibility\nsplitPlan <- kWayCrossValidation(nrow(alcohol), 3, NULL, NULL)\n\n# Sample code: Get cross-val predictions for main-effects only model\nalcohol$pred_add <- 0  # initialize the prediction vector\nfor(i in 1:3) {\n  split <- splitPlan[[i]]\n  model_add <- lm(fmla_add, data = alcohol[split$train, ])\n  alcohol$pred_add[split$app] <- predict(model_add, newdata = alcohol[split$app, ])\n}\n\n# Get the cross-val predictions for the model with interactions\nalcohol$pred_interaction <- 0 # initialize the prediction vector\nfor(i in 1:3) {\n  split <- splitPlan[[i]]\n  model_interaction <- lm(fmla_interaction, data = alcohol[split$train, ])\n  alcohol$pred_interaction[split$app] <- predict(model_interaction, newdata = alcohol[split$app, ])\n}\n\n# Get RMSE\nalcohol %>% \n  gather(key = modeltype, value = pred, pred_add, pred_interaction) %>%\n  mutate(residuals = Metabol - pred) %>%      \n  group_by(modeltype) %>%\n  summarize(rmse = sqrt(mean(residuals^2)))\n\n\n\n\n\n\n# Relative error -----\n# fdata is in the workspace\nsummary(fdata)\n\n# Examine the data: generate the summaries for the groups large and small:\nfdata %>% \n  group_by(label) %>%     # group by small/large purchases\n  summarize(min  = min(y),   # min of y\n            mean = mean(y),   # mean of y\n            max  = max(y))   # max of y\n\n# Fill in the blanks to add error columns\nfdata2 <- fdata %>% \n  group_by(label) %>%       # group by label\n  mutate(residual = pred -y,  # Residual\n         relerr   = residual / y)  # Relative error\n\n# Compare the rmse and rmse.rel of the large and small groups:\nfdata2 %>% \n  group_by(label) %>% \n  summarize(rmse     = sqrt(mean(residual^2)),   # RMSE\n            rmse.rel = sqrt(mean(relerr^2)))   # Root mean squared relative error\n\n# Plot the predictions for both groups of purchases\nggplot(fdata2, aes(x = pred, y = y, color = label)) + \n  geom_point() + \n  geom_abline() + \n  facet_wrap(~ label, ncol = 1, scales = \"free\") + \n  ggtitle(\"Outcome vs prediction\")\n\n\n\n\n\n# Modeling log-transformed monetary output ----\n# Examine Income2005 in the training set\nsummary(income_train$Income2005)\n\n# Write the formula for log income as a function of the tests and print it\n(fmla.log <- log(Income2005) ~ Arith + Word + Parag + Math + AFQT)\n\n# Fit the linear model\nmodel.log <-  lm(fmla.log, data = income_train)\n\n# Make predictions on income_test\nincome_test$logpred <- predict(model.log, newdata = income_test)\nsummary(income_test$logpred)\n\n# Convert the predictions to monetary units\nincome_test$pred.income <- exp(income_test$logpred)\nsummary(income_test$pred.income)\n\n#  Plot predicted income (x axis) vs income\nggplot(income_test, aes(x = pred.income, y = Income2005)) + \n  geom_point() + \n  geom_abline(color = \"blue\")\n\n\n\n\n# Comparing RMSE and root-mean-squared Relative Error ----\n# fmla.abs is in the workspace\n(fmla.abs <- Income2005 ~ Arith + Word + Parag + Math + AFQT)\n\n# model.abs is in the workspace\nmodel.abs <-  lm(fmla.abs, data = income_train)\nsummary(model.abs)\n\n# Add predictions to the test set\nincome_test <- income_test %>%\n  mutate(pred.absmodel = predict(model.abs, income_test),        # predictions from model.abs\n         pred.logmodel = exp(predict(model.log, income_test)))   # predictions from model.log\n\n# Gather the predictions and calculate residuals and relative error\nincome_long <- income_test %>% \n  gather(key = modeltype, value = pred, pred.absmodel, pred.logmodel) %>%\n  mutate(residual = pred - Income2005,   # residuals\n         relerr   = residual / Income2005)   # relative error\n\n# Calculate RMSE and relative RMSE and compare\nincome_long %>% \n  group_by(modeltype) %>%      # group by modeltype\n  summarize(rmse     = sqrt(mean(residual^2)),    # RMSE\n            rmse.rel = sqrt(mean(relerr^2)))    # Root mean squared relative error\n\n\n\n\n# Input transforms: the \"hockey stick\" ----\n# houseprice is in the workspace\nsummary(houseprice)\n\n# Create the formula for price as a function of squared size\n(fmla_sqr <- price ~ I(size^2))\n\n# Fit a model of price as a function of squared size (use fmla_sqr)\nmodel_sqr <- lm(fmla_sqr, data = houseprice)\n\n# Fit a model of price as a linear function of size\nmodel_lin <- lm(price ~ size, data = houseprice)\n\n# Make predictions and compare\nhouseprice %>% \n  mutate(pred_lin = predict(model_lin),       # predictions from linear model\n         pred_sqr = predict(model_sqr)) %>%   # predictions from quadratic model \n  gather(key = modeltype, value = pred, pred_lin, pred_sqr) %>% # gather the predictions\n  ggplot(aes(x = size)) + \n  geom_point(aes(y = price)) +                   # actual prices\n  geom_line(aes(y = pred, color = modeltype)) + # the predictions\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n# Input transforms: the \"hockey stick\" (2) ----\n# houseprice is in the workspace\nsummary(houseprice)\n\n# fmla_sqr is in the workspace\n(fmla_sqr <- price ~ I(size^2))\n\n# Create a splitting plan for 3-fold cross validation\nset.seed(34245)  # set the seed for reproducibility\nsplitPlan <- kWayCrossValidation(nrow(houseprice), 3, NULL, NULL)\n\n# Sample code: get cross-val predictions for price ~ size\nhouseprice$pred_lin <- 0  # initialize the prediction vector\nfor (i in 1:3) {\n  split <- splitPlan[[i]]\n  model_lin <- lm(price ~ size, data = houseprice[split$train,])\n  houseprice$pred_lin[split$app] <- predict(model_lin, newdata = houseprice[split$app,])\n}\n\n# Get cross-val predictions for price as a function of size^2 (use fmla_sqr)\nhouseprice$pred_sqr <- 0 # initialize the prediction vector\nfor (i in 1:3) {\n  split <- splitPlan[[i]]\n  model_sqr <- lm(fmla_sqr, data = houseprice[split$train, ])\n  houseprice$pred_sqr[split$app] <- predict(model_sqr, newdata = houseprice[split$app, ])\n}\n\n# Gather the predictions and calculate the residuals\nhouseprice_long <- houseprice %>%\n  gather(key = modeltype, value = pred, pred_lin, pred_sqr) %>%\n  mutate(residuals = pred - price)\n\n# Compare the cross-validated RMSE for the two models\nhouseprice_long %>% \n  group_by(modeltype) %>% # group by modeltype\n  summarize(rmse = sqrt(mean(residuals^2)))\n\n\n\n\n# Chapter 4: Dealing with Non-Linear Responses ----\n# Fit a model of sparrow survival probability ----\n# sparrow is in the workspace\nsummary(sparrow)\n\n# Create the survived column\nsparrow$survived <- ifelse(sparrow$status == \"Survived\", TRUE, FALSE)\n\n# Create the formula\n(fmla <- survived ~ total_length + weight + humerus)\n\n# Fit the logistic regression model\nsparrow_model <- glm(fmla, data = sparrow, family = binomial)\n\n# Call summary\nsummary(sparrow_model)\n\n# Call glance\n(perf <- glance(sparrow_model))\n\n# Calculate pseudo-R-squared\n(pseudoR2 <- 1 - perf$deviance / perf$null.deviance)\n\n\n\n\n\n# Predict sparrow survival ----\n# sparrow is in the workspace\nsummary(sparrow)\n\n# sparrow_model is in the workspace\nsummary(sparrow_model)\n\n# Make predictions\nsparrow$pred <- predict(sparrow_model, type = \"response\")\n\n# Look at gain curve\nGainCurvePlot(sparrow, \"pred\", \"survived\", \"sparrow survival model\")\n\n\n\n\n\n# Fit a model to predict bike rental counts -----\n# bikesJuly is in the workspace\nstr(bikesJuly)\n\n# The outcome column\noutcome <- \"cnt\"\n\n# The inputs to use\nvars <- c(\"hr\", \"holiday\", \"workingday\", \"weathersit\", \"temp\",  \"atemp\",  \"hum\" , \"windspeed\")\n\n\n# Create the formula string for bikes rented as a function of the inputs\n(fmla <- paste(outcome, \"~\", paste(vars, collapse = \" + \")))\n\n# Calculate the mean and variance of the outcome\n(mean_bikes <- mean(bikesJuly$cnt))\n(var_bikes <- var(bikesJuly$cnt))\n\n# Fit the model\nbike_model <- glm(fmla, data = bikesJuly, family = quasipoisson)\n\n# Call glance\n(perf <- glance(bike_model))\n\n# Calculate pseudo-R-squared\n(pseudoR2 <- 1 - perf$deviance / perf$null.deviance)\n\n\n\n\n# Predict bike rentals on new data ----\n# bikesAugust is in the workspace\nstr(bikesAugust)\n\n# bike_model is in the workspace\nsummary(bike_model)\n\n# Make predictions on August data\nbikesAugust$pred  <- predict(bike_model, newdata = bikesAugust, type = \"response\")\n\n# Calculate the RMSE\nbikesAugust %>% \n  mutate(residual = pred - cnt) %>%\n  summarize(rmse  = sqrt(mean(residual^2)))\n\n# Plot predictions vs cnt (pred on x-axis)\nggplot(bikesAugust, aes(x = pred, y = cnt)) +\n  geom_point() + \n  geom_abline(color = \"darkblue\")\n\n\n\n\n# Visualize the Bike Rental Predictions ----\n# Plot predictions and cnt by date/time\nquasipoisson_plot <- bikesAugust %>% \n  # set start to 0, convert unit to days\n  mutate(instant = (instant - min(instant))/24) %>%  \n  # gather cnt and pred into a value column\n  gather(key = valuetype, value = value, cnt, pred) %>%\n  filter(instant < 14) %>% # restric to first 14 days\n  # plot value by instant\n  ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + \n  geom_point() + \n  geom_line() + \n  scale_x_continuous(\"Day\", breaks = 0:14, labels = 0:14) + \n  scale_color_brewer(palette = \"Dark2\") + \n  ggtitle(\"Predicted August bike rentals, Quasipoisson model\")\n\nquasipoisson_plot\n\n\n# Model soybean growth with GAM ----\n# soybean_train is in the workspace\nsummary(soybean_train)\n\n# Plot weight vs Time (Time on x axis)\nggplot(soybean_train, aes(x = Time, y = weight)) + geom_point()\n\n# Create the formula \n(fmla.lin <- weight ~ Time )\n\n# Fit the Model.lin\nmodel.lin <- lm(fmla.lin, data = soybean_train)\n\n# Create the formula \n(fmla.gam <- weight ~ s(Time) )\n\n# Fit the GAM Model\nmodel.gam <- gam(fmla.gam, family = gaussian, data = soybean_train)\n\n# Call summary() on model.lin and look for R-squared\nsummary(model.lin)\n\n# Call summary() on model.gam and look for R-squared\nsummary(model.gam)\n\n# Call plot() on model.gam\nplot(model.gam)\n\n\n\n\n# Predict with the soybean model on test data ----\n# soybean_test is in the workspace\nsummary(soybean_test)\n\n# Get predictions from linear model\nsoybean_test$pred.lin <- predict(model.lin, newdata = soybean_test)\n\n# Get predictions from gam model\nsoybean_test$pred.gam <- as.numeric(predict(model.gam, newdata = soybean_test))\n\n# Gather the predictions into a \"long\" dataset\nsoybean_long <- soybean_test %>%\n  gather(key = modeltype, value = pred, pred.lin, pred.gam)\n\n# Calculate the rmse\nsoybean_long %>%\n  mutate(residual = weight - pred) %>%     # residuals\n  group_by(modeltype) %>%                  # group by modeltype\n  summarize(rmse = sqrt(mean(residual^2))) # calculate the RMSE\n\n# Compare the predictions against actual weights on the test data\nsoybean_long %>%\n  ggplot(aes(x = Time)) +                          # the column for the x axis\n  geom_point(aes(y = weight)) +                    # the y-column for the scatterplot\n  geom_point(aes(y = pred, color = modeltype)) +   # the y-column for the point-and-line plot\n  geom_line(aes(y = pred, color = modeltype, linetype = modeltype)) + # the y-column for the point-and-line plot\n  scale_color_brewer(palette = \"Dark2\")\n\n\n\n\n\n# Tree-Based Methods -----\n# Build a random forest model for bike rentals ----\n# bikesJuly is in the workspace\nstr(bikesJuly)\n\n# Random seed to reproduce results\nseed <- 423563\n\n# The outcome column\n(outcome <- \"cnt\")\n\n# The input variables\n(vars <- c(\"hr\", \"holiday\", \"workingday\", \"weathersit\", \"temp\", \"atemp\", \"hum\", \"windspeed\"))\n\n# Create the formula string for bikes rented as a function of the inputs\n(fmla <- paste(outcome, \"~\", paste(vars, collapse = \" + \")))\n\n# Fit and print the random forest model\n(bike_model_rf <- ranger(fmla, # formula \n                         bikesJuly, # data\n                         num.trees = 500, \n                         respect.unordered.factors = \"order\", \n                         seed = seed))\n\n\n\n\n# Predict bike rentals with the random forest model ----\n# bikesAugust is in the workspace\nstr(bikesAugust)\n\n# bike_model_rf is in the workspace\nbike_model_rf\n\n# Make predictions on the August data\nbikesAugust$pred <- predict(bike_model_rf, bikesAugust)$predictions\n\n# Calculate the RMSE of the predictions\nbikesAugust %>% \n  mutate(residual = cnt - pred)  %>% # calculate the residual\n  summarize(rmse  = sqrt(mean(residual^2)))      # calculate rmse\n\n# Plot actual outcome vs predictions (predictions on x-axis)\nggplot(bikesAugust, aes(x = pred, y = cnt)) + \n  geom_point() + \n  geom_abline()\n\n\n\n\n# Visualize random forest bike model predictions ----\n# Print quasipoisson_plot\nquasipoisson_plot\n\n# Plot predictions and cnt by date/time\nrandomforest_plot <- bikesAugust %>% \n  mutate(instant = (instant - min(instant))/24) %>%  # set start to 0, convert unit to days\n  gather(key = valuetype, value = value, cnt, pred) %>%\n  filter(instant < 14) %>% # first two weeks\n  ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + \n  geom_point() + \n  geom_line() + \n  scale_x_continuous(\"Day\", breaks = 0:14, labels = 0:14) + \n  scale_color_brewer(palette = \"Dark2\") + \n  ggtitle(\"Predicted August bike rentals, Random Forest plot\")\n\n\nrandomforest_plot\n\n\n\n\n\n\n\n# vtreat on a small example ----\n# !!!Kochrezept für Umwandlung Kategorialen Daten in die Dummy-Variablen ---- \n# dframe is in the workspace\ndframe\n\n# Create and print a vector of variable names\n(vars <- c(\"color\", \"size\"))\n\n# Load the package vtreat\nlibrary(vtreat)\n\n# Create the treatment plan\n  treatplan <- designTreatmentsZ(dframe, vars)\n\n# Examine the scoreFrame\n(scoreFrame <- treatplan %>%\n    use_series(scoreFrame) %>%\n    select(varName, origName, code))\n\n# We only want the rows with codes \"clean\" or \"lev\"\n(newvars <- scoreFrame %>%\n    filter(code %in% c(\"clean\", \"lev\")) %>%\n    use_series(varName))\n\n# Create the treated training data\n(dframe.treat <- prepare(treatplan, dframe, varRestriction = newvars))\n\n\n\n\n\n# Novel levels ----\n# treatplan is in the workspace\nsummary(treatplan)\n\n# newvars is in the workspace\nnewvars\n\n# Print dframe and testframe\ndframe\ntestframe\n\n# Use prepare() to one-hot-encode testframe\n(testframe.treat <- prepare(treatplan, testframe, varRestriction = newvars))\n\n\n\n\n# vtreat the bike rental data -----\n# The outcome column\n(outcome <- \"cnt\")\n\n# The input columns\n(vars <- c(\"hr\", \"holiday\", \"workingday\", \"weathersit\", \"temp\", \"atemp\", \"hum\", \"windspeed\"))\n\n# Load the package vtreat\nlibrary(vtreat)\n\n# Create the treatment plan from bikesJuly (the training data)\ntreatplan <- designTreatmentsZ(bikesJuly, vars, verbose = FALSE)\n\n# Get the \"clean\" and \"lev\" variables from the scoreFrame\n(newvars <- treatplan %>%\n    use_series(scoreFrame) %>%        \n    filter(code %in% c(\"clean\", \"lev\")) %>%  # get the rows you care about\n    use_series(varName))           # get the varName column\n\n# Prepare the training data\nbikesJuly.treat <- prepare(treatplan, bikesJuly,  varRestriction = newvars)\n\n# Prepare the test data\nbikesAugust.treat <- prepare(treatplan, bikesAugust,  varRestriction = newvars)\n\n# Call str() on the treated data\nstr(bikesJuly.treat)\nstr(bikesAugust.treat)\n# Note that the treated data does not include the outcome column. \n\n\n\n# Find the right number of trees for a gradient boosting machine ----\n# The July data is in the workspace\n# Run xgb.cv\ncv <- xgb.cv(data = as.matrix(bikesJuly.treat), \n             label = bikesJuly$cnt,\n             nrounds = 100,\n             nfold = 5,\n             objective = \"reg:linear\",\n             eta = 0.3,\n             max_depth = 6,\n             early_stopping_rounds = 10,\n             verbose = 0    # silent\n)\n\n# Get the evaluation log \nelog <- cv$evaluation_log\n\n# Determine and print how many trees minimize training and test error\nelog %>% \n  summarize(ntrees.train = which.min(train_rmse_mean),   # find the index of min(train_rmse_mean)\n            ntrees.test  = which.min(test_rmse_mean))   # find the index of min(test_rmse_mean)\n\n# In most cases, ntrees.test is less than ntrees.train. The training error keeps decreasing even after the test \n# error starts to increase. It's important to use cross-validation to find the right \n# number of trees (as determined by ntrees.test) and avoid an overfit model. \n\n\n\n# Fit an xgboost bike rental model and predict ----\n# The number of trees to use, as determined by xgb.cv\nntrees <- 51\n\n# Run xgboost\nbike_model_xgb <- xgboost(data = as.matrix(bikesJuly.treat), # training data as matrix\n                          label = bikesJuly$cnt,  # column of outcomes\n                          nrounds = ntrees,       # number of trees to build\n                          objective = \"reg:linear\", # objective\n                          eta = 0.3,\n                          depth = 6,\n                          verbose = 0  # silent\n)\n\n# Make predictions\nbikesAugust$pred <- predict(bike_model_xgb, as.matrix(bikesAugust.treat))\n\n# Plot predictions (on x axis) vs actual bike rental count\nggplot(bikesAugust, aes(x = pred, y = cnt)) + \n  geom_point() + \n  geom_abline()\n\n\n\n\n# Evaluate the xgboost bike rental model ----\n# bikesAugust is in the workspace\nstr(bikesAugust)\n\n# Calculate RMSE\nbikesAugust %>%\n  mutate(residuals = cnt - pred) %>%\n  summarize(rmse = sqrt(mean(residuals^2)))\n\n\n\n\n# Visualize the xgboost bike rental model ----\n# Print quasipoisson_plot\nquasipoisson_plot\n\n# Print randomforest_plot\nrandomforest_plot\n\n# Plot predictions and actual bike rentals as a function of time (days)\ngradientboosting_plot <- bikesAugust %>% \n  mutate(instant = (instant - min(instant))/24) %>%  # set start to 0, convert unit to days\n  gather(key = valuetype, value = value, cnt, pred) %>%\n  filter(instant < 14) %>% # first two weeks\n  ggplot(aes(x = instant, y = value, color = valuetype, linetype = valuetype)) + \n  geom_point() + \n  geom_line() + \n  scale_x_continuous(\"Day\", breaks = 0:14, labels = 0:14) + \n  scale_color_brewer(palette = \"Dark2\") + \n  ggtitle(\"Predicted August bike rentals, Gradient Boosting model\")\n\ngradientboosting_plot",
    "created" : 1501619582693.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "1385633276",
    "id" : "6754EE14",
    "lastKnownWriteTime" : 1506675141,
    "last_content_update" : 1506675141339,
    "path" : "C:/Users/d91067/Desktop/datacamp/01_R/MachineLearning/Supervised_Learning_in_R_Regression/Supervised_Learning_in_R_Regression.R",
    "project_path" : "Supervised_Learning_in_R_Regression.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}