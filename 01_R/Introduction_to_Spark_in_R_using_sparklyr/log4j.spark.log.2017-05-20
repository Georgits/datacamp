17/05/20 00:22:12 INFO SparkContext: Running Spark version 1.6.2
17/05/20 00:22:13 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/20 00:22:13 ERROR Shell: Failed to locate the winutils binary in the hadoop binary path
java.io.IOException: Could not locate executable null\bin\winutils.exe in the Hadoop binaries.
	at org.apache.hadoop.util.Shell.getQualifiedBinPath(Shell.java:355)
	at org.apache.hadoop.util.Shell.getWinUtilsPath(Shell.java:370)
	at org.apache.hadoop.util.Shell.<clinit>(Shell.java:363)
	at org.apache.hadoop.util.StringUtils.<clinit>(StringUtils.java:79)
	at org.apache.hadoop.security.Groups.parseStaticMapping(Groups.java:104)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:86)
	at org.apache.hadoop.security.Groups.<init>(Groups.java:66)
	at org.apache.hadoop.security.Groups.getUserToGroupsMappingService(Groups.java:280)
	at org.apache.hadoop.security.UserGroupInformation.initialize(UserGroupInformation.java:271)
	at org.apache.hadoop.security.UserGroupInformation.ensureInitialized(UserGroupInformation.java:248)
	at org.apache.hadoop.security.UserGroupInformation.loginUserFromSubject(UserGroupInformation.java:763)
	at org.apache.hadoop.security.UserGroupInformation.getLoginUser(UserGroupInformation.java:748)
	at org.apache.hadoop.security.UserGroupInformation.getCurrentUser(UserGroupInformation.java:621)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)
	at org.apache.spark.util.Utils$$anonfun$getCurrentUserName$1.apply(Utils.scala:2198)
	at scala.Option.getOrElse(Option.scala:120)
	at org.apache.spark.util.Utils$.getCurrentUserName(Utils.scala:2198)
	at org.apache.spark.SparkContext.<init>(SparkContext.scala:322)
	at org.apache.spark.SparkContext$.getOrCreate(SparkContext.scala:2281)
	at org.apache.spark.SparkContext.getOrCreate(SparkContext.scala)
	at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)
	at sun.reflect.NativeMethodAccessorImpl.invoke(Unknown Source)
	at sun.reflect.DelegatingMethodAccessorImpl.invoke(Unknown Source)
	at java.lang.reflect.Method.invoke(Unknown Source)
	at sparklyr.Invoke$.invoke(invoke.scala:94)
	at sparklyr.StreamHandler$.handleMethodCall(stream.scala:89)
	at sparklyr.StreamHandler$.read(stream.scala:55)
	at sparklyr.BackendHandler.channelRead0(handler.scala:49)
	at sparklyr.BackendHandler.channelRead0(handler.scala:14)
	at io.netty.channel.SimpleChannelInboundHandler.channelRead(SimpleChannelInboundHandler.java:105)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.MessageToMessageDecoder.channelRead(MessageToMessageDecoder.java:103)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.handler.codec.ByteToMessageDecoder.channelRead(ByteToMessageDecoder.java:244)
	at io.netty.channel.AbstractChannelHandlerContext.invokeChannelRead(AbstractChannelHandlerContext.java:308)
	at io.netty.channel.AbstractChannelHandlerContext.fireChannelRead(AbstractChannelHandlerContext.java:294)
	at io.netty.channel.DefaultChannelPipeline.fireChannelRead(DefaultChannelPipeline.java:846)
	at io.netty.channel.nio.AbstractNioByteChannel$NioByteUnsafe.read(AbstractNioByteChannel.java:131)
	at io.netty.channel.nio.NioEventLoop.processSelectedKey(NioEventLoop.java:511)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeysOptimized(NioEventLoop.java:468)
	at io.netty.channel.nio.NioEventLoop.processSelectedKeys(NioEventLoop.java:382)
	at io.netty.channel.nio.NioEventLoop.run(NioEventLoop.java:354)
	at io.netty.util.concurrent.SingleThreadEventExecutor$2.run(SingleThreadEventExecutor.java:111)
	at io.netty.util.concurrent.DefaultThreadFactory$DefaultRunnableDecorator.run(DefaultThreadFactory.java:137)
	at java.lang.Thread.run(Unknown Source)
17/05/20 00:22:13 INFO SecurityManager: Changing view acls to: d91067
17/05/20 00:22:13 INFO SecurityManager: Changing modify acls to: d91067
17/05/20 00:22:13 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(d91067); users with modify permissions: Set(d91067)
17/05/20 00:22:13 INFO Utils: Successfully started service 'sparkDriver' on port 58365.
17/05/20 00:22:14 INFO Slf4jLogger: Slf4jLogger started
17/05/20 00:22:14 INFO Remoting: Starting remoting
17/05/20 00:22:14 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@127.0.0.1:58378]
17/05/20 00:22:14 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 58378.
17/05/20 00:22:14 INFO SparkEnv: Registering MapOutputTracker
17/05/20 00:22:14 INFO SparkEnv: Registering BlockManagerMaster
17/05/20 00:22:14 INFO DiskBlockManager: Created local directory at C:\Users\d91067\AppData\Local\Temp\blockmgr-eb3a89c6-f27b-41a9-b528-c6f6706c51a2
17/05/20 00:22:14 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/05/20 00:22:14 INFO SparkEnv: Registering OutputCommitCoordinator
17/05/20 00:22:14 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/05/20 00:22:14 INFO SparkUI: Started SparkUI at http://127.0.0.1:4040
17/05/20 00:22:14 INFO HttpFileServer: HTTP File server directory is C:\Users\d91067\AppData\Local\Temp\spark-634d8e7c-d3ea-4d27-be92-f280ad644952\httpd-036dcfa0-86d1-4f4f-9b3c-2ab772ff7632
17/05/20 00:22:14 INFO HttpServer: Starting HTTP Server
17/05/20 00:22:14 INFO Utils: Successfully started service 'HTTP file server' on port 58383.
17/05/20 00:22:14 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.3/library/sparklyr/java/spark-csv_2.11-1.3.0.jar at http://127.0.0.1:58383/jars/spark-csv_2.11-1.3.0.jar with timestamp 1495232534514
17/05/20 00:22:14 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.3/library/sparklyr/java/commons-csv-1.1.jar at http://127.0.0.1:58383/jars/commons-csv-1.1.jar with timestamp 1495232534583
17/05/20 00:22:14 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.3/library/sparklyr/java/univocity-parsers-1.5.1.jar at http://127.0.0.1:58383/jars/univocity-parsers-1.5.1.jar with timestamp 1495232534595
17/05/20 00:22:14 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.3/library/sparklyr/java/sparklyr-1.6-2.10.jar at http://127.0.0.1:58383/jars/sparklyr-1.6-2.10.jar with timestamp 1495232534602
17/05/20 00:22:14 INFO Executor: Starting executor ID driver on host localhost
17/05/20 00:22:14 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 58416.
17/05/20 00:22:14 INFO NettyBlockTransferService: Server created on 58416
17/05/20 00:22:14 INFO BlockManagerMaster: Trying to register BlockManager
17/05/20 00:22:14 INFO BlockManagerMasterEndpoint: Registering block manager localhost:58416 with 511.1 MB RAM, BlockManagerId(driver, localhost, 58416)
17/05/20 00:22:14 INFO BlockManagerMaster: Registered BlockManager
17/05/20 00:22:15 INFO HiveContext: Initializing execution hive, version 1.2.1
17/05/20 00:22:15 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/05/20 00:22:15 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/05/20 00:22:15 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/05/20 00:22:15 INFO ObjectStore: ObjectStore, initialize called
17/05/20 00:22:15 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/05/20 00:22:15 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/05/20 00:22:15 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/05/20 00:22:16 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/05/20 00:22:17 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/05/20 00:22:17 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 00:22:17 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 00:22:18 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 00:22:18 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 00:22:19 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/05/20 00:22:19 INFO ObjectStore: Initialized ObjectStore
17/05/20 00:22:19 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/05/20 00:22:19 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/05/20 00:22:19 INFO HiveMetaStore: Added admin role in metastore
17/05/20 00:22:19 INFO HiveMetaStore: Added public role in metastore
17/05/20 00:22:19 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/05/20 00:22:19 INFO HiveMetaStore: 0: get_all_databases
17/05/20 00:22:19 INFO audit: ugi=d91067	ip=unknown-ip-addr	cmd=get_all_databases	
17/05/20 00:22:19 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/05/20 00:22:19 INFO audit: ugi=d91067	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/05/20 00:22:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 00:22:19 INFO SparkContext: Invoking stop() from shutdown hook
17/05/20 00:22:19 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/05/20 00:22:19 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/05/20 00:22:19 INFO MemoryStore: MemoryStore cleared
17/05/20 00:22:19 INFO BlockManager: BlockManager stopped
17/05/20 00:22:19 INFO BlockManagerMaster: BlockManagerMaster stopped
17/05/20 00:22:19 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/05/20 00:22:19 INFO SparkContext: Successfully stopped SparkContext
17/05/20 00:22:19 INFO ShutdownHookManager: Shutdown hook called
17/05/20 00:22:19 INFO ShutdownHookManager: Deleting directory C:\Users\d91067\AppData\Local\Temp\spark-9e657d90-1676-437b-808d-bcff4626edf1
17/05/20 00:22:19 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/05/20 00:22:19 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/05/20 00:22:19 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
17/05/20 00:22:20 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\d91067\AppData\Local\Temp\spark-9e657d90-1676-437b-808d-bcff4626edf1
java.io.IOException: Failed to delete: C:\Users\d91067\AppData\Local\Temp\spark-9e657d90-1676-437b-808d-bcff4626edf1
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/05/20 00:22:20 INFO ShutdownHookManager: Deleting directory C:\Users\d91067\AppData\Local\Temp\spark-634d8e7c-d3ea-4d27-be92-f280ad644952\httpd-036dcfa0-86d1-4f4f-9b3c-2ab772ff7632
17/05/20 00:22:20 INFO ShutdownHookManager: Deleting directory C:\Users\d91067\AppData\Local\Temp\spark-634d8e7c-d3ea-4d27-be92-f280ad644952
17/05/20 21:24:09 INFO SparkContext: Running Spark version 1.6.2
17/05/20 21:24:10 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/20 21:24:10 INFO SecurityManager: Changing view acls to: d91067
17/05/20 21:24:10 INFO SecurityManager: Changing modify acls to: d91067
17/05/20 21:24:10 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(d91067); users with modify permissions: Set(d91067)
17/05/20 21:24:10 INFO Utils: Successfully started service 'sparkDriver' on port 55306.
17/05/20 21:24:10 INFO Slf4jLogger: Slf4jLogger started
17/05/20 21:24:10 INFO Remoting: Starting remoting
17/05/20 21:24:11 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@127.0.0.1:55319]
17/05/20 21:24:11 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 55319.
17/05/20 21:24:11 INFO SparkEnv: Registering MapOutputTracker
17/05/20 21:24:11 INFO SparkEnv: Registering BlockManagerMaster
17/05/20 21:24:11 INFO DiskBlockManager: Created local directory at C:\Users\d91067\AppData\Local\Temp\blockmgr-cac75b12-a1e7-4c1c-b615-ac6961d01f16
17/05/20 21:24:11 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/05/20 21:24:11 INFO SparkEnv: Registering OutputCommitCoordinator
17/05/20 21:24:11 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/05/20 21:24:11 INFO SparkUI: Started SparkUI at http://127.0.0.1:4040
17/05/20 21:24:11 INFO HttpFileServer: HTTP File server directory is C:\Users\d91067\AppData\Local\Temp\spark-b1097140-55ac-469a-9a11-5f9486b79e69\httpd-beb2c67d-9848-4957-9d43-dd46424bc4e4
17/05/20 21:24:11 INFO HttpServer: Starting HTTP Server
17/05/20 21:24:11 INFO Utils: Successfully started service 'HTTP file server' on port 55324.
17/05/20 21:24:11 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.3/library/sparklyr/java/spark-csv_2.11-1.3.0.jar at http://127.0.0.1:55324/jars/spark-csv_2.11-1.3.0.jar with timestamp 1495308251498
17/05/20 21:24:11 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.3/library/sparklyr/java/commons-csv-1.1.jar at http://127.0.0.1:55324/jars/commons-csv-1.1.jar with timestamp 1495308251578
17/05/20 21:24:11 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.3/library/sparklyr/java/univocity-parsers-1.5.1.jar at http://127.0.0.1:55324/jars/univocity-parsers-1.5.1.jar with timestamp 1495308251594
17/05/20 21:24:11 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.3/library/sparklyr/java/sparklyr-1.6-2.10.jar at http://127.0.0.1:55324/jars/sparklyr-1.6-2.10.jar with timestamp 1495308251594
17/05/20 21:24:11 INFO Executor: Starting executor ID driver on host localhost
17/05/20 21:24:11 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55357.
17/05/20 21:24:11 INFO NettyBlockTransferService: Server created on 55357
17/05/20 21:24:11 INFO BlockManagerMaster: Trying to register BlockManager
17/05/20 21:24:11 INFO BlockManagerMasterEndpoint: Registering block manager localhost:55357 with 511.1 MB RAM, BlockManagerId(driver, localhost, 55357)
17/05/20 21:24:11 INFO BlockManagerMaster: Registered BlockManager
17/05/20 21:24:12 INFO HiveContext: Initializing execution hive, version 1.2.1
17/05/20 21:24:12 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/05/20 21:24:12 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/05/20 21:24:12 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/05/20 21:24:12 INFO ObjectStore: ObjectStore, initialize called
17/05/20 21:24:12 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/05/20 21:24:12 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/05/20 21:24:13 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/05/20 21:24:13 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/05/20 21:24:14 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/05/20 21:24:15 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:24:15 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:24:15 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:24:15 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:24:16 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/05/20 21:24:16 INFO ObjectStore: Initialized ObjectStore
17/05/20 21:24:16 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/05/20 21:24:16 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/05/20 21:24:16 INFO HiveMetaStore: Added admin role in metastore
17/05/20 21:24:16 INFO HiveMetaStore: Added public role in metastore
17/05/20 21:24:16 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/05/20 21:24:16 INFO HiveMetaStore: 0: get_all_databases
17/05/20 21:24:16 INFO audit: ugi=d91067	ip=unknown-ip-addr	cmd=get_all_databases	
17/05/20 21:24:16 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/05/20 21:24:16 INFO audit: ugi=d91067	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/05/20 21:24:16 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:24:17 INFO SessionState: Created HDFS directory: C:/Users/d91067/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/d91067
17/05/20 21:24:17 INFO SessionState: Created local directory: C:/Users/d91067/AppData/Local/Temp/54a44548-73f1-4296-a2bb-db116cc75d43_resources
17/05/20 21:24:17 INFO SessionState: Created HDFS directory: C:/Users/d91067/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/d91067/54a44548-73f1-4296-a2bb-db116cc75d43
17/05/20 21:24:17 INFO SessionState: Created local directory: C:/Users/d91067/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/54a44548-73f1-4296-a2bb-db116cc75d43
17/05/20 21:24:17 INFO SessionState: Created HDFS directory: C:/Users/d91067/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/d91067/54a44548-73f1-4296-a2bb-db116cc75d43/_tmp_space.db
17/05/20 21:24:17 INFO HiveContext: default warehouse location is C:\Users\d91067\AppData\Local\rstudio\spark\Cache\spark-1.6.2-bin-hadoop2.6\tmp\hive
17/05/20 21:24:17 INFO HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/05/20 21:24:17 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/05/20 21:24:17 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/05/20 21:24:17 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/05/20 21:24:17 INFO ObjectStore: ObjectStore, initialize called
17/05/20 21:24:17 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/05/20 21:24:17 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/05/20 21:24:17 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/05/20 21:24:18 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/05/20 21:24:18 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/05/20 21:24:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:24:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:24:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:24:19 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:24:19 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/05/20 21:24:19 INFO ObjectStore: Initialized ObjectStore
17/05/20 21:24:19 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/05/20 21:24:20 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/05/20 21:24:20 INFO HiveMetaStore: Added admin role in metastore
17/05/20 21:24:20 INFO HiveMetaStore: Added public role in metastore
17/05/20 21:24:20 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/05/20 21:24:20 INFO HiveMetaStore: 0: get_all_databases
17/05/20 21:24:20 INFO audit: ugi=d91067	ip=unknown-ip-addr	cmd=get_all_databases	
17/05/20 21:24:20 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/05/20 21:24:20 INFO audit: ugi=d91067	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/05/20 21:24:20 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:24:20 INFO SessionState: Created local directory: C:/Users/d91067/AppData/Local/Temp/b8fea433-88e2-402b-8b0a-22c17c9c3ecc_resources
17/05/20 21:24:20 INFO SessionState: Created HDFS directory: C:/Users/d91067/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/d91067/b8fea433-88e2-402b-8b0a-22c17c9c3ecc
17/05/20 21:24:20 INFO SessionState: Created local directory: C:/Users/d91067/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/b8fea433-88e2-402b-8b0a-22c17c9c3ecc
17/05/20 21:24:20 INFO SessionState: Created HDFS directory: C:/Users/d91067/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/d91067/b8fea433-88e2-402b-8b0a-22c17c9c3ecc/_tmp_space.db
17/05/20 21:24:20 INFO SparkContext: Invoking stop() from shutdown hook
17/05/20 21:24:20 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/05/20 21:24:20 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/05/20 21:24:20 INFO MemoryStore: MemoryStore cleared
17/05/20 21:24:20 INFO BlockManager: BlockManager stopped
17/05/20 21:24:20 INFO BlockManagerMaster: BlockManagerMaster stopped
17/05/20 21:24:20 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/05/20 21:24:20 INFO SparkContext: Successfully stopped SparkContext
17/05/20 21:24:20 INFO ShutdownHookManager: Shutdown hook called
17/05/20 21:24:20 INFO ShutdownHookManager: Deleting directory C:\Users\d91067\AppData\Local\Temp\spark-b1097140-55ac-469a-9a11-5f9486b79e69
17/05/20 21:24:20 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/05/20 21:24:20 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/05/20 21:24:20 INFO ShutdownHookManager: Deleting directory C:\Users\d91067\AppData\Local\Temp\spark-b1097140-55ac-469a-9a11-5f9486b79e69\httpd-beb2c67d-9848-4957-9d43-dd46424bc4e4
17/05/20 21:24:20 INFO ShutdownHookManager: Deleting directory C:\Users\d91067\AppData\Local\Temp\spark-02ee29d7-3814-4dd7-82db-a3bcc5580b28
17/05/20 21:24:20 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
17/05/20 21:24:21 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\d91067\AppData\Local\Temp\spark-02ee29d7-3814-4dd7-82db-a3bcc5580b28
java.io.IOException: Failed to delete: C:\Users\d91067\AppData\Local\Temp\spark-02ee29d7-3814-4dd7-82db-a3bcc5580b28
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/05/20 21:26:57 INFO SparkContext: Running Spark version 1.6.2
17/05/20 21:26:57 WARN NativeCodeLoader: Unable to load native-hadoop library for your platform... using builtin-java classes where applicable
17/05/20 21:26:58 INFO SecurityManager: Changing view acls to: d91067
17/05/20 21:26:58 INFO SecurityManager: Changing modify acls to: d91067
17/05/20 21:26:58 INFO SecurityManager: SecurityManager: authentication disabled; ui acls disabled; users with view permissions: Set(d91067); users with modify permissions: Set(d91067)
17/05/20 21:26:58 INFO Utils: Successfully started service 'sparkDriver' on port 55487.
17/05/20 21:26:58 INFO Slf4jLogger: Slf4jLogger started
17/05/20 21:26:58 INFO Remoting: Starting remoting
17/05/20 21:26:58 INFO Remoting: Remoting started; listening on addresses :[akka.tcp://sparkDriverActorSystem@127.0.0.1:55500]
17/05/20 21:26:58 INFO Utils: Successfully started service 'sparkDriverActorSystem' on port 55500.
17/05/20 21:26:58 INFO SparkEnv: Registering MapOutputTracker
17/05/20 21:26:58 INFO SparkEnv: Registering BlockManagerMaster
17/05/20 21:26:58 INFO DiskBlockManager: Created local directory at C:\Users\d91067\AppData\Local\Temp\blockmgr-140f7fca-d66c-4dec-b62a-612dcb323134
17/05/20 21:26:58 INFO MemoryStore: MemoryStore started with capacity 511.1 MB
17/05/20 21:26:58 INFO SparkEnv: Registering OutputCommitCoordinator
17/05/20 21:26:58 INFO Utils: Successfully started service 'SparkUI' on port 4040.
17/05/20 21:26:58 INFO SparkUI: Started SparkUI at http://127.0.0.1:4040
17/05/20 21:26:58 INFO HttpFileServer: HTTP File server directory is C:\Users\d91067\AppData\Local\Temp\spark-74c2ffc0-55bb-4147-983f-2be0c6204a95\httpd-870c9a3a-872e-4821-9f73-7fb7e7c5ba20
17/05/20 21:26:58 INFO HttpServer: Starting HTTP Server
17/05/20 21:26:58 INFO Utils: Successfully started service 'HTTP file server' on port 55505.
17/05/20 21:26:58 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.3/library/sparklyr/java/spark-csv_2.11-1.3.0.jar at http://127.0.0.1:55505/jars/spark-csv_2.11-1.3.0.jar with timestamp 1495308418969
17/05/20 21:26:59 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.3/library/sparklyr/java/commons-csv-1.1.jar at http://127.0.0.1:55505/jars/commons-csv-1.1.jar with timestamp 1495308419018
17/05/20 21:26:59 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.3/library/sparklyr/java/univocity-parsers-1.5.1.jar at http://127.0.0.1:55505/jars/univocity-parsers-1.5.1.jar with timestamp 1495308419027
17/05/20 21:26:59 INFO SparkContext: Added JAR file:/C:/Program%20Files/R/R-3.3.3/library/sparklyr/java/sparklyr-1.6-2.10.jar at http://127.0.0.1:55505/jars/sparklyr-1.6-2.10.jar with timestamp 1495308419035
17/05/20 21:26:59 INFO Executor: Starting executor ID driver on host localhost
17/05/20 21:26:59 INFO Utils: Successfully started service 'org.apache.spark.network.netty.NettyBlockTransferService' on port 55538.
17/05/20 21:26:59 INFO NettyBlockTransferService: Server created on 55538
17/05/20 21:26:59 INFO BlockManagerMaster: Trying to register BlockManager
17/05/20 21:26:59 INFO BlockManagerMasterEndpoint: Registering block manager localhost:55538 with 511.1 MB RAM, BlockManagerId(driver, localhost, 55538)
17/05/20 21:26:59 INFO BlockManagerMaster: Registered BlockManager
17/05/20 21:26:59 INFO HiveContext: Initializing execution hive, version 1.2.1
17/05/20 21:26:59 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/05/20 21:26:59 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/05/20 21:27:00 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/05/20 21:27:00 INFO ObjectStore: ObjectStore, initialize called
17/05/20 21:27:00 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/05/20 21:27:00 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/05/20 21:27:00 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/05/20 21:27:00 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/05/20 21:27:01 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/05/20 21:27:02 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:27:02 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:27:02 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:27:02 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:27:03 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/05/20 21:27:03 INFO ObjectStore: Initialized ObjectStore
17/05/20 21:27:03 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/05/20 21:27:03 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/05/20 21:27:03 INFO HiveMetaStore: Added admin role in metastore
17/05/20 21:27:03 INFO HiveMetaStore: Added public role in metastore
17/05/20 21:27:03 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/05/20 21:27:03 INFO HiveMetaStore: 0: get_all_databases
17/05/20 21:27:03 INFO audit: ugi=d91067	ip=unknown-ip-addr	cmd=get_all_databases	
17/05/20 21:27:03 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/05/20 21:27:03 INFO audit: ugi=d91067	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/05/20 21:27:03 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:27:03 INFO SessionState: Created local directory: C:/Users/d91067/AppData/Local/Temp/0f843fcb-429f-4898-acf9-bc4604adbc0e_resources
17/05/20 21:27:03 INFO SessionState: Created HDFS directory: C:/Users/d91067/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/d91067/0f843fcb-429f-4898-acf9-bc4604adbc0e
17/05/20 21:27:03 INFO SessionState: Created local directory: C:/Users/d91067/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/0f843fcb-429f-4898-acf9-bc4604adbc0e
17/05/20 21:27:03 INFO SessionState: Created HDFS directory: C:/Users/d91067/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/d91067/0f843fcb-429f-4898-acf9-bc4604adbc0e/_tmp_space.db
17/05/20 21:27:03 INFO HiveContext: default warehouse location is C:\Users\d91067\AppData\Local\rstudio\spark\Cache\spark-1.6.2-bin-hadoop2.6\tmp\hive
17/05/20 21:27:03 INFO HiveContext: Initializing HiveMetastoreConnection version 1.2.1 using Spark classes.
17/05/20 21:27:03 INFO ClientWrapper: Inspected Hadoop version: 2.6.0
17/05/20 21:27:03 INFO ClientWrapper: Loaded org.apache.hadoop.hive.shims.Hadoop23Shims for Hadoop version 2.6.0
17/05/20 21:27:04 INFO HiveMetaStore: 0: Opening raw store with implemenation class:org.apache.hadoop.hive.metastore.ObjectStore
17/05/20 21:27:04 INFO ObjectStore: ObjectStore, initialize called
17/05/20 21:27:04 INFO Persistence: Property hive.metastore.integral.jdo.pushdown unknown - will be ignored
17/05/20 21:27:04 INFO Persistence: Property datanucleus.cache.level2 unknown - will be ignored
17/05/20 21:27:04 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/05/20 21:27:04 WARN Connection: BoneCP specified but not present in CLASSPATH (or one of dependencies)
17/05/20 21:27:05 INFO ObjectStore: Setting MetaStore object pin classes with hive.metastore.cache.pinobjtypes="Table,StorageDescriptor,SerDeInfo,Partition,Database,Type,FieldSchema,Order"
17/05/20 21:27:05 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:27:05 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:27:06 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MFieldSchema" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:27:06 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MOrder" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:27:06 INFO MetaStoreDirectSql: Using direct SQL, underlying DB is DERBY
17/05/20 21:27:06 INFO ObjectStore: Initialized ObjectStore
17/05/20 21:27:06 WARN ObjectStore: Version information not found in metastore. hive.metastore.schema.verification is not enabled so recording the schema version 1.2.0
17/05/20 21:27:06 WARN ObjectStore: Failed to get database default, returning NoSuchObjectException
17/05/20 21:27:06 INFO HiveMetaStore: Added admin role in metastore
17/05/20 21:27:06 INFO HiveMetaStore: Added public role in metastore
17/05/20 21:27:06 INFO HiveMetaStore: No user is added in admin role, since config is empty
17/05/20 21:27:06 INFO HiveMetaStore: 0: get_all_databases
17/05/20 21:27:06 INFO audit: ugi=d91067	ip=unknown-ip-addr	cmd=get_all_databases	
17/05/20 21:27:06 INFO HiveMetaStore: 0: get_functions: db=default pat=*
17/05/20 21:27:06 INFO audit: ugi=d91067	ip=unknown-ip-addr	cmd=get_functions: db=default pat=*	
17/05/20 21:27:06 INFO Datastore: The class "org.apache.hadoop.hive.metastore.model.MResourceUri" is tagged as "embedded-only" so does not have its own datastore table.
17/05/20 21:27:06 INFO SessionState: Created local directory: C:/Users/d91067/AppData/Local/Temp/4803987f-3172-400a-8bba-dee87743e652_resources
17/05/20 21:27:06 INFO SessionState: Created HDFS directory: C:/Users/d91067/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/d91067/4803987f-3172-400a-8bba-dee87743e652
17/05/20 21:27:06 INFO SessionState: Created local directory: C:/Users/d91067/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/4803987f-3172-400a-8bba-dee87743e652
17/05/20 21:27:06 INFO SessionState: Created HDFS directory: C:/Users/d91067/AppData/Local/rstudio/spark/Cache/spark-1.6.2-bin-hadoop2.6/tmp/hive/d91067/4803987f-3172-400a-8bba-dee87743e652/_tmp_space.db
17/05/20 21:27:13 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/05/20 21:27:13 INFO audit: ugi=d91067	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/05/20 21:27:13 INFO SparkContext: Starting job: collect at utils.scala:59
17/05/20 21:27:13 INFO DAGScheduler: Got job 0 (collect at utils.scala:59) with 1 output partitions
17/05/20 21:27:13 INFO DAGScheduler: Final stage: ResultStage 0 (collect at utils.scala:59)
17/05/20 21:27:13 INFO DAGScheduler: Parents of final stage: List()
17/05/20 21:27:13 INFO DAGScheduler: Missing parents: List()
17/05/20 21:27:13 INFO DAGScheduler: Submitting ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56), which has no missing parents
17/05/20 21:27:13 INFO MemoryStore: Block broadcast_0 stored as values in memory (estimated size 5.4 KB, free 5.4 KB)
17/05/20 21:27:13 INFO MemoryStore: Block broadcast_0_piece0 stored as bytes in memory (estimated size 3.0 KB, free 8.4 KB)
17/05/20 21:27:13 INFO BlockManagerInfo: Added broadcast_0_piece0 in memory on localhost:55538 (size: 3.0 KB, free: 511.1 MB)
17/05/20 21:27:13 INFO SparkContext: Created broadcast 0 from broadcast at DAGScheduler.scala:1006
17/05/20 21:27:13 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 0 (MapPartitionsRDD[3] at map at utils.scala:56)
17/05/20 21:27:13 INFO TaskSchedulerImpl: Adding task set 0.0 with 1 tasks
17/05/20 21:27:13 INFO TaskSetManager: Starting task 0.0 in stage 0.0 (TID 0, localhost, partition 0,PROCESS_LOCAL, 2348 bytes)
17/05/20 21:27:13 INFO Executor: Running task 0.0 in stage 0.0 (TID 0)
17/05/20 21:27:13 INFO Executor: Fetching http://127.0.0.1:55505/jars/commons-csv-1.1.jar with timestamp 1495308419018
17/05/20 21:27:13 INFO Utils: Fetching http://127.0.0.1:55505/jars/commons-csv-1.1.jar to C:\Users\d91067\AppData\Local\Temp\spark-74c2ffc0-55bb-4147-983f-2be0c6204a95\userFiles-5410eee9-e446-4237-96d1-a0ba49744a10\fetchFileTemp6855475829824774263.tmp
17/05/20 21:27:14 INFO Executor: Adding file:/C:/Users/d91067/AppData/Local/Temp/spark-74c2ffc0-55bb-4147-983f-2be0c6204a95/userFiles-5410eee9-e446-4237-96d1-a0ba49744a10/commons-csv-1.1.jar to class loader
17/05/20 21:27:14 INFO Executor: Fetching http://127.0.0.1:55505/jars/univocity-parsers-1.5.1.jar with timestamp 1495308419027
17/05/20 21:27:14 INFO Utils: Fetching http://127.0.0.1:55505/jars/univocity-parsers-1.5.1.jar to C:\Users\d91067\AppData\Local\Temp\spark-74c2ffc0-55bb-4147-983f-2be0c6204a95\userFiles-5410eee9-e446-4237-96d1-a0ba49744a10\fetchFileTemp5896335377425828459.tmp
17/05/20 21:27:14 INFO Executor: Adding file:/C:/Users/d91067/AppData/Local/Temp/spark-74c2ffc0-55bb-4147-983f-2be0c6204a95/userFiles-5410eee9-e446-4237-96d1-a0ba49744a10/univocity-parsers-1.5.1.jar to class loader
17/05/20 21:27:14 INFO Executor: Fetching http://127.0.0.1:55505/jars/sparklyr-1.6-2.10.jar with timestamp 1495308419035
17/05/20 21:27:14 INFO Utils: Fetching http://127.0.0.1:55505/jars/sparklyr-1.6-2.10.jar to C:\Users\d91067\AppData\Local\Temp\spark-74c2ffc0-55bb-4147-983f-2be0c6204a95\userFiles-5410eee9-e446-4237-96d1-a0ba49744a10\fetchFileTemp1226717572967702961.tmp
17/05/20 21:27:14 INFO Executor: Adding file:/C:/Users/d91067/AppData/Local/Temp/spark-74c2ffc0-55bb-4147-983f-2be0c6204a95/userFiles-5410eee9-e446-4237-96d1-a0ba49744a10/sparklyr-1.6-2.10.jar to class loader
17/05/20 21:27:14 INFO Executor: Fetching http://127.0.0.1:55505/jars/spark-csv_2.11-1.3.0.jar with timestamp 1495308418969
17/05/20 21:27:14 INFO Utils: Fetching http://127.0.0.1:55505/jars/spark-csv_2.11-1.3.0.jar to C:\Users\d91067\AppData\Local\Temp\spark-74c2ffc0-55bb-4147-983f-2be0c6204a95\userFiles-5410eee9-e446-4237-96d1-a0ba49744a10\fetchFileTemp1754793425555073995.tmp
17/05/20 21:27:14 INFO Executor: Adding file:/C:/Users/d91067/AppData/Local/Temp/spark-74c2ffc0-55bb-4147-983f-2be0c6204a95/userFiles-5410eee9-e446-4237-96d1-a0ba49744a10/spark-csv_2.11-1.3.0.jar to class loader
17/05/20 21:27:14 INFO GenerateUnsafeProjection: Code generated in 109.533922 ms
17/05/20 21:27:14 INFO Executor: Finished task 0.0 in stage 0.0 (TID 0). 1060 bytes result sent to driver
17/05/20 21:27:14 INFO TaskSetManager: Finished task 0.0 in stage 0.0 (TID 0) in 499 ms on localhost (1/1)
17/05/20 21:27:14 INFO TaskSchedulerImpl: Removed TaskSet 0.0, whose tasks have all completed, from pool 
17/05/20 21:27:14 INFO DAGScheduler: ResultStage 0 (collect at utils.scala:59) finished in 0,510 s
17/05/20 21:27:14 INFO DAGScheduler: Job 0 finished: collect at utils.scala:59, took 0,653973 s
17/05/20 21:27:14 INFO MemoryStore: Block broadcast_1 stored as values in memory (estimated size 61.8 KB, free 70.2 KB)
17/05/20 21:27:14 INFO MemoryStore: Block broadcast_1_piece0 stored as bytes in memory (estimated size 19.3 KB, free 89.5 KB)
17/05/20 21:27:14 INFO BlockManagerInfo: Added broadcast_1_piece0 in memory on localhost:55538 (size: 19.3 KB, free: 511.1 MB)
17/05/20 21:27:14 INFO SparkContext: Created broadcast 1 from textFile at TextFile.scala:30
17/05/20 21:27:14 INFO FileInputFormat: Total input paths to process : 1
17/05/20 21:27:14 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/05/20 21:27:14 INFO DAGScheduler: Got job 1 (take at CsvRelation.scala:249) with 1 output partitions
17/05/20 21:27:14 INFO DAGScheduler: Final stage: ResultStage 1 (take at CsvRelation.scala:249)
17/05/20 21:27:14 INFO DAGScheduler: Parents of final stage: List()
17/05/20 21:27:14 INFO DAGScheduler: Missing parents: List()
17/05/20 21:27:14 INFO DAGScheduler: Submitting ResultStage 1 (C:\Users\d91067\AppData\Local\Temp\Rtmpo91v77/spark_serialize_23fd9360f2772a59264c2b1e865ca39cb72ec67532fe16d48d0ea5b134eb2ce8.csv MapPartitionsRDD[5] at textFile at TextFile.scala:30), which has no missing parents
17/05/20 21:27:14 INFO MemoryStore: Block broadcast_2 stored as values in memory (estimated size 3.2 KB, free 92.7 KB)
17/05/20 21:27:14 INFO MemoryStore: Block broadcast_2_piece0 stored as bytes in memory (estimated size 1946.0 B, free 94.6 KB)
17/05/20 21:27:14 INFO BlockManagerInfo: Added broadcast_2_piece0 in memory on localhost:55538 (size: 1946.0 B, free: 511.1 MB)
17/05/20 21:27:14 INFO SparkContext: Created broadcast 2 from broadcast at DAGScheduler.scala:1006
17/05/20 21:27:14 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 1 (C:\Users\d91067\AppData\Local\Temp\Rtmpo91v77/spark_serialize_23fd9360f2772a59264c2b1e865ca39cb72ec67532fe16d48d0ea5b134eb2ce8.csv MapPartitionsRDD[5] at textFile at TextFile.scala:30)
17/05/20 21:27:14 INFO TaskSchedulerImpl: Adding task set 1.0 with 1 tasks
17/05/20 21:27:14 INFO TaskSetManager: Starting task 0.0 in stage 1.0 (TID 1, localhost, partition 0,PROCESS_LOCAL, 2482 bytes)
17/05/20 21:27:14 INFO Executor: Running task 0.0 in stage 1.0 (TID 1)
17/05/20 21:27:14 INFO HadoopRDD: Input split: file:/C:/Users/d91067/AppData/Local/Temp/Rtmpo91v77/spark_serialize_23fd9360f2772a59264c2b1e865ca39cb72ec67532fe16d48d0ea5b134eb2ce8.csv:0+1011375
17/05/20 21:27:14 INFO deprecation: mapred.tip.id is deprecated. Instead, use mapreduce.task.id
17/05/20 21:27:14 INFO deprecation: mapred.task.id is deprecated. Instead, use mapreduce.task.attempt.id
17/05/20 21:27:14 INFO deprecation: mapred.task.is.map is deprecated. Instead, use mapreduce.task.ismap
17/05/20 21:27:14 INFO deprecation: mapred.task.partition is deprecated. Instead, use mapreduce.task.partition
17/05/20 21:27:14 INFO deprecation: mapred.job.id is deprecated. Instead, use mapreduce.job.id
17/05/20 21:27:14 INFO Executor: Finished task 0.0 in stage 1.0 (TID 1). 18599 bytes result sent to driver
17/05/20 21:27:14 INFO TaskSetManager: Finished task 0.0 in stage 1.0 (TID 1) in 63 ms on localhost (1/1)
17/05/20 21:27:14 INFO DAGScheduler: ResultStage 1 (take at CsvRelation.scala:249) finished in 0,063 s
17/05/20 21:27:14 INFO TaskSchedulerImpl: Removed TaskSet 1.0, whose tasks have all completed, from pool 
17/05/20 21:27:14 INFO DAGScheduler: Job 1 finished: take at CsvRelation.scala:249, took 0,049487 s
17/05/20 21:27:14 INFO ParseDriver: Parsing command: SELECT * FROM  `track_metadata`
17/05/20 21:27:15 INFO ParseDriver: Parse Completed
17/05/20 21:27:15 INFO MemoryStore: Block broadcast_3 stored as values in memory (estimated size 208.5 KB, free 303.1 KB)
17/05/20 21:27:15 INFO MemoryStore: Block broadcast_3_piece0 stored as bytes in memory (estimated size 19.3 KB, free 322.5 KB)
17/05/20 21:27:15 INFO BlockManagerInfo: Added broadcast_3_piece0 in memory on localhost:55538 (size: 19.3 KB, free: 511.1 MB)
17/05/20 21:27:15 INFO SparkContext: Created broadcast 3 from textFile at TextFile.scala:30
17/05/20 21:27:15 INFO FileInputFormat: Total input paths to process : 1
17/05/20 21:27:15 INFO SparkContext: Starting job: take at CsvRelation.scala:249
17/05/20 21:27:15 INFO DAGScheduler: Got job 2 (take at CsvRelation.scala:249) with 1 output partitions
17/05/20 21:27:15 INFO DAGScheduler: Final stage: ResultStage 2 (take at CsvRelation.scala:249)
17/05/20 21:27:15 INFO DAGScheduler: Parents of final stage: List()
17/05/20 21:27:15 INFO DAGScheduler: Missing parents: List()
17/05/20 21:27:15 INFO DAGScheduler: Submitting ResultStage 2 (C:\Users\d91067\AppData\Local\Temp\Rtmpo91v77/spark_serialize_23fd9360f2772a59264c2b1e865ca39cb72ec67532fe16d48d0ea5b134eb2ce8.csv MapPartitionsRDD[7] at textFile at TextFile.scala:30), which has no missing parents
17/05/20 21:27:15 INFO MemoryStore: Block broadcast_4 stored as values in memory (estimated size 3.2 KB, free 325.7 KB)
17/05/20 21:27:15 INFO MemoryStore: Block broadcast_4_piece0 stored as bytes in memory (estimated size 1947.0 B, free 327.6 KB)
17/05/20 21:27:15 INFO BlockManagerInfo: Added broadcast_4_piece0 in memory on localhost:55538 (size: 1947.0 B, free: 511.1 MB)
17/05/20 21:27:15 INFO SparkContext: Created broadcast 4 from broadcast at DAGScheduler.scala:1006
17/05/20 21:27:15 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 2 (C:\Users\d91067\AppData\Local\Temp\Rtmpo91v77/spark_serialize_23fd9360f2772a59264c2b1e865ca39cb72ec67532fe16d48d0ea5b134eb2ce8.csv MapPartitionsRDD[7] at textFile at TextFile.scala:30)
17/05/20 21:27:15 INFO TaskSchedulerImpl: Adding task set 2.0 with 1 tasks
17/05/20 21:27:15 INFO TaskSetManager: Starting task 0.0 in stage 2.0 (TID 2, localhost, partition 0,PROCESS_LOCAL, 2482 bytes)
17/05/20 21:27:15 INFO Executor: Running task 0.0 in stage 2.0 (TID 2)
17/05/20 21:27:15 INFO HadoopRDD: Input split: file:/C:/Users/d91067/AppData/Local/Temp/Rtmpo91v77/spark_serialize_23fd9360f2772a59264c2b1e865ca39cb72ec67532fe16d48d0ea5b134eb2ce8.csv:0+1011375
17/05/20 21:27:15 INFO Executor: Finished task 0.0 in stage 2.0 (TID 2). 18599 bytes result sent to driver
17/05/20 21:27:15 INFO TaskSetManager: Finished task 0.0 in stage 2.0 (TID 2) in 15 ms on localhost (1/1)
17/05/20 21:27:15 INFO TaskSchedulerImpl: Removed TaskSet 2.0, whose tasks have all completed, from pool 
17/05/20 21:27:15 INFO DAGScheduler: ResultStage 2 (take at CsvRelation.scala:249) finished in 0,015 s
17/05/20 21:27:15 INFO DAGScheduler: Job 2 finished: take at CsvRelation.scala:249, took 0,012374 s
17/05/20 21:27:15 INFO MemoryStore: Block broadcast_5 stored as values in memory (estimated size 208.5 KB, free 536.1 KB)
17/05/20 21:27:15 INFO MemoryStore: Block broadcast_5_piece0 stored as bytes in memory (estimated size 19.3 KB, free 555.4 KB)
17/05/20 21:27:15 INFO BlockManagerInfo: Added broadcast_5_piece0 in memory on localhost:55538 (size: 19.3 KB, free: 511.1 MB)
17/05/20 21:27:15 INFO SparkContext: Created broadcast 5 from textFile at TextFile.scala:30
17/05/20 21:27:15 INFO FileInputFormat: Total input paths to process : 1
17/05/20 21:27:15 INFO SparkContext: Starting job: sql at null:-2
17/05/20 21:27:15 INFO DAGScheduler: Registering RDD 17 (sql at null:-2)
17/05/20 21:27:15 INFO DAGScheduler: Got job 3 (sql at null:-2) with 1 output partitions
17/05/20 21:27:15 INFO DAGScheduler: Final stage: ResultStage 4 (sql at null:-2)
17/05/20 21:27:15 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 3)
17/05/20 21:27:15 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 3)
17/05/20 21:27:15 INFO DAGScheduler: Submitting ShuffleMapStage 3 (MapPartitionsRDD[17] at sql at null:-2), which has no missing parents
17/05/20 21:27:15 INFO MemoryStore: Block broadcast_6 stored as values in memory (estimated size 23.9 KB, free 579.3 KB)
17/05/20 21:27:15 INFO MemoryStore: Block broadcast_6_piece0 stored as bytes in memory (estimated size 10.3 KB, free 589.6 KB)
17/05/20 21:27:15 INFO BlockManagerInfo: Added broadcast_6_piece0 in memory on localhost:55538 (size: 10.3 KB, free: 511.1 MB)
17/05/20 21:27:15 INFO SparkContext: Created broadcast 6 from broadcast at DAGScheduler.scala:1006
17/05/20 21:27:15 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 3 (MapPartitionsRDD[17] at sql at null:-2)
17/05/20 21:27:15 INFO TaskSchedulerImpl: Adding task set 3.0 with 2 tasks
17/05/20 21:27:15 INFO TaskSetManager: Starting task 0.0 in stage 3.0 (TID 3, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:27:15 INFO TaskSetManager: Starting task 1.0 in stage 3.0 (TID 4, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:27:15 INFO Executor: Running task 0.0 in stage 3.0 (TID 3)
17/05/20 21:27:15 INFO Executor: Running task 1.0 in stage 3.0 (TID 4)
17/05/20 21:27:15 INFO CacheManager: Partition rdd_14_1 not found, computing it
17/05/20 21:27:15 INFO CacheManager: Partition rdd_14_0 not found, computing it
17/05/20 21:27:15 INFO HadoopRDD: Input split: file:/C:/Users/d91067/AppData/Local/Temp/Rtmpo91v77/spark_serialize_23fd9360f2772a59264c2b1e865ca39cb72ec67532fe16d48d0ea5b134eb2ce8.csv:1011375+1011376
17/05/20 21:27:15 INFO HadoopRDD: Input split: file:/C:/Users/d91067/AppData/Local/Temp/Rtmpo91v77/spark_serialize_23fd9360f2772a59264c2b1e865ca39cb72ec67532fe16d48d0ea5b134eb2ce8.csv:0+1011375
17/05/20 21:27:15 INFO GenerateUnsafeProjection: Code generated in 15.296988 ms
17/05/20 21:27:15 INFO MemoryStore: Block rdd_14_0 stored as values in memory (estimated size 990.9 KB, free 1580.5 KB)
17/05/20 21:27:15 INFO MemoryStore: Block rdd_14_1 stored as values in memory (estimated size 988.3 KB, free 2.5 MB)
17/05/20 21:27:15 INFO BlockManagerInfo: Added rdd_14_0 in memory on localhost:55538 (size: 990.9 KB, free: 510.1 MB)
17/05/20 21:27:15 INFO BlockManagerInfo: Added rdd_14_1 in memory on localhost:55538 (size: 988.3 KB, free: 509.1 MB)
17/05/20 21:27:15 INFO GeneratePredicate: Code generated in 3.371734 ms
17/05/20 21:27:15 INFO GenerateColumnAccessor: Code generated in 11.254849 ms
17/05/20 21:27:15 INFO GenerateMutableProjection: Code generated in 4.331977 ms
17/05/20 21:27:15 INFO GenerateUnsafeProjection: Code generated in 4.349631 ms
17/05/20 21:27:15 INFO GenerateMutableProjection: Code generated in 6.985273 ms
17/05/20 21:27:15 INFO GenerateUnsafeRowJoiner: Code generated in 5.524174 ms
17/05/20 21:27:15 INFO BlockManagerInfo: Removed broadcast_4_piece0 on localhost:55538 in memory (size: 1947.0 B, free: 509.1 MB)
17/05/20 21:27:15 INFO GenerateUnsafeProjection: Code generated in 7.14374 ms
17/05/20 21:27:15 INFO ContextCleaner: Cleaned accumulator 5
17/05/20 21:27:15 INFO BlockManagerInfo: Removed broadcast_3_piece0 on localhost:55538 in memory (size: 19.3 KB, free: 509.1 MB)
17/05/20 21:27:15 INFO BlockManagerInfo: Removed broadcast_2_piece0 on localhost:55538 in memory (size: 1946.0 B, free: 509.1 MB)
17/05/20 21:27:15 INFO ContextCleaner: Cleaned accumulator 4
17/05/20 21:27:15 INFO BlockManagerInfo: Removed broadcast_1_piece0 on localhost:55538 in memory (size: 19.3 KB, free: 509.2 MB)
17/05/20 21:27:15 INFO BlockManagerInfo: Removed broadcast_0_piece0 on localhost:55538 in memory (size: 3.0 KB, free: 509.2 MB)
17/05/20 21:27:15 INFO ContextCleaner: Cleaned accumulator 3
17/05/20 21:27:15 INFO ContextCleaner: Cleaned accumulator 2
17/05/20 21:27:16 INFO Executor: Finished task 1.0 in stage 3.0 (TID 4). 5965 bytes result sent to driver
17/05/20 21:27:16 INFO Executor: Finished task 0.0 in stage 3.0 (TID 3). 7300 bytes result sent to driver
17/05/20 21:27:16 INFO TaskSetManager: Finished task 1.0 in stage 3.0 (TID 4) in 395 ms on localhost (1/2)
17/05/20 21:27:16 INFO TaskSetManager: Finished task 0.0 in stage 3.0 (TID 3) in 395 ms on localhost (2/2)
17/05/20 21:27:16 INFO TaskSchedulerImpl: Removed TaskSet 3.0, whose tasks have all completed, from pool 
17/05/20 21:27:16 INFO DAGScheduler: ShuffleMapStage 3 (sql at null:-2) finished in 0,395 s
17/05/20 21:27:16 INFO DAGScheduler: looking for newly runnable stages
17/05/20 21:27:16 INFO DAGScheduler: running: Set()
17/05/20 21:27:16 INFO DAGScheduler: waiting: Set(ResultStage 4)
17/05/20 21:27:16 INFO DAGScheduler: failed: Set()
17/05/20 21:27:16 INFO DAGScheduler: Submitting ResultStage 4 (MapPartitionsRDD[20] at sql at null:-2), which has no missing parents
17/05/20 21:27:16 INFO MemoryStore: Block broadcast_7 stored as values in memory (estimated size 9.3 KB, free 2.2 MB)
17/05/20 21:27:16 INFO MemoryStore: Block broadcast_7_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2.2 MB)
17/05/20 21:27:16 INFO BlockManagerInfo: Added broadcast_7_piece0 in memory on localhost:55538 (size: 4.6 KB, free: 509.2 MB)
17/05/20 21:27:16 INFO SparkContext: Created broadcast 7 from broadcast at DAGScheduler.scala:1006
17/05/20 21:27:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 4 (MapPartitionsRDD[20] at sql at null:-2)
17/05/20 21:27:16 INFO TaskSchedulerImpl: Adding task set 4.0 with 1 tasks
17/05/20 21:27:16 INFO TaskSetManager: Starting task 0.0 in stage 4.0 (TID 5, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/05/20 21:27:16 INFO Executor: Running task 0.0 in stage 4.0 (TID 5)
17/05/20 21:27:16 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/05/20 21:27:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/20 21:27:16 INFO GenerateMutableProjection: Code generated in 6.458966 ms
17/05/20 21:27:16 INFO GenerateMutableProjection: Code generated in 3.441935 ms
17/05/20 21:27:16 INFO Executor: Finished task 0.0 in stage 4.0 (TID 5). 1830 bytes result sent to driver
17/05/20 21:27:16 INFO TaskSetManager: Finished task 0.0 in stage 4.0 (TID 5) in 94 ms on localhost (1/1)
17/05/20 21:27:16 INFO TaskSchedulerImpl: Removed TaskSet 4.0, whose tasks have all completed, from pool 
17/05/20 21:27:16 INFO DAGScheduler: ResultStage 4 (sql at null:-2) finished in 0,094 s
17/05/20 21:27:16 INFO DAGScheduler: Job 3 finished: sql at null:-2, took 0,525434 s
17/05/20 21:27:16 INFO ParseDriver: Parsing command: SELECT count(*) FROM  `track_metadata`
17/05/20 21:27:16 INFO ParseDriver: Parse Completed
17/05/20 21:27:16 INFO SparkContext: Starting job: collect at utils.scala:195
17/05/20 21:27:16 INFO DAGScheduler: Registering RDD 24 (collect at utils.scala:195)
17/05/20 21:27:16 INFO DAGScheduler: Got job 4 (collect at utils.scala:195) with 1 output partitions
17/05/20 21:27:16 INFO DAGScheduler: Final stage: ResultStage 6 (collect at utils.scala:195)
17/05/20 21:27:16 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 5)
17/05/20 21:27:16 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 5)
17/05/20 21:27:16 INFO DAGScheduler: Submitting ShuffleMapStage 5 (MapPartitionsRDD[24] at collect at utils.scala:195), which has no missing parents
17/05/20 21:27:16 INFO MemoryStore: Block broadcast_8 stored as values in memory (estimated size 24.0 KB, free 2.2 MB)
17/05/20 21:27:16 INFO MemoryStore: Block broadcast_8_piece0 stored as bytes in memory (estimated size 10.3 KB, free 2.2 MB)
17/05/20 21:27:16 INFO BlockManagerInfo: Added broadcast_8_piece0 in memory on localhost:55538 (size: 10.3 KB, free: 509.1 MB)
17/05/20 21:27:16 INFO SparkContext: Created broadcast 8 from broadcast at DAGScheduler.scala:1006
17/05/20 21:27:16 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 5 (MapPartitionsRDD[24] at collect at utils.scala:195)
17/05/20 21:27:16 INFO TaskSchedulerImpl: Adding task set 5.0 with 2 tasks
17/05/20 21:27:16 INFO TaskSetManager: Starting task 0.0 in stage 5.0 (TID 6, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:27:16 INFO TaskSetManager: Starting task 1.0 in stage 5.0 (TID 7, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:27:16 INFO Executor: Running task 1.0 in stage 5.0 (TID 7)
17/05/20 21:27:16 INFO Executor: Running task 0.0 in stage 5.0 (TID 6)
17/05/20 21:27:16 INFO BlockManager: Found block rdd_14_1 locally
17/05/20 21:27:16 INFO BlockManager: Found block rdd_14_0 locally
17/05/20 21:27:16 INFO Executor: Finished task 1.0 in stage 5.0 (TID 7). 2679 bytes result sent to driver
17/05/20 21:27:16 INFO Executor: Finished task 0.0 in stage 5.0 (TID 6). 2679 bytes result sent to driver
17/05/20 21:27:16 INFO TaskSetManager: Finished task 1.0 in stage 5.0 (TID 7) in 16 ms on localhost (1/2)
17/05/20 21:27:16 INFO TaskSetManager: Finished task 0.0 in stage 5.0 (TID 6) in 16 ms on localhost (2/2)
17/05/20 21:27:16 INFO TaskSchedulerImpl: Removed TaskSet 5.0, whose tasks have all completed, from pool 
17/05/20 21:27:16 INFO DAGScheduler: ShuffleMapStage 5 (collect at utils.scala:195) finished in 0,016 s
17/05/20 21:27:16 INFO DAGScheduler: looking for newly runnable stages
17/05/20 21:27:16 INFO DAGScheduler: running: Set()
17/05/20 21:27:16 INFO DAGScheduler: waiting: Set(ResultStage 6)
17/05/20 21:27:16 INFO DAGScheduler: failed: Set()
17/05/20 21:27:16 INFO DAGScheduler: Submitting ResultStage 6 (MapPartitionsRDD[27] at collect at utils.scala:195), which has no missing parents
17/05/20 21:27:16 INFO MemoryStore: Block broadcast_9 stored as values in memory (estimated size 9.4 KB, free 2.2 MB)
17/05/20 21:27:16 INFO MemoryStore: Block broadcast_9_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2.2 MB)
17/05/20 21:27:16 INFO BlockManagerInfo: Added broadcast_9_piece0 in memory on localhost:55538 (size: 4.6 KB, free: 509.1 MB)
17/05/20 21:27:16 INFO SparkContext: Created broadcast 9 from broadcast at DAGScheduler.scala:1006
17/05/20 21:27:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 6 (MapPartitionsRDD[27] at collect at utils.scala:195)
17/05/20 21:27:16 INFO TaskSchedulerImpl: Adding task set 6.0 with 1 tasks
17/05/20 21:27:16 INFO TaskSetManager: Starting task 0.0 in stage 6.0 (TID 8, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/05/20 21:27:16 INFO Executor: Running task 0.0 in stage 6.0 (TID 8)
17/05/20 21:27:16 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/05/20 21:27:16 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/20 21:27:16 INFO Executor: Finished task 0.0 in stage 6.0 (TID 8). 1830 bytes result sent to driver
17/05/20 21:27:16 INFO TaskSetManager: Finished task 0.0 in stage 6.0 (TID 8) in 1 ms on localhost (1/1)
17/05/20 21:27:16 INFO TaskSchedulerImpl: Removed TaskSet 6.0, whose tasks have all completed, from pool 
17/05/20 21:27:16 INFO DAGScheduler: ResultStage 6 (collect at utils.scala:195) finished in 0,001 s
17/05/20 21:27:16 INFO DAGScheduler: Job 4 finished: collect at utils.scala:195, took 0,043699 s
17/05/20 21:27:16 INFO ParseDriver: Parsing command: SELECT *
FROM `track_metadata` AS `zzz1`
WHERE (0 = 1)
17/05/20 21:27:16 INFO ParseDriver: Parse Completed
17/05/20 21:27:16 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/05/20 21:27:16 INFO audit: ugi=d91067	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/05/20 21:27:16 INFO SparkContext: Starting job: collect at utils.scala:195
17/05/20 21:27:16 INFO DAGScheduler: Got job 5 (collect at utils.scala:195) with 1 output partitions
17/05/20 21:27:16 INFO DAGScheduler: Final stage: ResultStage 7 (collect at utils.scala:195)
17/05/20 21:27:16 INFO DAGScheduler: Parents of final stage: List()
17/05/20 21:27:16 INFO DAGScheduler: Missing parents: List()
17/05/20 21:27:16 INFO DAGScheduler: Submitting ResultStage 7 (MapPartitionsRDD[29] at collect at utils.scala:195), which has no missing parents
17/05/20 21:27:16 INFO MemoryStore: Block broadcast_10 stored as values in memory (estimated size 1968.0 B, free 2.3 MB)
17/05/20 21:27:16 INFO MemoryStore: Block broadcast_10_piece0 stored as bytes in memory (estimated size 1224.0 B, free 2.3 MB)
17/05/20 21:27:16 INFO BlockManagerInfo: Added broadcast_10_piece0 in memory on localhost:55538 (size: 1224.0 B, free: 509.1 MB)
17/05/20 21:27:16 INFO SparkContext: Created broadcast 10 from broadcast at DAGScheduler.scala:1006
17/05/20 21:27:16 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 7 (MapPartitionsRDD[29] at collect at utils.scala:195)
17/05/20 21:27:16 INFO TaskSchedulerImpl: Adding task set 7.0 with 1 tasks
17/05/20 21:27:16 INFO TaskSetManager: Starting task 0.0 in stage 7.0 (TID 9, localhost, partition 0,PROCESS_LOCAL, 2656 bytes)
17/05/20 21:27:16 INFO Executor: Running task 0.0 in stage 7.0 (TID 9)
17/05/20 21:27:16 INFO Executor: Finished task 0.0 in stage 7.0 (TID 9). 1268 bytes result sent to driver
17/05/20 21:27:16 INFO TaskSetManager: Finished task 0.0 in stage 7.0 (TID 9) in 31 ms on localhost (1/1)
17/05/20 21:27:16 INFO DAGScheduler: ResultStage 7 (collect at utils.scala:195) finished in 0,031 s
17/05/20 21:27:16 INFO TaskSchedulerImpl: Removed TaskSet 7.0, whose tasks have all completed, from pool 
17/05/20 21:27:16 INFO DAGScheduler: Job 5 finished: collect at utils.scala:195, took 0,008376 s
17/05/20 21:27:37 INFO HiveMetaStore: 0: get_tables: db=default pat=.*
17/05/20 21:27:37 INFO audit: ugi=d91067	ip=unknown-ip-addr	cmd=get_tables: db=default pat=.*	
17/05/20 21:27:37 INFO SparkContext: Starting job: collect at utils.scala:59
17/05/20 21:27:37 INFO DAGScheduler: Got job 6 (collect at utils.scala:59) with 1 output partitions
17/05/20 21:27:37 INFO DAGScheduler: Final stage: ResultStage 8 (collect at utils.scala:59)
17/05/20 21:27:37 INFO DAGScheduler: Parents of final stage: List()
17/05/20 21:27:37 INFO DAGScheduler: Missing parents: List()
17/05/20 21:27:37 INFO DAGScheduler: Submitting ResultStage 8 (MapPartitionsRDD[33] at map at utils.scala:56), which has no missing parents
17/05/20 21:27:37 INFO MemoryStore: Block broadcast_11 stored as values in memory (estimated size 5.4 KB, free 2.3 MB)
17/05/20 21:27:37 INFO MemoryStore: Block broadcast_11_piece0 stored as bytes in memory (estimated size 3.0 KB, free 2.3 MB)
17/05/20 21:27:37 INFO BlockManagerInfo: Added broadcast_11_piece0 in memory on localhost:55538 (size: 3.0 KB, free: 509.1 MB)
17/05/20 21:27:37 INFO SparkContext: Created broadcast 11 from broadcast at DAGScheduler.scala:1006
17/05/20 21:27:37 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 8 (MapPartitionsRDD[33] at map at utils.scala:56)
17/05/20 21:27:37 INFO TaskSchedulerImpl: Adding task set 8.0 with 1 tasks
17/05/20 21:27:37 INFO TaskSetManager: Starting task 0.0 in stage 8.0 (TID 10, localhost, partition 0,PROCESS_LOCAL, 2656 bytes)
17/05/20 21:27:37 INFO Executor: Running task 0.0 in stage 8.0 (TID 10)
17/05/20 21:27:37 INFO Executor: Finished task 0.0 in stage 8.0 (TID 10). 1077 bytes result sent to driver
17/05/20 21:27:37 INFO TaskSetManager: Finished task 0.0 in stage 8.0 (TID 10) in 0 ms on localhost (1/1)
17/05/20 21:27:37 INFO DAGScheduler: ResultStage 8 (collect at utils.scala:59) finished in 0,000 s
17/05/20 21:27:37 INFO TaskSchedulerImpl: Removed TaskSet 8.0, whose tasks have all completed, from pool 
17/05/20 21:27:37 INFO DAGScheduler: Job 6 finished: collect at utils.scala:59, took 0,007654 s
17/05/20 21:27:57 INFO ParseDriver: Parsing command: SELECT *
FROM `track_metadata` AS `zzz2`
WHERE (0 = 1)
17/05/20 21:27:57 INFO ParseDriver: Parse Completed
17/05/20 21:28:00 INFO ParseDriver: Parsing command: SELECT *
FROM `track_metadata`
17/05/20 21:28:00 INFO ParseDriver: Parse Completed
17/05/20 21:28:00 INFO SparkContext: Starting job: count at null:-2
17/05/20 21:28:00 INFO DAGScheduler: Registering RDD 36 (count at null:-2)
17/05/20 21:28:00 INFO DAGScheduler: Got job 7 (count at null:-2) with 1 output partitions
17/05/20 21:28:00 INFO DAGScheduler: Final stage: ResultStage 10 (count at null:-2)
17/05/20 21:28:00 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 9)
17/05/20 21:28:00 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 9)
17/05/20 21:28:00 INFO DAGScheduler: Submitting ShuffleMapStage 9 (MapPartitionsRDD[36] at count at null:-2), which has no missing parents
17/05/20 21:28:00 INFO MemoryStore: Block broadcast_12 stored as values in memory (estimated size 23.9 KB, free 2.3 MB)
17/05/20 21:28:00 INFO MemoryStore: Block broadcast_12_piece0 stored as bytes in memory (estimated size 10.3 KB, free 2.3 MB)
17/05/20 21:28:00 INFO BlockManagerInfo: Added broadcast_12_piece0 in memory on localhost:55538 (size: 10.3 KB, free: 509.1 MB)
17/05/20 21:28:00 INFO SparkContext: Created broadcast 12 from broadcast at DAGScheduler.scala:1006
17/05/20 21:28:00 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 9 (MapPartitionsRDD[36] at count at null:-2)
17/05/20 21:28:00 INFO TaskSchedulerImpl: Adding task set 9.0 with 2 tasks
17/05/20 21:28:00 INFO TaskSetManager: Starting task 0.0 in stage 9.0 (TID 11, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:28:00 INFO TaskSetManager: Starting task 1.0 in stage 9.0 (TID 12, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:28:00 INFO Executor: Running task 0.0 in stage 9.0 (TID 11)
17/05/20 21:28:00 INFO Executor: Running task 1.0 in stage 9.0 (TID 12)
17/05/20 21:28:00 INFO BlockManager: Found block rdd_14_1 locally
17/05/20 21:28:00 INFO BlockManager: Found block rdd_14_0 locally
17/05/20 21:28:00 INFO Executor: Finished task 1.0 in stage 9.0 (TID 12). 2679 bytes result sent to driver
17/05/20 21:28:00 INFO Executor: Finished task 0.0 in stage 9.0 (TID 11). 2679 bytes result sent to driver
17/05/20 21:28:00 INFO TaskSetManager: Finished task 1.0 in stage 9.0 (TID 12) in 31 ms on localhost (1/2)
17/05/20 21:28:00 INFO TaskSetManager: Finished task 0.0 in stage 9.0 (TID 11) in 31 ms on localhost (2/2)
17/05/20 21:28:00 INFO TaskSchedulerImpl: Removed TaskSet 9.0, whose tasks have all completed, from pool 
17/05/20 21:28:00 INFO DAGScheduler: ShuffleMapStage 9 (count at null:-2) finished in 0,031 s
17/05/20 21:28:00 INFO DAGScheduler: looking for newly runnable stages
17/05/20 21:28:00 INFO DAGScheduler: running: Set()
17/05/20 21:28:00 INFO DAGScheduler: waiting: Set(ResultStage 10)
17/05/20 21:28:00 INFO DAGScheduler: failed: Set()
17/05/20 21:28:00 INFO DAGScheduler: Submitting ResultStage 10 (MapPartitionsRDD[39] at count at null:-2), which has no missing parents
17/05/20 21:28:00 INFO MemoryStore: Block broadcast_13 stored as values in memory (estimated size 9.3 KB, free 2.3 MB)
17/05/20 21:28:00 INFO MemoryStore: Block broadcast_13_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2.3 MB)
17/05/20 21:28:00 INFO BlockManagerInfo: Added broadcast_13_piece0 in memory on localhost:55538 (size: 4.6 KB, free: 509.1 MB)
17/05/20 21:28:00 INFO SparkContext: Created broadcast 13 from broadcast at DAGScheduler.scala:1006
17/05/20 21:28:00 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 10 (MapPartitionsRDD[39] at count at null:-2)
17/05/20 21:28:00 INFO TaskSchedulerImpl: Adding task set 10.0 with 1 tasks
17/05/20 21:28:00 INFO TaskSetManager: Starting task 0.0 in stage 10.0 (TID 13, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/05/20 21:28:00 INFO Executor: Running task 0.0 in stage 10.0 (TID 13)
17/05/20 21:28:00 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/05/20 21:28:00 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/20 21:28:00 INFO Executor: Finished task 0.0 in stage 10.0 (TID 13). 1830 bytes result sent to driver
17/05/20 21:28:00 INFO TaskSetManager: Finished task 0.0 in stage 10.0 (TID 13) in 0 ms on localhost (1/1)
17/05/20 21:28:00 INFO DAGScheduler: ResultStage 10 (count at null:-2) finished in 0,015 s
17/05/20 21:28:00 INFO DAGScheduler: Job 7 finished: count at null:-2, took 0,047601 s
17/05/20 21:28:00 INFO TaskSchedulerImpl: Removed TaskSet 10.0, whose tasks have all completed, from pool 
17/05/20 21:29:43 INFO ParseDriver: Parsing command: SELECT *
FROM `track_metadata`
17/05/20 21:29:43 INFO ParseDriver: Parse Completed
17/05/20 21:29:43 INFO SparkContext: Starting job: count at null:-2
17/05/20 21:29:43 INFO DAGScheduler: Registering RDD 42 (count at null:-2)
17/05/20 21:29:43 INFO DAGScheduler: Got job 8 (count at null:-2) with 1 output partitions
17/05/20 21:29:43 INFO DAGScheduler: Final stage: ResultStage 12 (count at null:-2)
17/05/20 21:29:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 11)
17/05/20 21:29:43 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 11)
17/05/20 21:29:43 INFO DAGScheduler: Submitting ShuffleMapStage 11 (MapPartitionsRDD[42] at count at null:-2), which has no missing parents
17/05/20 21:29:43 INFO MemoryStore: Block broadcast_14 stored as values in memory (estimated size 23.9 KB, free 2.3 MB)
17/05/20 21:29:43 INFO MemoryStore: Block broadcast_14_piece0 stored as bytes in memory (estimated size 10.3 KB, free 2.3 MB)
17/05/20 21:29:43 INFO BlockManagerInfo: Added broadcast_14_piece0 in memory on localhost:55538 (size: 10.3 KB, free: 509.1 MB)
17/05/20 21:29:43 INFO SparkContext: Created broadcast 14 from broadcast at DAGScheduler.scala:1006
17/05/20 21:29:43 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 11 (MapPartitionsRDD[42] at count at null:-2)
17/05/20 21:29:43 INFO TaskSchedulerImpl: Adding task set 11.0 with 2 tasks
17/05/20 21:29:43 INFO TaskSetManager: Starting task 0.0 in stage 11.0 (TID 14, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:29:43 INFO TaskSetManager: Starting task 1.0 in stage 11.0 (TID 15, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:29:43 INFO Executor: Running task 0.0 in stage 11.0 (TID 14)
17/05/20 21:29:43 INFO Executor: Running task 1.0 in stage 11.0 (TID 15)
17/05/20 21:29:43 INFO BlockManager: Found block rdd_14_0 locally
17/05/20 21:29:43 INFO BlockManager: Found block rdd_14_1 locally
17/05/20 21:29:43 INFO Executor: Finished task 1.0 in stage 11.0 (TID 15). 2679 bytes result sent to driver
17/05/20 21:29:43 INFO TaskSetManager: Finished task 1.0 in stage 11.0 (TID 15) in 19 ms on localhost (1/2)
17/05/20 21:29:43 INFO Executor: Finished task 0.0 in stage 11.0 (TID 14). 2679 bytes result sent to driver
17/05/20 21:29:43 INFO TaskSetManager: Finished task 0.0 in stage 11.0 (TID 14) in 53 ms on localhost (2/2)
17/05/20 21:29:43 INFO DAGScheduler: ShuffleMapStage 11 (count at null:-2) finished in 0,053 s
17/05/20 21:29:43 INFO TaskSchedulerImpl: Removed TaskSet 11.0, whose tasks have all completed, from pool 
17/05/20 21:29:43 INFO DAGScheduler: looking for newly runnable stages
17/05/20 21:29:43 INFO DAGScheduler: running: Set()
17/05/20 21:29:43 INFO DAGScheduler: waiting: Set(ResultStage 12)
17/05/20 21:29:43 INFO DAGScheduler: failed: Set()
17/05/20 21:29:43 INFO DAGScheduler: Submitting ResultStage 12 (MapPartitionsRDD[45] at count at null:-2), which has no missing parents
17/05/20 21:29:43 INFO MemoryStore: Block broadcast_15 stored as values in memory (estimated size 9.3 KB, free 2.4 MB)
17/05/20 21:29:43 INFO MemoryStore: Block broadcast_15_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2.4 MB)
17/05/20 21:29:43 INFO BlockManagerInfo: Added broadcast_15_piece0 in memory on localhost:55538 (size: 4.6 KB, free: 509.1 MB)
17/05/20 21:29:43 INFO SparkContext: Created broadcast 15 from broadcast at DAGScheduler.scala:1006
17/05/20 21:29:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 12 (MapPartitionsRDD[45] at count at null:-2)
17/05/20 21:29:43 INFO TaskSchedulerImpl: Adding task set 12.0 with 1 tasks
17/05/20 21:29:43 INFO TaskSetManager: Starting task 0.0 in stage 12.0 (TID 16, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/05/20 21:29:43 INFO Executor: Running task 0.0 in stage 12.0 (TID 16)
17/05/20 21:29:43 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/05/20 21:29:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/20 21:29:43 INFO Executor: Finished task 0.0 in stage 12.0 (TID 16). 1830 bytes result sent to driver
17/05/20 21:29:43 INFO TaskSetManager: Finished task 0.0 in stage 12.0 (TID 16) in 16 ms on localhost (1/1)
17/05/20 21:29:43 INFO TaskSchedulerImpl: Removed TaskSet 12.0, whose tasks have all completed, from pool 
17/05/20 21:29:43 INFO DAGScheduler: ResultStage 12 (count at null:-2) finished in 0,016 s
17/05/20 21:29:43 INFO DAGScheduler: Job 8 finished: count at null:-2, took 0,048810 s
17/05/20 21:29:43 INFO ParseDriver: Parsing command: SELECT *
FROM `track_metadata`
17/05/20 21:29:43 INFO ParseDriver: Parse Completed
17/05/20 21:29:43 INFO SparkContext: Starting job: count at null:-2
17/05/20 21:29:43 INFO DAGScheduler: Registering RDD 48 (count at null:-2)
17/05/20 21:29:43 INFO DAGScheduler: Got job 9 (count at null:-2) with 1 output partitions
17/05/20 21:29:43 INFO DAGScheduler: Final stage: ResultStage 14 (count at null:-2)
17/05/20 21:29:43 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 13)
17/05/20 21:29:43 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 13)
17/05/20 21:29:43 INFO DAGScheduler: Submitting ShuffleMapStage 13 (MapPartitionsRDD[48] at count at null:-2), which has no missing parents
17/05/20 21:29:43 INFO MemoryStore: Block broadcast_16 stored as values in memory (estimated size 23.9 KB, free 2.4 MB)
17/05/20 21:29:43 INFO MemoryStore: Block broadcast_16_piece0 stored as bytes in memory (estimated size 10.3 KB, free 2.4 MB)
17/05/20 21:29:43 INFO BlockManagerInfo: Added broadcast_16_piece0 in memory on localhost:55538 (size: 10.3 KB, free: 509.1 MB)
17/05/20 21:29:43 INFO SparkContext: Created broadcast 16 from broadcast at DAGScheduler.scala:1006
17/05/20 21:29:43 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 13 (MapPartitionsRDD[48] at count at null:-2)
17/05/20 21:29:43 INFO TaskSchedulerImpl: Adding task set 13.0 with 2 tasks
17/05/20 21:29:43 INFO TaskSetManager: Starting task 0.0 in stage 13.0 (TID 17, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:29:43 INFO TaskSetManager: Starting task 1.0 in stage 13.0 (TID 18, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:29:43 INFO Executor: Running task 1.0 in stage 13.0 (TID 18)
17/05/20 21:29:43 INFO Executor: Running task 0.0 in stage 13.0 (TID 17)
17/05/20 21:29:43 INFO BlockManager: Found block rdd_14_1 locally
17/05/20 21:29:43 INFO BlockManager: Found block rdd_14_0 locally
17/05/20 21:29:43 INFO Executor: Finished task 1.0 in stage 13.0 (TID 18). 2679 bytes result sent to driver
17/05/20 21:29:43 INFO TaskSetManager: Finished task 1.0 in stage 13.0 (TID 18) in 1 ms on localhost (1/2)
17/05/20 21:29:43 INFO Executor: Finished task 0.0 in stage 13.0 (TID 17). 2679 bytes result sent to driver
17/05/20 21:29:43 INFO DAGScheduler: ShuffleMapStage 13 (count at null:-2) finished in 0,001 s
17/05/20 21:29:43 INFO TaskSetManager: Finished task 0.0 in stage 13.0 (TID 17) in 1 ms on localhost (2/2)
17/05/20 21:29:43 INFO TaskSchedulerImpl: Removed TaskSet 13.0, whose tasks have all completed, from pool 
17/05/20 21:29:43 INFO DAGScheduler: looking for newly runnable stages
17/05/20 21:29:43 INFO DAGScheduler: running: Set()
17/05/20 21:29:43 INFO DAGScheduler: waiting: Set(ResultStage 14)
17/05/20 21:29:43 INFO DAGScheduler: failed: Set()
17/05/20 21:29:43 INFO DAGScheduler: Submitting ResultStage 14 (MapPartitionsRDD[51] at count at null:-2), which has no missing parents
17/05/20 21:29:43 INFO MemoryStore: Block broadcast_17 stored as values in memory (estimated size 9.3 KB, free 2.4 MB)
17/05/20 21:29:43 INFO MemoryStore: Block broadcast_17_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2.4 MB)
17/05/20 21:29:43 INFO BlockManagerInfo: Added broadcast_17_piece0 in memory on localhost:55538 (size: 4.6 KB, free: 509.1 MB)
17/05/20 21:29:43 INFO SparkContext: Created broadcast 17 from broadcast at DAGScheduler.scala:1006
17/05/20 21:29:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 14 (MapPartitionsRDD[51] at count at null:-2)
17/05/20 21:29:43 INFO TaskSchedulerImpl: Adding task set 14.0 with 1 tasks
17/05/20 21:29:43 INFO TaskSetManager: Starting task 0.0 in stage 14.0 (TID 19, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/05/20 21:29:43 INFO Executor: Running task 0.0 in stage 14.0 (TID 19)
17/05/20 21:29:43 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/05/20 21:29:43 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/20 21:29:43 INFO Executor: Finished task 0.0 in stage 14.0 (TID 19). 1830 bytes result sent to driver
17/05/20 21:29:43 INFO TaskSetManager: Finished task 0.0 in stage 14.0 (TID 19) in 32 ms on localhost (1/1)
17/05/20 21:29:43 INFO DAGScheduler: ResultStage 14 (count at null:-2) finished in 0,032 s
17/05/20 21:29:43 INFO TaskSchedulerImpl: Removed TaskSet 14.0, whose tasks have all completed, from pool 
17/05/20 21:29:43 INFO DAGScheduler: Job 9 finished: count at null:-2, took 0,039620 s
17/05/20 21:29:43 INFO ParseDriver: Parsing command: SELECT *
FROM `track_metadata`
LIMIT 5
17/05/20 21:29:43 INFO ParseDriver: Parse Completed
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 37
17/05/20 21:29:43 INFO BlockManagerInfo: Removed broadcast_7_piece0 on localhost:55538 in memory (size: 4.6 KB, free: 509.1 MB)
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 16
17/05/20 21:29:43 INFO BlockManagerInfo: Removed broadcast_6_piece0 on localhost:55538 in memory (size: 10.3 KB, free: 509.1 MB)
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 15
17/05/20 21:29:43 INFO ContextCleaner: Cleaned shuffle 0
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 14
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 13
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 12
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 11
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 10
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 9
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 8
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 7
17/05/20 21:29:43 INFO BlockManagerInfo: Removed broadcast_17_piece0 on localhost:55538 in memory (size: 4.6 KB, free: 509.1 MB)
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 59
17/05/20 21:29:43 INFO BlockManagerInfo: Removed broadcast_16_piece0 on localhost:55538 in memory (size: 10.3 KB, free: 509.1 MB)
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 58
17/05/20 21:29:43 INFO ContextCleaner: Cleaned shuffle 4
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 57
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 56
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 55
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 54
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 53
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 52
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 51
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 50
17/05/20 21:29:43 INFO BlockManagerInfo: Removed broadcast_15_piece0 on localhost:55538 in memory (size: 4.6 KB, free: 509.1 MB)
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 49
17/05/20 21:29:43 INFO BlockManagerInfo: Removed broadcast_14_piece0 on localhost:55538 in memory (size: 10.3 KB, free: 509.1 MB)
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 48
17/05/20 21:29:43 INFO ContextCleaner: Cleaned shuffle 3
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 47
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 46
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 45
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 44
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 43
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 42
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 41
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 40
17/05/20 21:29:43 INFO BlockManagerInfo: Removed broadcast_13_piece0 on localhost:55538 in memory (size: 4.6 KB, free: 509.1 MB)
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 39
17/05/20 21:29:43 INFO BlockManagerInfo: Removed broadcast_12_piece0 on localhost:55538 in memory (size: 10.3 KB, free: 509.2 MB)
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 38
17/05/20 21:29:43 INFO ContextCleaner: Cleaned shuffle 2
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 36
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 35
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 34
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 33
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 32
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 31
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 30
17/05/20 21:29:43 INFO BlockManagerInfo: Removed broadcast_11_piece0 on localhost:55538 in memory (size: 3.0 KB, free: 509.2 MB)
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 29
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 28
17/05/20 21:29:43 INFO BlockManagerInfo: Removed broadcast_10_piece0 on localhost:55538 in memory (size: 1224.0 B, free: 509.2 MB)
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 27
17/05/20 21:29:43 INFO BlockManagerInfo: Removed broadcast_9_piece0 on localhost:55538 in memory (size: 4.6 KB, free: 509.2 MB)
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 26
17/05/20 21:29:43 INFO BlockManagerInfo: Removed broadcast_8_piece0 on localhost:55538 in memory (size: 10.3 KB, free: 509.2 MB)
17/05/20 21:29:43 INFO ContextCleaner: Cleaned accumulator 25
17/05/20 21:29:43 INFO ContextCleaner: Cleaned shuffle 1
17/05/20 21:29:43 INFO SparkContext: Starting job: collect at utils.scala:195
17/05/20 21:29:43 INFO DAGScheduler: Got job 10 (collect at utils.scala:195) with 1 output partitions
17/05/20 21:29:43 INFO DAGScheduler: Final stage: ResultStage 15 (collect at utils.scala:195)
17/05/20 21:29:43 INFO DAGScheduler: Parents of final stage: List()
17/05/20 21:29:43 INFO DAGScheduler: Missing parents: List()
17/05/20 21:29:43 INFO DAGScheduler: Submitting ResultStage 15 (MapPartitionsRDD[54] at collect at utils.scala:195), which has no missing parents
17/05/20 21:29:43 INFO MemoryStore: Block broadcast_18 stored as values in memory (estimated size 19.6 KB, free 2.2 MB)
17/05/20 21:29:43 INFO MemoryStore: Block broadcast_18_piece0 stored as bytes in memory (estimated size 8.5 KB, free 2.2 MB)
17/05/20 21:29:43 INFO BlockManagerInfo: Added broadcast_18_piece0 in memory on localhost:55538 (size: 8.5 KB, free: 509.2 MB)
17/05/20 21:29:43 INFO SparkContext: Created broadcast 18 from broadcast at DAGScheduler.scala:1006
17/05/20 21:29:43 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 15 (MapPartitionsRDD[54] at collect at utils.scala:195)
17/05/20 21:29:43 INFO TaskSchedulerImpl: Adding task set 15.0 with 1 tasks
17/05/20 21:29:43 INFO TaskSetManager: Starting task 0.0 in stage 15.0 (TID 20, localhost, partition 0,PROCESS_LOCAL, 2482 bytes)
17/05/20 21:29:43 INFO Executor: Running task 0.0 in stage 15.0 (TID 20)
17/05/20 21:29:43 INFO BlockManager: Found block rdd_14_0 locally
17/05/20 21:29:43 INFO GenerateColumnAccessor: Code generated in 13.954125 ms
17/05/20 21:29:43 INFO GenerateSafeProjection: Code generated in 7.08257 ms
17/05/20 21:29:43 INFO Executor: Finished task 0.0 in stage 15.0 (TID 20). 9793 bytes result sent to driver
17/05/20 21:29:43 INFO TaskSetManager: Finished task 0.0 in stage 15.0 (TID 20) in 32 ms on localhost (1/1)
17/05/20 21:29:43 INFO DAGScheduler: ResultStage 15 (collect at utils.scala:195) finished in 0,032 s
17/05/20 21:29:43 INFO TaskSchedulerImpl: Removed TaskSet 15.0, whose tasks have all completed, from pool 
17/05/20 21:29:43 INFO DAGScheduler: Job 10 finished: collect at utils.scala:195, took 0,041581 s
17/05/20 21:29:58 INFO ParseDriver: Parsing command: SELECT *
FROM `track_metadata`
17/05/20 21:29:58 INFO ParseDriver: Parse Completed
17/05/20 21:29:58 INFO SparkContext: Starting job: count at null:-2
17/05/20 21:29:58 INFO DAGScheduler: Registering RDD 57 (count at null:-2)
17/05/20 21:29:58 INFO DAGScheduler: Got job 11 (count at null:-2) with 1 output partitions
17/05/20 21:29:58 INFO DAGScheduler: Final stage: ResultStage 17 (count at null:-2)
17/05/20 21:29:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 16)
17/05/20 21:29:58 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 16)
17/05/20 21:29:58 INFO DAGScheduler: Submitting ShuffleMapStage 16 (MapPartitionsRDD[57] at count at null:-2), which has no missing parents
17/05/20 21:29:58 INFO MemoryStore: Block broadcast_19 stored as values in memory (estimated size 23.9 KB, free 2.2 MB)
17/05/20 21:29:58 INFO MemoryStore: Block broadcast_19_piece0 stored as bytes in memory (estimated size 10.3 KB, free 2.2 MB)
17/05/20 21:29:58 INFO BlockManagerInfo: Added broadcast_19_piece0 in memory on localhost:55538 (size: 10.3 KB, free: 509.2 MB)
17/05/20 21:29:58 INFO SparkContext: Created broadcast 19 from broadcast at DAGScheduler.scala:1006
17/05/20 21:29:58 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 16 (MapPartitionsRDD[57] at count at null:-2)
17/05/20 21:29:58 INFO TaskSchedulerImpl: Adding task set 16.0 with 2 tasks
17/05/20 21:29:58 INFO TaskSetManager: Starting task 0.0 in stage 16.0 (TID 21, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:29:58 INFO TaskSetManager: Starting task 1.0 in stage 16.0 (TID 22, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:29:58 INFO Executor: Running task 1.0 in stage 16.0 (TID 22)
17/05/20 21:29:58 INFO Executor: Running task 0.0 in stage 16.0 (TID 21)
17/05/20 21:29:58 INFO BlockManager: Found block rdd_14_1 locally
17/05/20 21:29:58 INFO BlockManager: Found block rdd_14_0 locally
17/05/20 21:29:58 INFO Executor: Finished task 1.0 in stage 16.0 (TID 22). 2679 bytes result sent to driver
17/05/20 21:29:58 INFO TaskSetManager: Finished task 1.0 in stage 16.0 (TID 22) in 31 ms on localhost (1/2)
17/05/20 21:29:58 INFO Executor: Finished task 0.0 in stage 16.0 (TID 21). 2679 bytes result sent to driver
17/05/20 21:29:58 INFO TaskSetManager: Finished task 0.0 in stage 16.0 (TID 21) in 31 ms on localhost (2/2)
17/05/20 21:29:58 INFO TaskSchedulerImpl: Removed TaskSet 16.0, whose tasks have all completed, from pool 
17/05/20 21:29:58 INFO DAGScheduler: ShuffleMapStage 16 (count at null:-2) finished in 0,031 s
17/05/20 21:29:58 INFO DAGScheduler: looking for newly runnable stages
17/05/20 21:29:58 INFO DAGScheduler: running: Set()
17/05/20 21:29:58 INFO DAGScheduler: waiting: Set(ResultStage 17)
17/05/20 21:29:58 INFO DAGScheduler: failed: Set()
17/05/20 21:29:58 INFO DAGScheduler: Submitting ResultStage 17 (MapPartitionsRDD[60] at count at null:-2), which has no missing parents
17/05/20 21:29:58 INFO MemoryStore: Block broadcast_20 stored as values in memory (estimated size 9.3 KB, free 2.2 MB)
17/05/20 21:29:58 INFO MemoryStore: Block broadcast_20_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2.2 MB)
17/05/20 21:29:58 INFO BlockManagerInfo: Added broadcast_20_piece0 in memory on localhost:55538 (size: 4.6 KB, free: 509.2 MB)
17/05/20 21:29:58 INFO SparkContext: Created broadcast 20 from broadcast at DAGScheduler.scala:1006
17/05/20 21:29:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 17 (MapPartitionsRDD[60] at count at null:-2)
17/05/20 21:29:58 INFO TaskSchedulerImpl: Adding task set 17.0 with 1 tasks
17/05/20 21:29:58 INFO TaskSetManager: Starting task 0.0 in stage 17.0 (TID 23, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/05/20 21:29:58 INFO Executor: Running task 0.0 in stage 17.0 (TID 23)
17/05/20 21:29:58 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/05/20 21:29:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/20 21:29:58 INFO Executor: Finished task 0.0 in stage 17.0 (TID 23). 1830 bytes result sent to driver
17/05/20 21:29:58 INFO TaskSetManager: Finished task 0.0 in stage 17.0 (TID 23) in 0 ms on localhost (1/1)
17/05/20 21:29:58 INFO DAGScheduler: ResultStage 17 (count at null:-2) finished in 0,000 s
17/05/20 21:29:58 INFO TaskSchedulerImpl: Removed TaskSet 17.0, whose tasks have all completed, from pool 
17/05/20 21:29:58 INFO DAGScheduler: Job 11 finished: count at null:-2, took 0,038725 s
17/05/20 21:29:58 INFO ParseDriver: Parsing command: SELECT *
FROM `track_metadata`
17/05/20 21:29:58 INFO ParseDriver: Parse Completed
17/05/20 21:29:58 INFO SparkContext: Starting job: count at null:-2
17/05/20 21:29:58 INFO DAGScheduler: Registering RDD 63 (count at null:-2)
17/05/20 21:29:58 INFO DAGScheduler: Got job 12 (count at null:-2) with 1 output partitions
17/05/20 21:29:58 INFO DAGScheduler: Final stage: ResultStage 19 (count at null:-2)
17/05/20 21:29:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 18)
17/05/20 21:29:58 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 18)
17/05/20 21:29:58 INFO DAGScheduler: Submitting ShuffleMapStage 18 (MapPartitionsRDD[63] at count at null:-2), which has no missing parents
17/05/20 21:29:58 INFO MemoryStore: Block broadcast_21 stored as values in memory (estimated size 23.9 KB, free 2.3 MB)
17/05/20 21:29:58 INFO MemoryStore: Block broadcast_21_piece0 stored as bytes in memory (estimated size 10.3 KB, free 2.3 MB)
17/05/20 21:29:58 INFO BlockManagerInfo: Added broadcast_21_piece0 in memory on localhost:55538 (size: 10.3 KB, free: 509.1 MB)
17/05/20 21:29:58 INFO SparkContext: Created broadcast 21 from broadcast at DAGScheduler.scala:1006
17/05/20 21:29:58 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 18 (MapPartitionsRDD[63] at count at null:-2)
17/05/20 21:29:58 INFO TaskSchedulerImpl: Adding task set 18.0 with 2 tasks
17/05/20 21:29:58 INFO TaskSetManager: Starting task 0.0 in stage 18.0 (TID 24, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:29:58 INFO TaskSetManager: Starting task 1.0 in stage 18.0 (TID 25, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:29:58 INFO Executor: Running task 0.0 in stage 18.0 (TID 24)
17/05/20 21:29:58 INFO Executor: Running task 1.0 in stage 18.0 (TID 25)
17/05/20 21:29:58 INFO BlockManager: Found block rdd_14_0 locally
17/05/20 21:29:58 INFO BlockManager: Found block rdd_14_1 locally
17/05/20 21:29:58 INFO Executor: Finished task 1.0 in stage 18.0 (TID 25). 2679 bytes result sent to driver
17/05/20 21:29:58 INFO Executor: Finished task 0.0 in stage 18.0 (TID 24). 2679 bytes result sent to driver
17/05/20 21:29:58 INFO TaskSetManager: Finished task 1.0 in stage 18.0 (TID 25) in 0 ms on localhost (1/2)
17/05/20 21:29:58 INFO TaskSetManager: Finished task 0.0 in stage 18.0 (TID 24) in 0 ms on localhost (2/2)
17/05/20 21:29:58 INFO TaskSchedulerImpl: Removed TaskSet 18.0, whose tasks have all completed, from pool 
17/05/20 21:29:58 INFO DAGScheduler: ShuffleMapStage 18 (count at null:-2) finished in 0,000 s
17/05/20 21:29:58 INFO DAGScheduler: looking for newly runnable stages
17/05/20 21:29:58 INFO DAGScheduler: running: Set()
17/05/20 21:29:58 INFO DAGScheduler: waiting: Set(ResultStage 19)
17/05/20 21:29:58 INFO DAGScheduler: failed: Set()
17/05/20 21:29:58 INFO DAGScheduler: Submitting ResultStage 19 (MapPartitionsRDD[66] at count at null:-2), which has no missing parents
17/05/20 21:29:58 INFO MemoryStore: Block broadcast_22 stored as values in memory (estimated size 9.3 KB, free 2.3 MB)
17/05/20 21:29:58 INFO MemoryStore: Block broadcast_22_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2.3 MB)
17/05/20 21:29:58 INFO BlockManagerInfo: Added broadcast_22_piece0 in memory on localhost:55538 (size: 4.6 KB, free: 509.1 MB)
17/05/20 21:29:58 INFO SparkContext: Created broadcast 22 from broadcast at DAGScheduler.scala:1006
17/05/20 21:29:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 19 (MapPartitionsRDD[66] at count at null:-2)
17/05/20 21:29:58 INFO TaskSchedulerImpl: Adding task set 19.0 with 1 tasks
17/05/20 21:29:58 INFO TaskSetManager: Starting task 0.0 in stage 19.0 (TID 26, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/05/20 21:29:58 INFO Executor: Running task 0.0 in stage 19.0 (TID 26)
17/05/20 21:29:58 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/05/20 21:29:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/20 21:29:58 INFO Executor: Finished task 0.0 in stage 19.0 (TID 26). 1830 bytes result sent to driver
17/05/20 21:29:58 INFO TaskSetManager: Finished task 0.0 in stage 19.0 (TID 26) in 0 ms on localhost (1/1)
17/05/20 21:29:58 INFO TaskSchedulerImpl: Removed TaskSet 19.0, whose tasks have all completed, from pool 
17/05/20 21:29:58 INFO DAGScheduler: ResultStage 19 (count at null:-2) finished in 0,000 s
17/05/20 21:29:58 INFO DAGScheduler: Job 12 finished: count at null:-2, took 0,029135 s
17/05/20 21:29:58 INFO ParseDriver: Parsing command: SELECT *
FROM `track_metadata`
17/05/20 21:29:58 INFO ParseDriver: Parse Completed
17/05/20 21:29:58 INFO SparkContext: Starting job: count at null:-2
17/05/20 21:29:58 INFO DAGScheduler: Registering RDD 69 (count at null:-2)
17/05/20 21:29:58 INFO DAGScheduler: Got job 13 (count at null:-2) with 1 output partitions
17/05/20 21:29:58 INFO DAGScheduler: Final stage: ResultStage 21 (count at null:-2)
17/05/20 21:29:58 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 20)
17/05/20 21:29:58 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 20)
17/05/20 21:29:58 INFO DAGScheduler: Submitting ShuffleMapStage 20 (MapPartitionsRDD[69] at count at null:-2), which has no missing parents
17/05/20 21:29:58 INFO MemoryStore: Block broadcast_23 stored as values in memory (estimated size 23.9 KB, free 2.3 MB)
17/05/20 21:29:58 INFO MemoryStore: Block broadcast_23_piece0 stored as bytes in memory (estimated size 10.3 KB, free 2.3 MB)
17/05/20 21:29:58 INFO BlockManagerInfo: Added broadcast_23_piece0 in memory on localhost:55538 (size: 10.3 KB, free: 509.1 MB)
17/05/20 21:29:58 INFO SparkContext: Created broadcast 23 from broadcast at DAGScheduler.scala:1006
17/05/20 21:29:58 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 20 (MapPartitionsRDD[69] at count at null:-2)
17/05/20 21:29:58 INFO TaskSchedulerImpl: Adding task set 20.0 with 2 tasks
17/05/20 21:29:58 INFO TaskSetManager: Starting task 0.0 in stage 20.0 (TID 27, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:29:58 INFO TaskSetManager: Starting task 1.0 in stage 20.0 (TID 28, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:29:58 INFO Executor: Running task 0.0 in stage 20.0 (TID 27)
17/05/20 21:29:58 INFO Executor: Running task 1.0 in stage 20.0 (TID 28)
17/05/20 21:29:58 INFO BlockManager: Found block rdd_14_0 locally
17/05/20 21:29:58 INFO BlockManager: Found block rdd_14_1 locally
17/05/20 21:29:58 INFO Executor: Finished task 1.0 in stage 20.0 (TID 28). 2679 bytes result sent to driver
17/05/20 21:29:58 INFO Executor: Finished task 0.0 in stage 20.0 (TID 27). 2679 bytes result sent to driver
17/05/20 21:29:58 INFO TaskSetManager: Finished task 1.0 in stage 20.0 (TID 28) in 15 ms on localhost (1/2)
17/05/20 21:29:58 INFO TaskSetManager: Finished task 0.0 in stage 20.0 (TID 27) in 15 ms on localhost (2/2)
17/05/20 21:29:58 INFO DAGScheduler: ShuffleMapStage 20 (count at null:-2) finished in 0,015 s
17/05/20 21:29:58 INFO TaskSchedulerImpl: Removed TaskSet 20.0, whose tasks have all completed, from pool 
17/05/20 21:29:58 INFO DAGScheduler: looking for newly runnable stages
17/05/20 21:29:58 INFO DAGScheduler: running: Set()
17/05/20 21:29:58 INFO DAGScheduler: waiting: Set(ResultStage 21)
17/05/20 21:29:58 INFO DAGScheduler: failed: Set()
17/05/20 21:29:58 INFO DAGScheduler: Submitting ResultStage 21 (MapPartitionsRDD[72] at count at null:-2), which has no missing parents
17/05/20 21:29:58 INFO MemoryStore: Block broadcast_24 stored as values in memory (estimated size 9.3 KB, free 2.3 MB)
17/05/20 21:29:58 INFO MemoryStore: Block broadcast_24_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2.3 MB)
17/05/20 21:29:58 INFO BlockManagerInfo: Added broadcast_24_piece0 in memory on localhost:55538 (size: 4.6 KB, free: 509.1 MB)
17/05/20 21:29:58 INFO SparkContext: Created broadcast 24 from broadcast at DAGScheduler.scala:1006
17/05/20 21:29:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 21 (MapPartitionsRDD[72] at count at null:-2)
17/05/20 21:29:58 INFO TaskSchedulerImpl: Adding task set 21.0 with 1 tasks
17/05/20 21:29:58 INFO TaskSetManager: Starting task 0.0 in stage 21.0 (TID 29, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/05/20 21:29:58 INFO Executor: Running task 0.0 in stage 21.0 (TID 29)
17/05/20 21:29:58 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/05/20 21:29:58 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/20 21:29:58 INFO Executor: Finished task 0.0 in stage 21.0 (TID 29). 1830 bytes result sent to driver
17/05/20 21:29:58 INFO TaskSetManager: Finished task 0.0 in stage 21.0 (TID 29) in 0 ms on localhost (1/1)
17/05/20 21:29:58 INFO DAGScheduler: ResultStage 21 (count at null:-2) finished in 0,016 s
17/05/20 21:29:58 INFO TaskSchedulerImpl: Removed TaskSet 21.0, whose tasks have all completed, from pool 
17/05/20 21:29:58 INFO DAGScheduler: Job 13 finished: count at null:-2, took 0,026942 s
17/05/20 21:29:58 INFO ParseDriver: Parsing command: SELECT *
FROM `track_metadata`
LIMIT 46
17/05/20 21:29:58 INFO ParseDriver: Parse Completed
17/05/20 21:29:58 INFO SparkContext: Starting job: collect at utils.scala:195
17/05/20 21:29:58 INFO DAGScheduler: Got job 14 (collect at utils.scala:195) with 1 output partitions
17/05/20 21:29:58 INFO DAGScheduler: Final stage: ResultStage 22 (collect at utils.scala:195)
17/05/20 21:29:58 INFO DAGScheduler: Parents of final stage: List()
17/05/20 21:29:58 INFO DAGScheduler: Missing parents: List()
17/05/20 21:29:58 INFO DAGScheduler: Submitting ResultStage 22 (MapPartitionsRDD[75] at collect at utils.scala:195), which has no missing parents
17/05/20 21:29:58 INFO MemoryStore: Block broadcast_25 stored as values in memory (estimated size 19.6 KB, free 2.3 MB)
17/05/20 21:29:58 INFO MemoryStore: Block broadcast_25_piece0 stored as bytes in memory (estimated size 8.5 KB, free 2.4 MB)
17/05/20 21:29:58 INFO BlockManagerInfo: Added broadcast_25_piece0 in memory on localhost:55538 (size: 8.5 KB, free: 509.1 MB)
17/05/20 21:29:58 INFO SparkContext: Created broadcast 25 from broadcast at DAGScheduler.scala:1006
17/05/20 21:29:58 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 22 (MapPartitionsRDD[75] at collect at utils.scala:195)
17/05/20 21:29:58 INFO TaskSchedulerImpl: Adding task set 22.0 with 1 tasks
17/05/20 21:29:58 INFO TaskSetManager: Starting task 0.0 in stage 22.0 (TID 30, localhost, partition 0,PROCESS_LOCAL, 2482 bytes)
17/05/20 21:29:58 INFO Executor: Running task 0.0 in stage 22.0 (TID 30)
17/05/20 21:29:58 INFO BlockManager: Found block rdd_14_0 locally
17/05/20 21:29:58 INFO Executor: Finished task 0.0 in stage 22.0 (TID 30). 76938 bytes result sent to driver
17/05/20 21:29:58 INFO TaskSetManager: Finished task 0.0 in stage 22.0 (TID 30) in 0 ms on localhost (1/1)
17/05/20 21:29:58 INFO DAGScheduler: ResultStage 22 (collect at utils.scala:195) finished in 0,000 s
17/05/20 21:29:58 INFO TaskSchedulerImpl: Removed TaskSet 22.0, whose tasks have all completed, from pool 
17/05/20 21:29:58 INFO DAGScheduler: Job 14 finished: collect at utils.scala:195, took 0,014326 s
17/05/20 21:30:47 INFO ParseDriver: Parsing command: SELECT *
FROM `track_metadata`
17/05/20 21:30:47 INFO ParseDriver: Parse Completed
17/05/20 21:30:48 INFO SparkContext: Starting job: count at null:-2
17/05/20 21:30:48 INFO DAGScheduler: Registering RDD 78 (count at null:-2)
17/05/20 21:30:48 INFO DAGScheduler: Got job 15 (count at null:-2) with 1 output partitions
17/05/20 21:30:48 INFO DAGScheduler: Final stage: ResultStage 24 (count at null:-2)
17/05/20 21:30:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 23)
17/05/20 21:30:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 23)
17/05/20 21:30:48 INFO DAGScheduler: Submitting ShuffleMapStage 23 (MapPartitionsRDD[78] at count at null:-2), which has no missing parents
17/05/20 21:30:48 INFO MemoryStore: Block broadcast_26 stored as values in memory (estimated size 23.9 KB, free 2.4 MB)
17/05/20 21:30:48 INFO MemoryStore: Block broadcast_26_piece0 stored as bytes in memory (estimated size 10.3 KB, free 2.4 MB)
17/05/20 21:30:48 INFO BlockManagerInfo: Added broadcast_26_piece0 in memory on localhost:55538 (size: 10.3 KB, free: 509.1 MB)
17/05/20 21:30:48 INFO SparkContext: Created broadcast 26 from broadcast at DAGScheduler.scala:1006
17/05/20 21:30:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 23 (MapPartitionsRDD[78] at count at null:-2)
17/05/20 21:30:48 INFO TaskSchedulerImpl: Adding task set 23.0 with 2 tasks
17/05/20 21:30:48 INFO TaskSetManager: Starting task 0.0 in stage 23.0 (TID 31, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:30:48 INFO TaskSetManager: Starting task 1.0 in stage 23.0 (TID 32, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:30:48 INFO Executor: Running task 0.0 in stage 23.0 (TID 31)
17/05/20 21:30:48 INFO Executor: Running task 1.0 in stage 23.0 (TID 32)
17/05/20 21:30:48 INFO BlockManager: Found block rdd_14_1 locally
17/05/20 21:30:48 INFO BlockManager: Found block rdd_14_0 locally
17/05/20 21:30:48 INFO Executor: Finished task 0.0 in stage 23.0 (TID 31). 2679 bytes result sent to driver
17/05/20 21:30:48 INFO Executor: Finished task 1.0 in stage 23.0 (TID 32). 2679 bytes result sent to driver
17/05/20 21:30:48 INFO TaskSetManager: Finished task 0.0 in stage 23.0 (TID 31) in 15 ms on localhost (1/2)
17/05/20 21:30:48 INFO TaskSetManager: Finished task 1.0 in stage 23.0 (TID 32) in 15 ms on localhost (2/2)
17/05/20 21:30:48 INFO TaskSchedulerImpl: Removed TaskSet 23.0, whose tasks have all completed, from pool 
17/05/20 21:30:48 INFO DAGScheduler: ShuffleMapStage 23 (count at null:-2) finished in 0,015 s
17/05/20 21:30:48 INFO DAGScheduler: looking for newly runnable stages
17/05/20 21:30:48 INFO DAGScheduler: running: Set()
17/05/20 21:30:48 INFO DAGScheduler: waiting: Set(ResultStage 24)
17/05/20 21:30:48 INFO DAGScheduler: failed: Set()
17/05/20 21:30:48 INFO DAGScheduler: Submitting ResultStage 24 (MapPartitionsRDD[81] at count at null:-2), which has no missing parents
17/05/20 21:30:48 INFO MemoryStore: Block broadcast_27 stored as values in memory (estimated size 9.3 KB, free 2.4 MB)
17/05/20 21:30:48 INFO MemoryStore: Block broadcast_27_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2.4 MB)
17/05/20 21:30:48 INFO BlockManagerInfo: Added broadcast_27_piece0 in memory on localhost:55538 (size: 4.6 KB, free: 509.1 MB)
17/05/20 21:30:48 INFO SparkContext: Created broadcast 27 from broadcast at DAGScheduler.scala:1006
17/05/20 21:30:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 24 (MapPartitionsRDD[81] at count at null:-2)
17/05/20 21:30:48 INFO TaskSchedulerImpl: Adding task set 24.0 with 1 tasks
17/05/20 21:30:48 INFO TaskSetManager: Starting task 0.0 in stage 24.0 (TID 33, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/05/20 21:30:48 INFO Executor: Running task 0.0 in stage 24.0 (TID 33)
17/05/20 21:30:48 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/05/20 21:30:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/20 21:30:48 INFO Executor: Finished task 0.0 in stage 24.0 (TID 33). 1830 bytes result sent to driver
17/05/20 21:30:48 INFO TaskSetManager: Finished task 0.0 in stage 24.0 (TID 33) in 16 ms on localhost (1/1)
17/05/20 21:30:48 INFO TaskSchedulerImpl: Removed TaskSet 24.0, whose tasks have all completed, from pool 
17/05/20 21:30:48 INFO DAGScheduler: ResultStage 24 (count at null:-2) finished in 0,016 s
17/05/20 21:30:48 INFO DAGScheduler: Job 15 finished: count at null:-2, took 0,025264 s
17/05/20 21:30:48 INFO ParseDriver: Parsing command: SELECT *
FROM `track_metadata`
17/05/20 21:30:48 INFO ParseDriver: Parse Completed
17/05/20 21:30:48 INFO SparkContext: Starting job: count at null:-2
17/05/20 21:30:48 INFO DAGScheduler: Registering RDD 84 (count at null:-2)
17/05/20 21:30:48 INFO DAGScheduler: Got job 16 (count at null:-2) with 1 output partitions
17/05/20 21:30:48 INFO DAGScheduler: Final stage: ResultStage 26 (count at null:-2)
17/05/20 21:30:48 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 25)
17/05/20 21:30:48 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 25)
17/05/20 21:30:48 INFO DAGScheduler: Submitting ShuffleMapStage 25 (MapPartitionsRDD[84] at count at null:-2), which has no missing parents
17/05/20 21:30:48 INFO MemoryStore: Block broadcast_28 stored as values in memory (estimated size 23.9 KB, free 2.4 MB)
17/05/20 21:30:48 INFO MemoryStore: Block broadcast_28_piece0 stored as bytes in memory (estimated size 10.3 KB, free 2.4 MB)
17/05/20 21:30:48 INFO BlockManagerInfo: Added broadcast_28_piece0 in memory on localhost:55538 (size: 10.3 KB, free: 509.1 MB)
17/05/20 21:30:48 INFO SparkContext: Created broadcast 28 from broadcast at DAGScheduler.scala:1006
17/05/20 21:30:48 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 25 (MapPartitionsRDD[84] at count at null:-2)
17/05/20 21:30:48 INFO TaskSchedulerImpl: Adding task set 25.0 with 2 tasks
17/05/20 21:30:48 INFO TaskSetManager: Starting task 0.0 in stage 25.0 (TID 34, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:30:48 INFO TaskSetManager: Starting task 1.0 in stage 25.0 (TID 35, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:30:48 INFO Executor: Running task 0.0 in stage 25.0 (TID 34)
17/05/20 21:30:48 INFO Executor: Running task 1.0 in stage 25.0 (TID 35)
17/05/20 21:30:48 INFO BlockManager: Found block rdd_14_0 locally
17/05/20 21:30:48 INFO BlockManager: Found block rdd_14_1 locally
17/05/20 21:30:48 INFO Executor: Finished task 0.0 in stage 25.0 (TID 34). 2679 bytes result sent to driver
17/05/20 21:30:48 INFO TaskSetManager: Finished task 0.0 in stage 25.0 (TID 34) in 0 ms on localhost (1/2)
17/05/20 21:30:48 INFO Executor: Finished task 1.0 in stage 25.0 (TID 35). 2679 bytes result sent to driver
17/05/20 21:30:48 INFO TaskSetManager: Finished task 1.0 in stage 25.0 (TID 35) in 0 ms on localhost (2/2)
17/05/20 21:30:48 INFO TaskSchedulerImpl: Removed TaskSet 25.0, whose tasks have all completed, from pool 
17/05/20 21:30:48 INFO DAGScheduler: ShuffleMapStage 25 (count at null:-2) finished in 0,000 s
17/05/20 21:30:48 INFO DAGScheduler: looking for newly runnable stages
17/05/20 21:30:48 INFO DAGScheduler: running: Set()
17/05/20 21:30:48 INFO DAGScheduler: waiting: Set(ResultStage 26)
17/05/20 21:30:48 INFO DAGScheduler: failed: Set()
17/05/20 21:30:48 INFO DAGScheduler: Submitting ResultStage 26 (MapPartitionsRDD[87] at count at null:-2), which has no missing parents
17/05/20 21:30:48 INFO MemoryStore: Block broadcast_29 stored as values in memory (estimated size 9.3 KB, free 2.4 MB)
17/05/20 21:30:48 INFO MemoryStore: Block broadcast_29_piece0 stored as bytes in memory (estimated size 4.6 KB, free 2.4 MB)
17/05/20 21:30:48 INFO BlockManagerInfo: Added broadcast_29_piece0 in memory on localhost:55538 (size: 4.6 KB, free: 509.1 MB)
17/05/20 21:30:48 INFO SparkContext: Created broadcast 29 from broadcast at DAGScheduler.scala:1006
17/05/20 21:30:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 26 (MapPartitionsRDD[87] at count at null:-2)
17/05/20 21:30:48 INFO TaskSchedulerImpl: Adding task set 26.0 with 1 tasks
17/05/20 21:30:48 INFO TaskSetManager: Starting task 0.0 in stage 26.0 (TID 36, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/05/20 21:30:48 INFO Executor: Running task 0.0 in stage 26.0 (TID 36)
17/05/20 21:30:48 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/05/20 21:30:48 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/20 21:30:48 INFO Executor: Finished task 0.0 in stage 26.0 (TID 36). 1830 bytes result sent to driver
17/05/20 21:30:48 INFO TaskSetManager: Finished task 0.0 in stage 26.0 (TID 36) in 0 ms on localhost (1/1)
17/05/20 21:30:48 INFO TaskSchedulerImpl: Removed TaskSet 26.0, whose tasks have all completed, from pool 
17/05/20 21:30:48 INFO DAGScheduler: ResultStage 26 (count at null:-2) finished in 0,000 s
17/05/20 21:30:48 INFO DAGScheduler: Job 16 finished: count at null:-2, took 0,025623 s
17/05/20 21:30:48 INFO ParseDriver: Parsing command: SELECT *
FROM `track_metadata`
LIMIT 10
17/05/20 21:30:48 INFO ParseDriver: Parse Completed
17/05/20 21:30:48 INFO BlockManagerInfo: Removed broadcast_29_piece0 on localhost:55538 in memory (size: 4.6 KB, free: 509.1 MB)
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 111
17/05/20 21:30:48 INFO BlockManagerInfo: Removed broadcast_28_piece0 on localhost:55538 in memory (size: 10.3 KB, free: 509.1 MB)
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 110
17/05/20 21:30:48 INFO ContextCleaner: Cleaned shuffle 9
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 109
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 108
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 107
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 106
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 105
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 104
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 103
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 102
17/05/20 21:30:48 INFO BlockManagerInfo: Removed broadcast_27_piece0 on localhost:55538 in memory (size: 4.6 KB, free: 509.1 MB)
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 101
17/05/20 21:30:48 INFO BlockManagerInfo: Removed broadcast_26_piece0 on localhost:55538 in memory (size: 10.3 KB, free: 509.1 MB)
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 100
17/05/20 21:30:48 INFO ContextCleaner: Cleaned shuffle 8
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 99
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 98
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 97
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 96
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 95
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 94
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 93
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 92
17/05/20 21:30:48 INFO BlockManagerInfo: Removed broadcast_25_piece0 on localhost:55538 in memory (size: 8.5 KB, free: 509.1 MB)
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 91
17/05/20 21:30:48 INFO BlockManagerInfo: Removed broadcast_24_piece0 on localhost:55538 in memory (size: 4.6 KB, free: 509.1 MB)
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 90
17/05/20 21:30:48 INFO BlockManagerInfo: Removed broadcast_23_piece0 on localhost:55538 in memory (size: 10.3 KB, free: 509.1 MB)
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 89
17/05/20 21:30:48 INFO ContextCleaner: Cleaned shuffle 7
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 88
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 87
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 86
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 85
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 84
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 83
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 82
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 81
17/05/20 21:30:48 INFO BlockManagerInfo: Removed broadcast_22_piece0 on localhost:55538 in memory (size: 4.6 KB, free: 509.1 MB)
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 80
17/05/20 21:30:48 INFO BlockManagerInfo: Removed broadcast_21_piece0 on localhost:55538 in memory (size: 10.3 KB, free: 509.2 MB)
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 79
17/05/20 21:30:48 INFO ContextCleaner: Cleaned shuffle 6
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 78
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 77
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 76
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 75
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 74
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 73
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 72
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 71
17/05/20 21:30:48 INFO BlockManagerInfo: Removed broadcast_20_piece0 on localhost:55538 in memory (size: 4.6 KB, free: 509.2 MB)
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 70
17/05/20 21:30:48 INFO BlockManagerInfo: Removed broadcast_19_piece0 on localhost:55538 in memory (size: 10.3 KB, free: 509.2 MB)
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 69
17/05/20 21:30:48 INFO ContextCleaner: Cleaned shuffle 5
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 68
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 67
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 66
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 65
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 64
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 63
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 62
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 61
17/05/20 21:30:48 INFO BlockManagerInfo: Removed broadcast_18_piece0 on localhost:55538 in memory (size: 8.5 KB, free: 509.2 MB)
17/05/20 21:30:48 INFO ContextCleaner: Cleaned accumulator 60
17/05/20 21:30:48 INFO SparkContext: Starting job: collect at utils.scala:195
17/05/20 21:30:48 INFO DAGScheduler: Got job 17 (collect at utils.scala:195) with 1 output partitions
17/05/20 21:30:48 INFO DAGScheduler: Final stage: ResultStage 27 (collect at utils.scala:195)
17/05/20 21:30:48 INFO DAGScheduler: Parents of final stage: List()
17/05/20 21:30:48 INFO DAGScheduler: Missing parents: List()
17/05/20 21:30:48 INFO DAGScheduler: Submitting ResultStage 27 (MapPartitionsRDD[90] at collect at utils.scala:195), which has no missing parents
17/05/20 21:30:48 INFO MemoryStore: Block broadcast_30 stored as values in memory (estimated size 19.6 KB, free 2.2 MB)
17/05/20 21:30:48 INFO MemoryStore: Block broadcast_30_piece0 stored as bytes in memory (estimated size 8.5 KB, free 2.2 MB)
17/05/20 21:30:48 INFO BlockManagerInfo: Added broadcast_30_piece0 in memory on localhost:55538 (size: 8.5 KB, free: 509.2 MB)
17/05/20 21:30:48 INFO SparkContext: Created broadcast 30 from broadcast at DAGScheduler.scala:1006
17/05/20 21:30:48 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 27 (MapPartitionsRDD[90] at collect at utils.scala:195)
17/05/20 21:30:48 INFO TaskSchedulerImpl: Adding task set 27.0 with 1 tasks
17/05/20 21:30:48 INFO TaskSetManager: Starting task 0.0 in stage 27.0 (TID 37, localhost, partition 0,PROCESS_LOCAL, 2482 bytes)
17/05/20 21:30:48 INFO Executor: Running task 0.0 in stage 27.0 (TID 37)
17/05/20 21:30:48 INFO BlockManager: Found block rdd_14_0 locally
17/05/20 21:30:48 INFO Executor: Finished task 0.0 in stage 27.0 (TID 37). 21347 bytes result sent to driver
17/05/20 21:30:48 INFO TaskSetManager: Finished task 0.0 in stage 27.0 (TID 37) in 0 ms on localhost (1/1)
17/05/20 21:30:48 INFO DAGScheduler: ResultStage 27 (collect at utils.scala:195) finished in 0,016 s
17/05/20 21:30:48 INFO TaskSchedulerImpl: Removed TaskSet 27.0, whose tasks have all completed, from pool 
17/05/20 21:30:48 INFO DAGScheduler: Job 17 finished: collect at utils.scala:195, took 0,007698 s
17/05/20 21:31:18 INFO ParseDriver: Parsing command: SELECT *
FROM (SELECT `artist_name` AS `artist_name`, `title` AS `title`, `year` AS `year`
FROM `track_metadata`) `dncgftviau`
WHERE ((`year` >= 1960.0) AND (`year` < 1970.0))
17/05/20 21:31:18 INFO ParseDriver: Parse Completed
17/05/20 21:31:18 INFO SparkContext: Starting job: count at null:-2
17/05/20 21:31:18 INFO DAGScheduler: Registering RDD 95 (count at null:-2)
17/05/20 21:31:18 INFO DAGScheduler: Got job 18 (count at null:-2) with 1 output partitions
17/05/20 21:31:18 INFO DAGScheduler: Final stage: ResultStage 29 (count at null:-2)
17/05/20 21:31:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 28)
17/05/20 21:31:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 28)
17/05/20 21:31:18 INFO DAGScheduler: Submitting ShuffleMapStage 28 (MapPartitionsRDD[95] at count at null:-2), which has no missing parents
17/05/20 21:31:18 INFO MemoryStore: Block broadcast_31 stored as values in memory (estimated size 25.9 KB, free 2.2 MB)
17/05/20 21:31:18 INFO MemoryStore: Block broadcast_31_piece0 stored as bytes in memory (estimated size 11.1 KB, free 2.2 MB)
17/05/20 21:31:18 INFO BlockManagerInfo: Added broadcast_31_piece0 in memory on localhost:55538 (size: 11.1 KB, free: 509.2 MB)
17/05/20 21:31:18 INFO SparkContext: Created broadcast 31 from broadcast at DAGScheduler.scala:1006
17/05/20 21:31:18 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 28 (MapPartitionsRDD[95] at count at null:-2)
17/05/20 21:31:18 INFO TaskSchedulerImpl: Adding task set 28.0 with 2 tasks
17/05/20 21:31:18 INFO TaskSetManager: Starting task 0.0 in stage 28.0 (TID 38, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:31:18 INFO TaskSetManager: Starting task 1.0 in stage 28.0 (TID 39, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:31:18 INFO Executor: Running task 1.0 in stage 28.0 (TID 39)
17/05/20 21:31:18 INFO Executor: Running task 0.0 in stage 28.0 (TID 38)
17/05/20 21:31:18 INFO BlockManager: Found block rdd_14_0 locally
17/05/20 21:31:18 INFO BlockManager: Found block rdd_14_1 locally
17/05/20 21:31:18 INFO GenerateColumnAccessor: Code generated in 9.007574 ms
17/05/20 21:31:18 INFO GeneratePredicate: Code generated in 10.602918 ms
17/05/20 21:31:18 INFO Executor: Finished task 1.0 in stage 28.0 (TID 39). 2808 bytes result sent to driver
17/05/20 21:31:18 INFO TaskSetManager: Finished task 1.0 in stage 28.0 (TID 39) in 47 ms on localhost (1/2)
17/05/20 21:31:18 INFO Executor: Finished task 0.0 in stage 28.0 (TID 38). 2808 bytes result sent to driver
17/05/20 21:31:18 INFO TaskSetManager: Finished task 0.0 in stage 28.0 (TID 38) in 47 ms on localhost (2/2)
17/05/20 21:31:18 INFO DAGScheduler: ShuffleMapStage 28 (count at null:-2) finished in 0,047 s
17/05/20 21:31:18 INFO DAGScheduler: looking for newly runnable stages
17/05/20 21:31:18 INFO DAGScheduler: running: Set()
17/05/20 21:31:18 INFO DAGScheduler: waiting: Set(ResultStage 29)
17/05/20 21:31:18 INFO DAGScheduler: failed: Set()
17/05/20 21:31:18 INFO DAGScheduler: Submitting ResultStage 29 (MapPartitionsRDD[98] at count at null:-2), which has no missing parents
17/05/20 21:31:18 INFO TaskSchedulerImpl: Removed TaskSet 28.0, whose tasks have all completed, from pool 
17/05/20 21:31:18 INFO MemoryStore: Block broadcast_32 stored as values in memory (estimated size 10.8 KB, free 2.2 MB)
17/05/20 21:31:18 INFO MemoryStore: Block broadcast_32_piece0 stored as bytes in memory (estimated size 5.3 KB, free 2.2 MB)
17/05/20 21:31:18 INFO BlockManagerInfo: Added broadcast_32_piece0 in memory on localhost:55538 (size: 5.3 KB, free: 509.1 MB)
17/05/20 21:31:18 INFO SparkContext: Created broadcast 32 from broadcast at DAGScheduler.scala:1006
17/05/20 21:31:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 29 (MapPartitionsRDD[98] at count at null:-2)
17/05/20 21:31:18 INFO TaskSchedulerImpl: Adding task set 29.0 with 1 tasks
17/05/20 21:31:18 INFO TaskSetManager: Starting task 0.0 in stage 29.0 (TID 40, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/05/20 21:31:18 INFO Executor: Running task 0.0 in stage 29.0 (TID 40)
17/05/20 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/05/20 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/20 21:31:18 INFO Executor: Finished task 0.0 in stage 29.0 (TID 40). 1959 bytes result sent to driver
17/05/20 21:31:18 INFO TaskSetManager: Finished task 0.0 in stage 29.0 (TID 40) in 16 ms on localhost (1/1)
17/05/20 21:31:18 INFO TaskSchedulerImpl: Removed TaskSet 29.0, whose tasks have all completed, from pool 
17/05/20 21:31:18 INFO DAGScheduler: ResultStage 29 (count at null:-2) finished in 0,016 s
17/05/20 21:31:18 INFO DAGScheduler: Job 18 finished: count at null:-2, took 0,068795 s
17/05/20 21:31:18 INFO ParseDriver: Parsing command: SELECT *
FROM (SELECT `artist_name` AS `artist_name`, `title` AS `title`, `year` AS `year`
FROM `track_metadata`) `ttxrqprvlu`
WHERE ((`year` >= 1960.0) AND (`year` < 1970.0))
17/05/20 21:31:18 INFO ParseDriver: Parse Completed
17/05/20 21:31:18 INFO SparkContext: Starting job: count at null:-2
17/05/20 21:31:18 INFO DAGScheduler: Registering RDD 103 (count at null:-2)
17/05/20 21:31:18 INFO DAGScheduler: Got job 19 (count at null:-2) with 1 output partitions
17/05/20 21:31:18 INFO DAGScheduler: Final stage: ResultStage 31 (count at null:-2)
17/05/20 21:31:18 INFO DAGScheduler: Parents of final stage: List(ShuffleMapStage 30)
17/05/20 21:31:18 INFO DAGScheduler: Missing parents: List(ShuffleMapStage 30)
17/05/20 21:31:18 INFO DAGScheduler: Submitting ShuffleMapStage 30 (MapPartitionsRDD[103] at count at null:-2), which has no missing parents
17/05/20 21:31:18 INFO MemoryStore: Block broadcast_33 stored as values in memory (estimated size 25.9 KB, free 2.3 MB)
17/05/20 21:31:18 INFO MemoryStore: Block broadcast_33_piece0 stored as bytes in memory (estimated size 11.1 KB, free 2.3 MB)
17/05/20 21:31:18 INFO BlockManagerInfo: Added broadcast_33_piece0 in memory on localhost:55538 (size: 11.1 KB, free: 509.1 MB)
17/05/20 21:31:18 INFO SparkContext: Created broadcast 33 from broadcast at DAGScheduler.scala:1006
17/05/20 21:31:18 INFO DAGScheduler: Submitting 2 missing tasks from ShuffleMapStage 30 (MapPartitionsRDD[103] at count at null:-2)
17/05/20 21:31:18 INFO TaskSchedulerImpl: Adding task set 30.0 with 2 tasks
17/05/20 21:31:18 INFO TaskSetManager: Starting task 0.0 in stage 30.0 (TID 41, localhost, partition 0,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:31:18 INFO TaskSetManager: Starting task 1.0 in stage 30.0 (TID 42, localhost, partition 1,PROCESS_LOCAL, 2471 bytes)
17/05/20 21:31:18 INFO Executor: Running task 1.0 in stage 30.0 (TID 42)
17/05/20 21:31:18 INFO Executor: Running task 0.0 in stage 30.0 (TID 41)
17/05/20 21:31:18 INFO BlockManager: Found block rdd_14_1 locally
17/05/20 21:31:18 INFO BlockManager: Found block rdd_14_0 locally
17/05/20 21:31:18 INFO Executor: Finished task 1.0 in stage 30.0 (TID 42). 2808 bytes result sent to driver
17/05/20 21:31:18 INFO Executor: Finished task 0.0 in stage 30.0 (TID 41). 2808 bytes result sent to driver
17/05/20 21:31:18 INFO TaskSetManager: Finished task 1.0 in stage 30.0 (TID 42) in 0 ms on localhost (1/2)
17/05/20 21:31:18 INFO TaskSetManager: Finished task 0.0 in stage 30.0 (TID 41) in 0 ms on localhost (2/2)
17/05/20 21:31:18 INFO TaskSchedulerImpl: Removed TaskSet 30.0, whose tasks have all completed, from pool 
17/05/20 21:31:18 INFO DAGScheduler: ShuffleMapStage 30 (count at null:-2) finished in 0,000 s
17/05/20 21:31:18 INFO DAGScheduler: looking for newly runnable stages
17/05/20 21:31:18 INFO DAGScheduler: running: Set()
17/05/20 21:31:18 INFO DAGScheduler: waiting: Set(ResultStage 31)
17/05/20 21:31:18 INFO DAGScheduler: failed: Set()
17/05/20 21:31:18 INFO DAGScheduler: Submitting ResultStage 31 (MapPartitionsRDD[106] at count at null:-2), which has no missing parents
17/05/20 21:31:18 INFO MemoryStore: Block broadcast_34 stored as values in memory (estimated size 10.8 KB, free 2.3 MB)
17/05/20 21:31:18 INFO MemoryStore: Block broadcast_34_piece0 stored as bytes in memory (estimated size 5.3 KB, free 2.3 MB)
17/05/20 21:31:18 INFO BlockManagerInfo: Added broadcast_34_piece0 in memory on localhost:55538 (size: 5.3 KB, free: 509.1 MB)
17/05/20 21:31:18 INFO SparkContext: Created broadcast 34 from broadcast at DAGScheduler.scala:1006
17/05/20 21:31:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 31 (MapPartitionsRDD[106] at count at null:-2)
17/05/20 21:31:18 INFO TaskSchedulerImpl: Adding task set 31.0 with 1 tasks
17/05/20 21:31:18 INFO TaskSetManager: Starting task 0.0 in stage 31.0 (TID 43, localhost, partition 0,NODE_LOCAL, 2242 bytes)
17/05/20 21:31:18 INFO Executor: Running task 0.0 in stage 31.0 (TID 43)
17/05/20 21:31:18 INFO ShuffleBlockFetcherIterator: Getting 2 non-empty blocks out of 2 blocks
17/05/20 21:31:18 INFO ShuffleBlockFetcherIterator: Started 0 remote fetches in 0 ms
17/05/20 21:31:18 INFO Executor: Finished task 0.0 in stage 31.0 (TID 43). 1959 bytes result sent to driver
17/05/20 21:31:18 INFO TaskSetManager: Finished task 0.0 in stage 31.0 (TID 43) in 0 ms on localhost (1/1)
17/05/20 21:31:18 INFO TaskSchedulerImpl: Removed TaskSet 31.0, whose tasks have all completed, from pool 
17/05/20 21:31:18 INFO DAGScheduler: ResultStage 31 (count at null:-2) finished in 0,016 s
17/05/20 21:31:18 INFO DAGScheduler: Job 19 finished: count at null:-2, took 0,023441 s
17/05/20 21:31:18 INFO ParseDriver: Parsing command: SELECT *
FROM (SELECT *
FROM (SELECT `artist_name` AS `artist_name`, `title` AS `title`, `year` AS `year`
FROM `track_metadata`) `qnkgdvfzwd`
WHERE ((`year` >= 1960.0) AND (`year` < 1970.0))) `bquymivwxx`
LIMIT 10
17/05/20 21:31:18 INFO ParseDriver: Parse Completed
17/05/20 21:31:18 INFO SparkContext: Starting job: collect at utils.scala:195
17/05/20 21:31:18 INFO DAGScheduler: Got job 20 (collect at utils.scala:195) with 1 output partitions
17/05/20 21:31:18 INFO DAGScheduler: Final stage: ResultStage 32 (collect at utils.scala:195)
17/05/20 21:31:18 INFO DAGScheduler: Parents of final stage: List()
17/05/20 21:31:18 INFO DAGScheduler: Missing parents: List()
17/05/20 21:31:18 INFO DAGScheduler: Submitting ResultStage 32 (MapPartitionsRDD[111] at collect at utils.scala:195), which has no missing parents
17/05/20 21:31:18 INFO MemoryStore: Block broadcast_35 stored as values in memory (estimated size 22.7 KB, free 2.3 MB)
17/05/20 21:31:18 INFO MemoryStore: Block broadcast_35_piece0 stored as bytes in memory (estimated size 9.8 KB, free 2.3 MB)
17/05/20 21:31:18 INFO BlockManagerInfo: Added broadcast_35_piece0 in memory on localhost:55538 (size: 9.8 KB, free: 509.1 MB)
17/05/20 21:31:18 INFO SparkContext: Created broadcast 35 from broadcast at DAGScheduler.scala:1006
17/05/20 21:31:18 INFO DAGScheduler: Submitting 1 missing tasks from ResultStage 32 (MapPartitionsRDD[111] at collect at utils.scala:195)
17/05/20 21:31:18 INFO TaskSchedulerImpl: Adding task set 32.0 with 1 tasks
17/05/20 21:31:18 INFO TaskSetManager: Starting task 0.0 in stage 32.0 (TID 44, localhost, partition 0,PROCESS_LOCAL, 2482 bytes)
17/05/20 21:31:18 INFO Executor: Running task 0.0 in stage 32.0 (TID 44)
17/05/20 21:31:18 INFO BlockManager: Found block rdd_14_0 locally
17/05/20 21:31:18 INFO GenerateColumnAccessor: Code generated in 8.454581 ms
17/05/20 21:31:18 INFO GeneratePredicate: Code generated in 3.741216 ms
17/05/20 21:31:18 INFO GenerateUnsafeProjection: Code generated in 3.421408 ms
17/05/20 21:31:18 INFO GenerateSafeProjection: Code generated in 4.137794 ms
17/05/20 21:31:18 INFO Executor: Finished task 0.0 in stage 32.0 (TID 44). 3652 bytes result sent to driver
17/05/20 21:31:18 INFO TaskSetManager: Finished task 0.0 in stage 32.0 (TID 44) in 31 ms on localhost (1/1)
17/05/20 21:31:18 INFO DAGScheduler: ResultStage 32 (collect at utils.scala:195) finished in 0,031 s
17/05/20 21:31:18 INFO TaskSchedulerImpl: Removed TaskSet 32.0, whose tasks have all completed, from pool 
17/05/20 21:31:18 INFO DAGScheduler: Job 20 finished: collect at utils.scala:195, took 0,035825 s
17/05/20 21:31:42 INFO SparkContext: Invoking stop() from shutdown hook
17/05/20 21:31:42 INFO SparkUI: Stopped Spark web UI at http://127.0.0.1:4040
17/05/20 21:31:42 INFO MapOutputTrackerMasterEndpoint: MapOutputTrackerMasterEndpoint stopped!
17/05/20 21:31:43 INFO MemoryStore: MemoryStore cleared
17/05/20 21:31:43 INFO BlockManager: BlockManager stopped
17/05/20 21:31:43 INFO BlockManagerMaster: BlockManagerMaster stopped
17/05/20 21:31:43 INFO OutputCommitCoordinator$OutputCommitCoordinatorEndpoint: OutputCommitCoordinator stopped!
17/05/20 21:31:43 WARN SparkEnv: Exception while deleting Spark temp dir: C:\Users\d91067\AppData\Local\Temp\spark-74c2ffc0-55bb-4147-983f-2be0c6204a95\userFiles-5410eee9-e446-4237-96d1-a0ba49744a10
java.io.IOException: Failed to delete: C:\Users\d91067\AppData\Local\Temp\spark-74c2ffc0-55bb-4147-983f-2be0c6204a95\userFiles-5410eee9-e446-4237-96d1-a0ba49744a10
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.SparkEnv.stop(SparkEnv.scala:119)
	at org.apache.spark.SparkContext$$anonfun$stop$12.apply$mcV$sp(SparkContext.scala:1756)
	at org.apache.spark.util.Utils$.tryLogNonFatalError(Utils.scala:1219)
	at org.apache.spark.SparkContext.stop(SparkContext.scala:1755)
	at org.apache.spark.SparkContext$$anonfun$3.apply$mcV$sp(SparkContext.scala:596)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/05/20 21:31:43 INFO RemoteActorRefProvider$RemotingTerminator: Shutting down remote daemon.
17/05/20 21:31:43 INFO SparkContext: Successfully stopped SparkContext
17/05/20 21:31:43 INFO ShutdownHookManager: Shutdown hook called
17/05/20 21:31:43 INFO ShutdownHookManager: Deleting directory C:\Users\d91067\AppData\Local\Temp\spark-74c2ffc0-55bb-4147-983f-2be0c6204a95\httpd-870c9a3a-872e-4821-9f73-7fb7e7c5ba20
17/05/20 21:31:43 INFO RemoteActorRefProvider$RemotingTerminator: Remote daemon shut down; proceeding with flushing remote transports.
17/05/20 21:31:43 INFO ShutdownHookManager: Deleting directory C:\Users\d91067\AppData\Local\Temp\spark-74c2ffc0-55bb-4147-983f-2be0c6204a95
17/05/20 21:31:43 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\d91067\AppData\Local\Temp\spark-74c2ffc0-55bb-4147-983f-2be0c6204a95
java.io.IOException: Failed to delete: C:\Users\d91067\AppData\Local\Temp\spark-74c2ffc0-55bb-4147-983f-2be0c6204a95
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/05/20 21:31:43 INFO ShutdownHookManager: Deleting directory C:\Users\d91067\AppData\Local\Temp\spark-74c2ffc0-55bb-4147-983f-2be0c6204a95\userFiles-5410eee9-e446-4237-96d1-a0ba49744a10
17/05/20 21:31:43 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\d91067\AppData\Local\Temp\spark-74c2ffc0-55bb-4147-983f-2be0c6204a95\userFiles-5410eee9-e446-4237-96d1-a0ba49744a10
java.io.IOException: Failed to delete: C:\Users\d91067\AppData\Local\Temp\spark-74c2ffc0-55bb-4147-983f-2be0c6204a95\userFiles-5410eee9-e446-4237-96d1-a0ba49744a10
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
17/05/20 21:31:43 INFO ShutdownHookManager: Deleting directory C:\Users\d91067\AppData\Local\Temp\spark-1978c913-ae88-47e7-b3bd-e5704d6aaaee
17/05/20 21:31:43 INFO RemoteActorRefProvider$RemotingTerminator: Remoting shut down.
17/05/20 21:31:43 ERROR ShutdownHookManager: Exception while deleting Spark temp dir: C:\Users\d91067\AppData\Local\Temp\spark-1978c913-ae88-47e7-b3bd-e5704d6aaaee
java.io.IOException: Failed to delete: C:\Users\d91067\AppData\Local\Temp\spark-1978c913-ae88-47e7-b3bd-e5704d6aaaee
	at org.apache.spark.util.Utils$.deleteRecursively(Utils.scala:929)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:65)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1$$anonfun$apply$mcV$sp$3.apply(ShutdownHookManager.scala:62)
	at scala.collection.IndexedSeqOptimized$class.foreach(IndexedSeqOptimized.scala:33)
	at scala.collection.mutable.ArrayOps$ofRef.foreach(ArrayOps.scala:108)
	at org.apache.spark.util.ShutdownHookManager$$anonfun$1.apply$mcV$sp(ShutdownHookManager.scala:62)
	at org.apache.spark.util.SparkShutdownHook.run(ShutdownHookManager.scala:267)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1$$anonfun$apply$mcV$sp$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.Utils$.logUncaughtExceptions(Utils.scala:1801)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply$mcV$sp(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anonfun$runAll$1.apply(ShutdownHookManager.scala:239)
	at scala.util.Try$.apply(Try.scala:161)
	at org.apache.spark.util.SparkShutdownHookManager.runAll(ShutdownHookManager.scala:239)
	at org.apache.spark.util.SparkShutdownHookManager$$anon$2.run(ShutdownHookManager.scala:218)
	at org.apache.hadoop.util.ShutdownHookManager$1.run(ShutdownHookManager.java:54)
