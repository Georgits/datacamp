{
    "collab_server" : "",
    "contents" : "# 1. Chapter 1: Light My Fire: Starting To Use Spark With dplyr Syntax -----\n  # The connect-work-disconnect pattern ----\n    # Load sparklyr\n    library(sparklyr)\n    library(DBI)\n    library(tidyr)\n    library(tidytext)\n    library(microbenchmark)\n    library(dplyr)\n    library(data.table)\n\n    # Connect to your Spark cluster\n    spark_conn <- spark_connect(master = \"local\")\n    \n    # Print the version of Spark\n    spark_version(sc = spark_conn)\n    \n    # Disconnect from Spark\n    spark_disconnect(sc = spark_conn)\n    \n    \n    \n    # Copying data into Spark ----\n    # Explore track_metadata structure\n    track_metadata <- fread(\"track_metadata.csv\")\n    str(track_metadata)\n    \n    # Connect to your Spark cluster\n    spark_conn <- spark_connect(master = \"local\")\n    \n    # Copy track_metadata to Spark\n    track_metadata_tbl <- copy_to(dest = spark_conn, track_metadata)\n    \n    # List the data frames available in Spark\n    src_tbls(spark_conn)\n    \n    # # Disconnect from Spark\n    # spark_disconnect(sc = spark_conn)\n    \n    \n    \n    # Link to the track_metadata table in Spark\n    track_metadata_tbl <- tbl(spark_conn, \"track_metadata\")\n    \n    # See how big the dataset is\n    dim(track_metadata_tbl)\n    \n    # See how small the tibble is\n    object_size(track_metadata_tbl)\n    \n    \n    \n    # Exploring the structure of tibbles ----\n    # Print 5 rows, all columns\n    print(track_metadata_tbl, n = 5, width = Inf)\n    \n    # Examine structure of tibble\n    str(track_metadata_tbl)\n    \n    # Examine structure of data\n    glimpse(track_metadata_tbl)\n    \n    \n    \n    # !!! Selecting columns ----\n    # track_metadata_tbl has been pre-defined\n    track_metadata_tbl\n    \n    # Manipulate the track metadata\n    track_metadata_tbl %>%\n      # Select columns\n      select(artist_name, release, title, year)\n    \n    # Try to select columns using [ ]\n    tryCatch({\n      # Selection code here\n      track_metadata_tbl[, c(\"artist_name\", \"release\", \"title\", \"year\")]\n    },\n    error = print\n    )\n    # tryCatch(error = print) is a nice way to see errors without them stopping the execution of your code. \n    \n    \n    \n    \n    # Filtering rows ----\n    # Manipulate the track metadata\n    track_metadata_tbl %>%\n      # Select columns\n      select(artist_name, release, title, year) %>%\n      # Filter rows\n      filter(year >= 1960, year < 1970)\n    \n    \n    \n    # Arranging rows -----\n    # Manipulate the track metadata\n    track_metadata_tbl %>%\n      # Select columns\n      select(artist_name, release, title, year) %>%\n      # Filter rows\n      filter(year >= 1960, year < 1970) %>%\n      # Arrange rows\n      arrange(artist_name, desc(year), title)\n    \n    \n    # Mutating columns ----\n    # Manipulate the track metadata\n    track_metadata_tbl %>%\n      # Select columns\n      select(title, duration) %>%\n      # Mutate columns\n      mutate(duration_minutes = duration / 60)\n    \n    \n    # Summarizing columns ----\n    # Manipulate the track metadata\n    track_metadata_tbl %>%\n      # Select columns\n      select(title, duration) %>%\n      # Mutate columns\n      mutate(duration_minutes = duration / 60) %>%\n      # Summarize columns\n      summarize(mean_duration_minutes = mean(duration_minutes))\n    \n\n    \n        \n# 2. Chapter 2: Tools of the Trade: Advanced dplyr Usage  -----\n    # Mother's little helper (1) ----\n    track_metadata_tbl %>%\n      # Select columns starting with artist\n      select(starts_with(\"artist\"))\n    \n    track_metadata_tbl %>%\n      # Select columns ending with id\n      select(ends_with(\"id\"))\n    \n    \n    # !!!! Mother's little helper (2) ----\n    track_metadata_tbl %>%\n      # Select columns containing ti\n      select(contains(\"ti\"))\n    \n    track_metadata_tbl %>%\n      # Select columns matching ti.?t\n      select(matches(\"ti.?t\"))\n    # You can find columns that match a particular regex using the matches() select helper.\n    \n    \n    # Selecting unique rows ----\n    track_metadata_tbl %>%\n      # Only return rows with distinct artist_name\n      distinct(artist_name)\n    \n    \n    # !!! Common people ----\n    track_metadata_tbl %>%\n      # Count the artist_name values\n      count(artist_name, sort = TRUE) %>%\n      # Restrict to top 20\n      top_n(20)\n    # count(sort = TRUE) + top_n() is a pattern worth remembering\n    \n    \n    # !!!! Collecting data back from Spark ----\n    # copy_to() moves your data from R to Spark; collect() goes in the opposite direction. \n    # track_metadata_tbl has been pre-defined\n    track_metadata_tbl\n    \n    results <- track_metadata_tbl %>%\n      # Filter where artist familiarity is greater than 0.9\n      filter(artist_familiarity > 0.9)\n    \n    # Examine the class of the results\n    class(results)\n    \n    # Collect your results\n    collected <- results %>%\n      collect()\n    \n    # Examine the class of the collected results\n    class(collected)\n    \n    \n    \n    # Storing intermediate results ----\n    computed <- track_metadata_tbl %>%\n      # Filter where artist familiarity is greater than 0.8\n      filter(artist_familiarity > 0.8) %>%\n      # Compute the results\n      compute(\"familiar_artists\")\n    \n    # See the available datasets\n    src_tbls(spark_conn)\n    \n    # Examine the structure of the computed results\n    class(computed)\n    \n    \n    # Groups: great for music, great for data ----\n    duration_by_artist <- track_metadata_tbl %>%\n      # Group by artist\n      group_by(artist_name) %>%\n      # Calc mean duration\n      summarize(mean_duration = mean(duration))\n    \n    duration_by_artist %>%\n      # Sort by ascending mean duration\n      arrange(mean_duration)\n    \n    duration_by_artist %>%\n      # Sort by descending mean duration\n      arrange(desc(mean_duration))\n    \n    \n    \n    # Groups of mutants ----\n    track_metadata_tbl %>%\n      # Group by artist\n      group_by(artist_name) %>%\n      # Calc time since first release\n      mutate(time_since_first_release = year - min(year)) %>%\n      # Arrange by descending time since first release\n      arrange(time_since_first_release)\n    \n    \n    # Advanced Selection II: The SQL ----\n    # Write SQL query\n    query <- \"SELECT * FROM track_metadata WHERE year < 1935 AND duration > 300\"\n    \n    # Run the query\n    (results <- dbGetQuery(spark_conn, query))\n    \n    \n    \n    \n    # Left joins ----\n    # track_metadata_tbl and artist_terms_tbl have been pre-defined\n    track_metadata_tbl\n    artist_terms_tbl # fehlt\n    \n    # Left join artist terms to track metadata by artist_id\n    joined <- left_join(track_metadata_tbl, artist_terms_tbl, by = c(\"artist_id\"))\n    \n    # How many rows and columns are in the joined table?\n    dim(joined)\n    \n    \n    \n    # Anti joins ----\n    # track_metadata_tbl and artist_terms_tbl have been pre-defined\n    track_metadata_tbl\n    artist_terms_tbl\n    \n    # Anti join artist terms to track metadata by artist_id\n    joined <- anti_join(track_metadata_tbl, artist_terms_tbl, by = c(\"artist_id\"))\n    \n    # How many rows and columns are in the joined table?\n    dim(joined)\n    \n    \n    \n    # Semi joins ----\n    # track_metadata_tbl and artist_terms_tbl have been pre-defined\n    track_metadata_tbl\n    artist_terms_tbl\n    \n    # Semi join artist terms to track metadata by artist_id\n    joined <- semi_join(track_metadata_tbl, artist_terms_tbl, by = c(\"artist_id\"))\n    \n    # How many rows and columns are in the joined table?\n    dim(joined)\n    \n    \n    \n    \n    \n# 3. Chapter 3: Going Native: Use The Native Interface to Manipulate Spark DataFrames -----\n    # Transforming continuous variables to logical ----\n    # track_metadata_tbl has been pre-defined\n    # ft_binarizer() converts from continuous to logical;\n    track_metadata_tbl\n    \n    hotttnesss <- track_metadata_tbl %>%\n      # Select artist_hotttnesss\n      select(artist_hotttnesss) %>%\n      # Binarize to is_hottt_or_nottt\n      ft_binarizer(\"artist_hotttnesss\", \"is_hottt_or_nottt\", threshold = 0.5) %>%\n      # Collect the result\n      collect() %>%\n      # Convert is_hottt_or_nottt to logical\n      mutate(is_hottt_or_nottt = as.logical(is_hottt_or_nottt))\n    \n    # Draw a barplot of is_hottt_or_nottt\n    ggplot(hotttnesss, aes(is_hottt_or_nottt)) +\n      geom_bar()\n    \n    \n    \n    \n    # Transforming continuous variables into categorical (1) ----\n    # track_metadata_tbl, decades, decade_labels have been pre-defined\n    track_metadata_tbl\n    decades\n    decade_labels\n    \n    hotttnesss_over_time <- track_metadata_tbl %>%\n      # Select artist_hotttnesss and year\n      select(artist_hotttnesss, year) %>%\n      # Convert year to numeric\n      mutate(year = as.numeric(year)) %>%\n      # Bucketize year to decade\n      ft_bucketizer(\"year\", \"decade\", splits = decades) %>%\n      # Collect the result\n      collect() %>%\n      # Convert decade to factor\n      mutate(decade = factor(decade, labels = decade_labels))\n    \n    # Draw a boxplot of artist_hotttnesss by decade\n    ggplot(hotttnesss_over_time, aes(decade, artist_hotttnesss)) +\n      geom_boxplot()\n\n    \n    \n    \n    \n    #Transforming continuous variables into categorical (2) ----\n    # track_metadata_tbl, duration_labels have been pre-defined\n    track_metadata_tbl\n    duration_labels\n    \n    familiarity_by_duration <- track_metadata_tbl %>%\n      # Select duration and artist_familiarity\n      select(duration, artist_familiarity) %>%\n      # Bucketize duration\n      ft_quantile_discretizer(\"duration\", \"duration_bin\", n.buckets = 5) %>%\n      # Collect the result\n      collect() %>%\n      # Convert duration bin to factor\n      mutate(duration_bin = factor(duration_bin, labels = duration_labels))\n    \n    # Draw a boxplot of artist_familiarity by duration_bin\n    ggplot(familiarity_by_duration, aes(duration_bin, artist_familiarity)) +\n      geom_boxplot()\n    \n    \n    \n    # More than words: tokenization (1) ----\n    # The list-of-list-of-strings format can be transformed to a single character vector using unnest() from the tidyr package. \n    # There is currently no method for unnesting data on Spark, so for now, you have to collect it to R before transforming it. \n    # The code pattern to achieve this is as follows.\n    # track_metadata_tbl has been pre-defined\n    track_metadata_tbl\n    \n    title_text <- track_metadata_tbl %>%\n      # Select artist_name, title\n      select(artist_name, title) %>%\n      # Tokenize title to words\n      ft_tokenizer(\"title\", \"word\") %>%\n      # Collect the result\n      collect() %>%\n      # Flatten the word column \n      mutate(word = lapply(word, as.character)) %>% \n      # Unnest the list column\n      unnest(word)\n    \n    \n    \n    \n    # More than words: tokenization (2) ----\n    # Tibbles attached to the title words and sentiment lexicon stored in Spark have been \n    # pre-defined as title_text_tbl and afinn_sentiments_tbl respectively.\n    # title_text_tbl, afinn_sentiments_tbl have been pre-defined\n    title_text_tbl\n    afinn_sentiments_tbl\n    \n    sentimental_artists <- title_text_tbl %>%\n      # Inner join with sentiments on word field\n      inner_join(afinn_sentiments_tbl, by = \"word\") %>%\n      # Group by artist\n      group_by(artist_name) %>%\n      # Summarize to get positivity\n      summarize(positivity = sum(score))\n    \n    sentimental_artists %>%\n      # Arrange by ascending positivity\n      arrange(positivity) %>%\n      # Get top 5\n      top_n(5)\n    \n    sentimental_artists %>%\n      # Arrange by descending positivity\n      arrange(desc(positivity)) %>%\n      # Get top 5\n      top_n(5)\n    \n    \n    \n    \n    # More than words: tokenization (3) ----\n    # The dataset contains a field named artist_mbid that contains an ID for the artist on MusicBrainz, a music metadata encyclopedia website. \n    # The IDs take the form of hexadecimal numbers split by hyphens, for example, 65b785d9-499f-48e6-9063-3a1fd1bd488d\n    \n    # track_metadata_tbl has been pre-defined\n    track_metadata_tbl\n    \n    track_metadata_tbl %>%\n      # Select artist_mbid column\n      select(artist_mbid) %>%\n      # Split it by hyphens\n      ft_regex_tokenizer(\"artist_mbid\", \"artist_mbid_chunks\", pattern = \"-\")\n    \n    \n    \n    \n    # Sorting vs. arranging ----\n    # Sometimes native methods are faster than the dplyr equivalent; sometimes it is the other way around. \n    # Profile your code if you need to see where the slowness occurs.\n    # track_metadata_tbl has been pre-defined\n    track_metadata_tbl\n    \n    # Compare timings of arrange() and sdf_sort()\n    microbenchmark(\n      arranged = track_metadata_tbl %>%\n        # Arrange by year, then artist_name, then release, then title\n        arrange(year, artist_name, release, title) %>%\n        # Collect the result\n        collect(),\n      sorted = track_metadata_tbl %>%\n        # Sort by year, then artist_name, then release, then title\n        sdf_sort(c(\"year\", \"artist_name\", \"release\", \"title\")) %>%\n        # Collect the result\n        collect(),\n      times = 5\n    )\n    \n    \n    # Exploring Spark data types ----\n    # sparklyr has a function named sdf_schema() for exploring the columns of a tibble on the R side. It's easy to call; and a little painful to deal with the return value.\n    # track_metadata_tbl has been pre-defined\n    track_metadata_tbl\n    \n    # Get the schema\n    (schema <- sdf_schema(track_metadata_tbl))\n    \n    # Transform the schema\n    schema %>%\n      lapply(function(x) do.call(data_frame, x)) %>%\n      bind_rows()\n    # Alternative: glimpse\n    \n    \n    \n    \n    # Shrinking the data by sampling ----\n    # sdf_sample() can also be used for things like bootstrapping, which use sampling with replacement. \n    # Since the results of the sampling are random, and you will likely want to reuse the shrunken dataset, \n    # it is common to use compute() to store the results as another Spark data frame. \n    # track_metadata_tbl has been pre-defined\n    track_metadata_tbl\n    \n    track_metadata_tbl %>%\n      # Sample the data without replacement\n      sdf_sample(fraction = 0.01, replacement = FALSE, seed = 20000229) %>%\n      # Compute the result\n      compute(\"sample_track_metadata\")\n    \n    \n    \n    # Training/testing partitions ----\n    #  you can use any set names that you like, and partition the data into more than two sets. So the following is also valid.\n    # The return value is a list of tibbles. you can access each one using the usual list indexing operators.\n    # a_tibble %>%\n    # sdf_partition(a = 0.1, b = 0.2, c = 0.3, d = 0.4)\n    # partitioned$a ; partitioned[[\"b\"]]\n    # track_metadata_tbl has been pre-defined\n    track_metadata_tbl\n    \n    partitioned <- track_metadata_tbl %>%\n      # Partition into training and testing sets\n      sdf_partition(training = 0.7, testing = 0.3)\n    \n    # Glimpse the structure of the training set\n    glimpse(partitioned$training)\n    \n    # Glimpse the structure of the testing set\n    glimpse(partitioned$testing)\n    \n    \n    \n    \n    \n# 4.Chapter: Case Study: Learning to be a Machine: Running Machine Learning Models on Spark ----\n    # !!! Machine learning functions ----\n    # You can see the list of all the machine learning functions using ls()\n    # ls(\"package:sparklyr\", pattern = \"^ml\")\n    \n    \n    # (Hey you) What's that sound? ----\n    # timbre has been pre-defined\n    timbre\n    \n    # Calculate column means\n    (mean_timbre <- colMeans(timbre))\n    \n    \n    \n    # Working with parquet files ----\n    # Technically speaking, parquet file is a misnomer. When you store data in parquet format, you actually get a whole directory worth of files. \n    # The data is split across multiple .parquet files, allowing it to be easily stored on multiple machines, and there are some metadata files too, describing the contents of each column.\n    \n    # sparklyr can import parquet files using spark_read_parquet(). This function takes a Spark connection, \n    # a string naming the Spark DataFrame that should be created, and a path to the parquet directory. \n    # Note that this function will import the data directly into Spark, which is typically faster than importing the data into R, then using copy_to() to copy the data from R to Spark.\n    # spark_read_parquet(sc, \"a_dataset\", \"path/to/parquet/dir\")\n    \n    # List the files in the parquet dir\n    filenames <- dir(parquet_dir, full.names = TRUE)\n    \n    # Show the filenames and their sizes\n    data_frame(\n      filename = basename(filenames),\n      size_bytes = file.size(filenames)\n    )\n    \n    # Import the data into Spark\n    timbre_tbl <- spark_read_parquet(spark_conn, \"timbre\", parquet_dir)\n    \n    \n    # Come together ----\n    # track_metadata_tbl, timbre_tbl pre-defined\n    track_metadata_tbl\n    timbre_tbl\n    \n    track_metadata_tbl %>%\n      # Inner join to timbre_tbl\n      inner_join(timbre_tbl, by = \"track_id\") %>%\n      # Convert year to numeric\n      mutate(year = as.numeric(year))\n    \n    \n    \n    # Partitioning data with a group effect ----\n    # track_data_tbl has been pre-defined\n    track_data_tbl\n    \n    training_testing_artist_ids <- track_data_tbl %>%\n      # Select the artist ID\n      select(artist_id) %>%\n      # Get distinct rows\n      distinct() %>%\n      # Partition into training/testing sets\n      sdf_partition(training = 0.7, testing = 0.3)\n    \n    track_data_to_model_tbl <- track_data_tbl %>%\n      # Inner join to training partition\n      inner_join(training_testing_artist_ids$training, by = \"artist_id\")\n    \n    track_data_to_predict_tbl <- track_data_tbl %>%\n      # Inner join to testing partition\n      inner_join(training_testing_artist_ids$testing, by = \"artist_id\")\n    \n    \n    \n    \n    # Gradient boosted trees: modeling ----\n    # track_data_to_model_tbl has been pre-defined\n    track_data_to_model_tbl\n    \n    feature_colnames <- track_data_to_model_tbl %>%\n      # Get the column names\n      colnames() %>%\n      # Limit to the timbre columns\n      str_subset(fixed(\"timbre\"))\n    \n    gradient_boosted_trees_model <- track_data_to_model_tbl %>%\n      # Run the gradient boosted trees model\n      ml_gradient_boosted_trees(\"year\", feature_colnames)\n    \n    \n    \n    \n    # Gradient boosted trees: prediction ----\n    # Note that currently adding a prediction column has to be done locally, so you must collect the results first.\n    \n    # predicted_vs_actual <- testing_data %>%\n    #   select(response) %>%\n    #   collect() %>%\n    #   mutate(predicted_response = predict(a_model, testing_data))\n    \n    # training, testing sets & model are pre-defined\n    track_data_to_model_tbl\n    track_data_to_predict_tbl\n    gradient_boosted_trees_model\n    \n    responses <- track_data_to_predict_tbl %>%\n      # Select the response column\n      select(year) %>%\n      # Collect the results\n      collect() %>%\n      # Add in the predictions\n      mutate(\n        predicted_year = predict(\n          gradient_boosted_trees_model,\n          track_data_to_predict_tbl\n        )\n      )\n    \n    \n    \n    # Gradient boosted trees: visualization ----\n    # One slightly tricky thing here is that sparklyr doesn't yet support the residuals() function in all its machine learning models. Consequently, \n    # you have to calculate the residuals yourself (predicted responses minus actual responses).\n    # responses has been pre-defined\n    responses\n    \n    # Draw a scatterplot of predicted vs. actual\n    ggplot(responses, aes(actual, predicted)) +\n      # Add the points\n      geom_point(alpha = 0.1) +\n      # Add a line at actual = predicted\n      geom_abline(intercept = 0, slope = 1)\n    \n    residuals <- responses %>%\n      # Transmute response data to residuals\n      transmute(residual = predicted - actual)\n    \n    # Draw a density plot of residuals\n    ggplot(residuals, aes(residual)) +\n      # Add a density curve\n      geom_density() +\n      # Add a vertical line through zero\n      geom_vline(xintercept = 0)\n    \n    \n    \n    \n    # Random Forest: modeling -----\n    # track_data_to_model_tbl has been pre-defined\n    track_data_to_model_tbl\n    \n    # Get the timbre columns\n    feature_colnames <- track_data_to_model_tbl %>%\n      # Get the column names\n      colnames() %>%\n      # Limit to the timbre columns\n      str_subset(fixed(\"timbre\"))\n    \n    # Run the random forest model\n    random_forest_model <- track_data_to_model_tbl %>%\n      # Run the gradient boosted trees model\n      ml_random_forest(\"year\", feature_colnames)\n    \n    \n    \n    \n    # Random Forest: prediction ----\n    # training, testing sets & model are pre-defined\n    track_data_to_model_tbl\n    track_data_to_predict_tbl\n    random_forest_model\n    \n    # Create a response vs. actual dataset\n    responses <- track_data_to_predict_tbl %>%\n      # Select the response column\n      select(year) %>%\n      # Collect the results\n      collect() %>%\n      # Add in the predictions\n      mutate(\n        predicted_year = predict(\n          random_forest_model,\n          track_data_to_predict_tbl\n        )\n      )\n    \n    \n    # Random Forest: visualization ----\n    # both_responses has been pre-defined\n    both_responses\n    \n    # Draw a scatterplot of predicted vs. actual\n    ggplot(both_responses, aes(actual, predicted, color = model)) +\n      # Add a smoothed line\n      geom_smooth() +\n      # Add a line at actual = predicted\n      geom_abline(intercept = 0, slope = 1)\n    \n    # Create a tibble of residuals\n    residuals <- both_responses %>%\n      # Transmute response data to residuals\n      mutate(residual = predicted - actual)\n    \n    # Draw a density plot of residuals\n    ggplot(residuals, aes(residual, color = model)) +\n      # Add a density curve\n      geom_density() +\n      # Add a vertical line through zero\n      geom_vline(xintercept = 0)\n    \n    \n    \n    \n    # Comparing model performance ----\n    # both_responses has been pre-defined\n    both_responses\n    \n    # Create a residual sum of squares dataset\n    both_responses %>%\n      mutate(residual = predicted - actual) %>%\n      group_by(model) %>%\n      summarize(rmse = sqrt(mean(residual^2)))",
    "created" : 1495232026236.000,
    "dirty" : false,
    "encoding" : "UTF-8",
    "folds" : "",
    "hash" : "3922566526",
    "id" : "D2378F23",
    "lastKnownWriteTime" : 1496787246,
    "last_content_update" : 1496787246878,
    "path" : "C:/Users/d91067/Desktop/datacamp/Introduction_to_Spark_in_R_using_sparklyr/Spark_in_R.R",
    "project_path" : "Spark_in_R.R",
    "properties" : {
        "tempName" : "Untitled1"
    },
    "relative_order" : 1,
    "source_on_save" : false,
    "source_window" : "",
    "type" : "r_source"
}